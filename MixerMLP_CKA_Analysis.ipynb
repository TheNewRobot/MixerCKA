{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zSzgroG_Suq"
      },
      "source": [
        "# Study of Image classification with modern MLP Mixer model and CKA\n",
        "\n",
        "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmtajQ2N_Suw"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example implements three modern attention-free, multi-layer perceptron (MLP) based models for image\n",
        "classification, demonstrated on the CIFAR-100 dataset:\n",
        "\n",
        "1. The [MLP-Mixer](https://arxiv.org/abs/2105.01601) model, by Ilya Tolstikhin et al., based on two types of MLPs.\n",
        "\n",
        "The purpose of the example is not to compare between these models, as they might perform differently on\n",
        "different datasets with well-tuned hyperparameters. Rather, it is to show simple implementations of their\n",
        "main building blocks.\n",
        "\n",
        "This example requires TensorFlow 2.4 or higher, as well as\n",
        "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n",
        "which can be installed using the following command:\n",
        "\n",
        "```shell\n",
        "pip install -U tensorflow-addons\n",
        "update tensorflow core to 2.7.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U087DdDw_Suy"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnTyoluw_Suz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt \n",
        "from CKA import linear_CKA, kernel_CKA\n",
        "import seaborn as sns "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUuu2OAS_Su0"
      },
      "source": [
        "## Prepare the data\n",
        "C1FAR 100 = 100 classes, each 600 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_1Ql-fV_Su1"
      },
      "outputs": [],
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
        "#plt.imshow(x_train[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DAOrIHu_Su2"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1iMiVS7o_Su3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 8 X 8 = 64 \n",
            "Patches per image: 64\n",
            "Elements per patch (3 channels): 192\n"
          ]
        }
      ],
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 128 # The paper also fine tunes this to 512\n",
        "num_epochs = 20\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  # We'll resize input images to this size. Square\n",
        "patch_size = 8  # Size of the patches to be extracted from the input images. Square\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
        "embedding_dim = 256  # Number of hidden units.\n",
        "num_blocks = 4  # Number of Mixer Layers\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08mpnEZH_Su5"
      },
      "source": [
        "## Build a classification model\n",
        "\n",
        "We implement a method that builds a classifier given the processing blocks. \\\n",
        "Positional Encoding = https://kazemnejad.com/blog/transformer_architecture_positional_encoding/  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra-bXojQ_Su6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_classifier(blocks, positional_encoding=False):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data. ## (data_augmentation)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches. ## (patches_4)\n",
        "    patches = Patches(patch_size, num_patches)(augmented)\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor. ## (dense_163)\n",
        "    x = layers.Dense(units=embedding_dim)(patches)\n",
        "    if positional_encoding:\n",
        "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "        position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=embedding_dim\n",
        "        )(positions)\n",
        "        x = x + position_embedding\n",
        "    # Process x using the module blocks. ## (sequential_82)\n",
        "    x = blocks(x)\n",
        "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. ## (Global Average Pooling)\n",
        "    representation = layers.GlobalAveragePooling1D()(x)\n",
        "    # Apply dropout.\n",
        "    representation = layers.Dropout(rate=dropout_rate)(representation) ## (Dropout)\n",
        "    # Compute logits outputs.\n",
        "    logits = layers.Dense(num_classes)(representation) ## (dense_164) - output\n",
        "    # Create the Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpQFU8H__Su8"
      },
      "source": [
        "## Define an experiment\n",
        "\n",
        "We implement a utility function to compile, train, and evaluate a given model. \\\n",
        "Adam Algorithm with Weight Decay: https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/AdamW \\\n",
        "Losses: https://keras.io/api/losses/ \\\n",
        "Reduce learning rate: https://keras.io/api/callbacks/reduce_lr_on_plateau/ \\\n",
        "Logits: https://www.youtube.com/watch?v=icQaFxKa_J0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E1zD2BY_Su8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
        "    )\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        #Negative Log Likelihood = Categorical Cross Entropy\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5\n",
        "    )\n",
        "    # Create an early stopping regularization callback. \n",
        "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxeE0cmM_Su9"
      },
      "source": [
        "## Use data augmentation\n",
        "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh6hFPWX_Su-"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G77cUgiu_Su-"
      },
      "source": [
        "## Implement patch extraction as a layer\n",
        "Atributes and heritage: https://pythones.net/funcion-super-en-python-bien-explicada-ejemplos-oop/ \\\n",
        "Extract Patches: https://www.tensorflow.org/api_docs/python/tf/image/extract_patches \\\n",
        "Reshape: https://www.tensorflow.org/api_docs/python/tf/reshape \\\n",
        "If one component of shape is the special value -1, the size of that dimension is computed so that the total size remains constant. \\\n",
        "Preprocessing data: https://www.tensorflow.org/guide/keras/preprocessing_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYKNktXe_Su_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, num_patches):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def call(self, images):\n",
        "        #Extract the shape dimension in the position 0 = columns\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            #Without overlapping, stride horizontally and vertically\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
        "            rates=[1, 1, 1, 1],\n",
        "            #Patches contained in the images are considered, no zero padding\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        #shape[-1], number of colummns, as well as shape[0]\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7yehcSS_Su_"
      },
      "source": [
        "## The MLP-Mixer model\n",
        "\n",
        "The MLP-Mixer is an architecture based exclusively on\n",
        "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
        "\n",
        "1. One applied independently to image patches, which mixes the per-location features.\n",
        "2. The other applied across patches (along channels), which mixes spatial information.\n",
        "\n",
        "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
        "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
        "instead of batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvwg4e2n_SvA"
      },
      "source": [
        "### Implement the MLP-Mixer module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT6wVEki_SvA"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MLPMixerLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
        "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.mlp1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=num_patches),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.mlp2 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize(inputs)\n",
        "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "        x_channels = tf.linalg.matrix_transpose(x)\n",
        "        # Apply mlp1 on each channel independently.\n",
        "        mlp1_outputs = self.mlp1(x_channels)\n",
        "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
        "        # Add skip connection.\n",
        "        x = mlp1_outputs + inputs\n",
        "        # Apply layer normalization.\n",
        "        x_patches = self.normalize(x)\n",
        "        # Apply mlp2 on each patch independtenly.\n",
        "        mlp2_outputs = self.mlp2(x_patches)\n",
        "        # Add skip connection.\n",
        "        x = x + mlp2_outputs\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUiufq92_SvA"
      },
      "source": [
        "### Build, train, and evaluate the MLP-Mixer model\n",
        "\n",
        "Note that training the model with the current settings on a V100 GPUs\n",
        "takes around 8 seconds per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkJmd_7i_SvB"
      },
      "outputs": [],
      "source": [
        "mlpmixer_blocks = keras.Sequential(\n",
        "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.005\n",
        "mlpmixer_classifier = build_classifier(mlpmixer_blocks) # Returns the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlpmixer_classifier.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = run_experiment(mlpmixer_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1R6tSH2_SvC"
      },
      "source": [
        "The MLP-Mixer model tends to have much less number of parameters compared\n",
        "to convolutional and transformer-based models, which leads to less training and\n",
        "serving computational cost.\n",
        "\n",
        "As mentioned in the [MLP-Mixer](https://arxiv.org/abs/2105.01601) paper,\n",
        "when pre-trained on large datasets, or with modern regularization schemes,\n",
        "the MLP-Mixer attains competitive scores to state-of-the-art models.\n",
        "You can obtain better results by increasing the embedding dimensions,\n",
        "increasing the number of mixer blocks, and training the model for longer.\n",
        "You may also try to increase the size of the input images and use different patch sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization of activations\n",
        "Here are some important methods of each layer\n",
        "- dir()\n",
        "- type()\n",
        "- ._name\n",
        "- .get_input_shape_at(0)\n",
        "\n",
        "```shell\n",
        "1block\n",
        "mlpmixer_classifier.layers[4].layers[0].mlp1.layers[0]\n",
        "mlpmixer_classifier.layers[4].layers[0].mlp1.layers[2]\n",
        "mlpmixer_classifier.layers[4].layers[0].mlp2.layers[0]\n",
        "mlpmixer_classifier.layers[4].layers[0].mlp2.layers[2]\n",
        "mlpmixer_classifier.layers[4].layers[1].mlp1.layers[0]\n",
        "mlpmixer_classifier.layers[4].layers[1].mlp1.layers[0]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Mixer_Layer_Outputs_base(layer_name,example):\n",
        "    model = mlpmixer_classifier.layers[4].layers[1].mlp1\n",
        "    layer_output=model.get_layer(layer_name).output\n",
        "    intermediate_model=tf.keras.models.Model(inputs=model.input,outputs=layer_output)\n",
        "    augmented = data_augmentation(x_train[example])\n",
        "    b = Patches(patch_size, num_patches)(augmented)\n",
        "    a = layers.Dense(units=embedding_dim)(b)\n",
        "    inp = tf.reshape(a,[1,256,64])\n",
        "    intermediate_prediction=intermediate_model.predict(inp)\n",
        "    layactivation = intermediate_prediction.reshape((256,64))\n",
        "    visualize_out(layactivation,layer_name,example)\n",
        "    return layactivation\n",
        "#test = Mixer_Layer_Outputs_base('dense_4',4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_out(result,layer_name,example):\n",
        "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
        "    ax1.imshow(x_train[example])\n",
        "    ax1.set_title('Original_Figure, Class:' + str(y_train[example][0]))\n",
        "    ax2.imshow(result)\n",
        "    ax2.set_title('Activations of layer: '+ '\"' + layer_name + '\"')\n",
        "    print(np.shape(result))\n",
        "    print(np.shape(x_train[example]))\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Regularization(example):\n",
        "    augmented = data_augmentation(x_train[example])\n",
        "    b = Patches(patch_size, num_patches)(augmented)\n",
        "    a = layers.Dense(units=embedding_dim)(b)\n",
        "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
        "    return inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Mixer_Activations(example):\n",
        "    total_activations = list()\n",
        "    model1 = mlpmixer_classifier.layers[4]\n",
        "    example = Regularization(example)\n",
        "    for i in range(num_blocks):\n",
        "        shape=(embedding_dim,num_patches)\n",
        "        model = model1.layers[i].mlp1\n",
        "        int_total_activations = Mixer_Layer_Outputs(model,example,shape)\n",
        "        total_activations.append(int_total_activations)\n",
        "        shape=(num_patches,embedding_dim)\n",
        "        model = model1.layers[i].mlp2\n",
        "        int_total_activations = Mixer_Layer_Outputs(model,example,shape)\n",
        "        total_activations.append(int_total_activations)\n",
        "    return total_activations\n",
        "#result = Mixer_Activations(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Mixer_Layer_Outputs(model,example,shape):\n",
        "    intermediate_model=tf.keras.models.Model(inputs=model.input,outputs=model.output)\n",
        "    example = tf.reshape(example,[1,shape[0],shape[1]])\n",
        "    intermediate_prediction =intermediate_model.predict(example)\n",
        "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
        "    #visualize_out(layactivation,layer_name,example)\n",
        "    return layactivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Heatmap(result,type,bl):\n",
        "    dim = len(result)\n",
        "    heatmap_kernel = np.zeros((dim,dim))\n",
        "    heatmap_linear = np.zeros((dim,dim))\n",
        "    axis_labels = list()\n",
        "    for i in range(dim):\n",
        "        for j in range(dim):\n",
        "            heatmap_kernel[i][j] = kernel_CKA(result[i],result[j])\n",
        "            heatmap_linear[i][j] = linear_CKA(result[i],result[j])  \n",
        "        axis_labels_inter = str('L%iB%i'%((i//2)+1,(i%2)+1))\n",
        "        axis_labels.append(axis_labels_inter)\n",
        "    ax = plt.axes()\n",
        "    ax.set_xlabel('L=# MixerLayer, B = #Block within the MixerLayer')\n",
        "    ax.set_title('Similarity Measures - Index: CKA-'+ type)\n",
        "    if type == 'kernel':\n",
        "        sns.heatmap(heatmap_kernel, xticklabels=axis_labels, yticklabels=axis_labels, ax = ax, annot=bl) \n",
        "    elif type == 'linear':\n",
        "        sns.heatmap(heatmap_linear, xticklabels=axis_labels, yticklabels=axis_labels, ax = ax, annot=bl)\n",
        "    else:\n",
        "        print('Error, there is no type of plot define for the selection (2nd argument)')\n",
        "    \n",
        "    return heatmap_kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CKA Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup additional libraries\n",
        "\n",
        "GitHub1: https://github.com/yuanli2333/CKA-Centered-Kernel-Alignment/blob/master/CKA.ipynb \\\n",
        "GitHub2: https://github.com/jayroxis/CKA-similarity/blob/main/CKA.ipynb \\\n",
        "Paper: https://arxiv.org/pdf/1905.00414.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(x_train[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Repu_First_Activity\n",
        "result = Mixer_Activations(2)\n",
        "plot_heatmap = Heatmap(result,'kernel',False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reporte: Curva de aprendizaje\n",
        "def diagnostico(histories):\n",
        "    for i in range(len(histories)):\n",
        "        # Graph loss\n",
        "        pyplot.subplot(2,1,1)\n",
        "        pyplot.title(\"Cross Entropy Loss\")\n",
        "        pyplot.plot(histories[i].history[\"loss\"], color = 'blue', label = 'Entramiento')\n",
        "        pyplot.plot(histories[i].history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
        "        # Graph accuracy\n",
        "        pyplot.subplot(2,1,2)\n",
        "        pyplot.title('Classification accuracy')\n",
        "        pyplot.plot(histories[i].history[\"accuracy\"], color = 'blue', label = 'Entramiento')\n",
        "        pyplot.plot(histories[i].history[\"val_accuracy\"], color = 'blue', label = 'Testing')\n",
        "    pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    mixer_json = mlpmixer_classifier.to_json()\n",
        "    #Save the architecture of the model\n",
        "    with open(name_file +\".json\",'w') as json_file:\n",
        "        json_file.write(mixer_json)\n",
        "    # Save the weights in a hdf5 file\n",
        "    mlpmixer_classifier.save_weights( name_file +\".h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mlp_image_classification",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
