{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSzgroG_Suq"
   },
   "source": [
    "# Study of Image classification with modern MLP Mixer model and CKA\n",
    "\n",
    "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
    "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U087DdDw_Suy"
   },
   "source": [
    "# Setup for the MLP-Mixer Architecture\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AnTyoluw_Suz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import pickle\n",
    "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
    "from CKA_Google import *\n",
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Understand Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DAOrIHu_Su2"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1iMiVS7o_Su3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224 = 50176\n",
      "Patch size: 32 X 32 = 1024 \n",
      "Patches per image: 49\n",
      "Elements per patch (3 channels): 3072\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 512 \n",
    "num_epochs = 50\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Selected Architecture: B/32\n",
    "\n",
    "image_size = 224  # We'll resize input images to this size. Square\n",
    "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
    "embedding_dim = [384]  # Fixed Embedding Dimension\n",
    "num_blocks = [8,12,24,32]\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d_%H-%M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuu2OAS_Su0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Dataset for training \n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "#plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mpnEZH_Su5"
   },
   "source": [
    "## Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ra-bXojQ_Su6"
   },
   "outputs": [],
   "source": [
    "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data. \n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches. \n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks. ## (sequential_82)\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation) \n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpQFU8H__Su8"
   },
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9E1zD2BY_Su8"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        #Negative Log Likelihood = Categorical Cross Entropy\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping regularization callback. \n",
    "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
    "    #early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    #    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    #)\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history, accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxeE0cmM_Su9"
   },
   "source": [
    "## Use data augmentation\n",
    "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zh6hFPWX_Su-"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G77cUgiu_Su-"
   },
   "source": [
    "## Implement patch extraction as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYKNktXe_Su_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        #Extract the shape dimension in the position 0 = columns\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Without overlapping, stride horizontally and vertically\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
    "            rates=[1, 1, 1, 1],\n",
    "            #Patches contained in the images are considered, no zero padding\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        #shape[-1], number of colummns, as well as shape[0]\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Patches, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'patch_size' : self.patch_size ,\n",
    "            'num_patches' : self.num_patches\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yehcSS_Su_"
   },
   "source": [
    "## The MLP-Mixer model\n",
    "\n",
    "The MLP-Mixer is an architecture based exclusively on\n",
    "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "1. One applied independently to image patches, which mixes the per-location features.\n",
    "2. The other applied across patches (along channels), which mixes spatial information.\n",
    "\n",
    "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
    "instead of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwg4e2n_SvA"
   },
   "source": [
    "### Implement the MLP-Mixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dT6wVEki_SvA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp2 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super(MLPMixerLayer, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'num_patches' : num_patches,\n",
    "            'embedding_dim' : embedding_dim,\n",
    "            'dropout_rate' : dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUiufq92_SvA"
   },
   "source": [
    "## Build, train, and evaluate the MLP-Mixer model\n",
    "\n",
    "Note that training the model with the current settings on a V100 GPUs\n",
    "takes around 8 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report: Learning Curve\n",
    "def curves(history):\n",
    "    ymax1 = min(history[\"loss\"])\n",
    "    xmax1 = history[\"loss\"].index(ymax1)\n",
    "    ymax2 = min(history[\"val_loss\"])\n",
    "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
    "    plt.title(\"Cross Entropy Loss\")\n",
    "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
    "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Graph accuracy\n",
    "    ymax3 = max(history[\"acc\"])\n",
    "    xmax3 = history[\"acc\"].index(ymax3)\n",
    "    ymax4 = max(history[\"val_acc\"])\n",
    "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
    "    ymax5 = max(history[\"top5-acc\"])\n",
    "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
    "    ymax6 = max(history[\"val_top5-acc\"])\n",
    "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Classification accuracy')\n",
    "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Classification top5-acc')\n",
    "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.suptitle(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain activations + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers + Patches + One dense layer\n",
    "def Preprocessing(num_example):\n",
    "    augmented = data_augmentation(x_train[num_example])\n",
    "    b = Patches(patch_size, num_patches)(augmented)\n",
    "    a = layers.Dense(units=embedding_dim)(b)\n",
    "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
    "def Batch_Preprocessing(batch_size):\n",
    "    #Vector with the number of Sample of the Xtrain\n",
    "    a  = list(range(0,x_train.shape[0]))\n",
    "    b = random.sample(a,batch_size)\n",
    "    batch_regularization = list()\n",
    "    for i in range(0,batch_size):\n",
    "        inter_result = Preprocessing(b[i])\n",
    "        batch_regularization.append(inter_result)\n",
    "    return batch_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_out(result,layer_number,example):\n",
    "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "    ax1.imshow(x_train[example])\n",
    "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
    "    ax2.imshow(result[layer_number])\n",
    "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
    "    #The input is fixed to the beginning of the mlp blocks\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[0]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of layer's activation\n",
    "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
    "def Heatmap(result,type,sigma):\n",
    "    dim = len(result)\n",
    "    k = (dim - 1)\n",
    "    heatmap_CKA = np.zeros((dim,dim))\n",
    "    for i in range(0,dim):\n",
    "        tr = (dim - 1)\n",
    "        for j in range(0,dim):\n",
    "            if type == 'kernel':\n",
    "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
    "            elif type == 'linear':\n",
    "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
    "            else:\n",
    "                print('There is no such category, try again')\n",
    "                break\n",
    "\n",
    "            tr -= 1\n",
    "        k -= 1\n",
    "    #print('CKA' + type + 'calculated')\n",
    "    return heatmap_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of heatmaps (obsolet)\n",
    "def Prom_Mixer_Heatmaps(batch_result,type):\n",
    "    mat_heatmaps = list()\n",
    "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
    "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
    "        mat_heatmaps.append(heatmap_raw)\n",
    "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
    "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
    "    return prom_mixer_heatmap,mat_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 4\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i'%(i+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(3,3))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Understand Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A: Different Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create different mlpmixers according to an array of widths or depths\n",
    "def mlpmixer_iterations(num_patches,experiment,embedding_dim,num_blocks):\n",
    "    it_widths = len(embedding_dim)\n",
    "    it_blocks = len(num_blocks)\n",
    "    for j in range(it_widths):\n",
    "        for i in range(it_blocks):\n",
    "            mlpmixer_blocks = keras.Sequential(\n",
    "            [MLPMixerLayer(num_patches, embedding_dim[j], dropout_rate) for _ in range(num_blocks[i])] # creates the number of block without a \n",
    "            )\n",
    "            mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim[j]) # Returns the model\n",
    "            history,accuracy, top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "            #Saving Results\n",
    "            pwd = 'Results_Article/'+ str(experiment) +'/mlpmixer_'+ str(num_blocks[i]) + 'ly_' + str(embedding_dim[j]) + 'Dc'\n",
    "            mlpmixer_classifier.save(pwd)\n",
    "            np.save( pwd + '/history.npy',history.history)\n",
    "            with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "                pickle.dump(accuracy,file)\n",
    "            with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "                pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 22s 166ms/step - loss: 4.5357 - acc: 0.2477 - top5-acc: 0.7327 - val_loss: 1.6583 - val_acc: 0.4126 - val_top5-acc: 0.8918 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.5635 - acc: 0.4339 - top5-acc: 0.9056 - val_loss: 1.4205 - val_acc: 0.4888 - val_top5-acc: 0.9284 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.4376 - acc: 0.4803 - top5-acc: 0.9248 - val_loss: 1.3171 - val_acc: 0.5300 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.3638 - acc: 0.5105 - top5-acc: 0.9326 - val_loss: 1.2638 - val_acc: 0.5534 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.3066 - acc: 0.5281 - top5-acc: 0.9405 - val_loss: 1.2072 - val_acc: 0.5664 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2447 - acc: 0.5540 - top5-acc: 0.9465 - val_loss: 1.1831 - val_acc: 0.5790 - val_top5-acc: 0.9604 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2208 - acc: 0.5617 - top5-acc: 0.9488 - val_loss: 1.1487 - val_acc: 0.6006 - val_top5-acc: 0.9596 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.1781 - acc: 0.5800 - top5-acc: 0.9523 - val_loss: 1.1225 - val_acc: 0.6008 - val_top5-acc: 0.9610 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.1532 - acc: 0.5894 - top5-acc: 0.9566 - val_loss: 1.1691 - val_acc: 0.5954 - val_top5-acc: 0.9586 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.1397 - acc: 0.5933 - top5-acc: 0.9566 - val_loss: 1.0936 - val_acc: 0.6082 - val_top5-acc: 0.9704 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.1088 - acc: 0.6071 - top5-acc: 0.9589 - val_loss: 1.0490 - val_acc: 0.6314 - val_top5-acc: 0.9608 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.0936 - acc: 0.6096 - top5-acc: 0.9619 - val_loss: 1.0411 - val_acc: 0.6256 - val_top5-acc: 0.9672 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.0779 - acc: 0.6185 - top5-acc: 0.9624 - val_loss: 0.9939 - val_acc: 0.6574 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.0600 - acc: 0.6233 - top5-acc: 0.9637 - val_loss: 0.9685 - val_acc: 0.6592 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.0398 - acc: 0.6324 - top5-acc: 0.9653 - val_loss: 0.9640 - val_acc: 0.6572 - val_top5-acc: 0.9724 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.0176 - acc: 0.6388 - top5-acc: 0.9672 - val_loss: 0.9790 - val_acc: 0.6556 - val_top5-acc: 0.9740 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.0021 - acc: 0.6450 - top5-acc: 0.9683 - val_loss: 0.9462 - val_acc: 0.6682 - val_top5-acc: 0.9732 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9780 - acc: 0.6529 - top5-acc: 0.9693 - val_loss: 0.9483 - val_acc: 0.6674 - val_top5-acc: 0.9738 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9760 - acc: 0.6572 - top5-acc: 0.9693 - val_loss: 0.9087 - val_acc: 0.6768 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9694 - acc: 0.6546 - top5-acc: 0.9719 - val_loss: 0.9108 - val_acc: 0.6832 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9540 - acc: 0.6626 - top5-acc: 0.9714 - val_loss: 0.9202 - val_acc: 0.6840 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9582 - acc: 0.6644 - top5-acc: 0.9712 - val_loss: 0.9062 - val_acc: 0.6878 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9274 - acc: 0.6711 - top5-acc: 0.9736 - val_loss: 0.8797 - val_acc: 0.6946 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9264 - acc: 0.6748 - top5-acc: 0.9724 - val_loss: 0.8621 - val_acc: 0.7036 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9121 - acc: 0.6758 - top5-acc: 0.9739 - val_loss: 0.8693 - val_acc: 0.7004 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9076 - acc: 0.6794 - top5-acc: 0.9739 - val_loss: 0.8738 - val_acc: 0.6916 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8980 - acc: 0.6807 - top5-acc: 0.9749 - val_loss: 0.9016 - val_acc: 0.6910 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8921 - acc: 0.6874 - top5-acc: 0.9751 - val_loss: 0.8944 - val_acc: 0.6890 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8768 - acc: 0.6900 - top5-acc: 0.9767 - val_loss: 0.8453 - val_acc: 0.7044 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8721 - acc: 0.6932 - top5-acc: 0.9771 - val_loss: 0.8827 - val_acc: 0.7082 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8700 - acc: 0.6912 - top5-acc: 0.9780 - val_loss: 0.8513 - val_acc: 0.7060 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 6.8246 - acc: 0.5125 - top5-acc: 0.8694 - val_loss: 5.0151 - val_acc: 0.2980 - val_top5-acc: 0.7706 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.8540 - acc: 0.4639 - top5-acc: 0.9050 - val_loss: 1.1275 - val_acc: 0.5968 - val_top5-acc: 0.9632 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.1265 - acc: 0.6008 - top5-acc: 0.9572 - val_loss: 1.0319 - val_acc: 0.6346 - val_top5-acc: 0.9700 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.0124 - acc: 0.6409 - top5-acc: 0.9669 - val_loss: 0.9569 - val_acc: 0.6670 - val_top5-acc: 0.9726 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9663 - acc: 0.6603 - top5-acc: 0.9703 - val_loss: 0.9268 - val_acc: 0.6764 - val_top5-acc: 0.9774 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9334 - acc: 0.6700 - top5-acc: 0.9723 - val_loss: 0.8913 - val_acc: 0.6908 - val_top5-acc: 0.9784 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.9034 - acc: 0.6820 - top5-acc: 0.9746 - val_loss: 0.8851 - val_acc: 0.6924 - val_top5-acc: 0.9796 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8812 - acc: 0.6908 - top5-acc: 0.9750 - val_loss: 0.8676 - val_acc: 0.7026 - val_top5-acc: 0.9770 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8486 - acc: 0.7034 - top5-acc: 0.9771 - val_loss: 0.8422 - val_acc: 0.7082 - val_top5-acc: 0.9800 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8377 - acc: 0.7052 - top5-acc: 0.9786 - val_loss: 0.8219 - val_acc: 0.7160 - val_top5-acc: 0.9790 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8200 - acc: 0.7121 - top5-acc: 0.9793 - val_loss: 0.8271 - val_acc: 0.7134 - val_top5-acc: 0.9796 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8144 - acc: 0.7144 - top5-acc: 0.9796 - val_loss: 0.8069 - val_acc: 0.7188 - val_top5-acc: 0.9814 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 14s 158ms/step - loss: 0.8006 - acc: 0.7190 - top5-acc: 0.9812 - val_loss: 0.8034 - val_acc: 0.7236 - val_top5-acc: 0.9814 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 0.7940 - acc: 0.7250 - top5-acc: 0.9810 - val_loss: 0.7929 - val_acc: 0.7230 - val_top5-acc: 0.9814 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 0.7846 - acc: 0.7221 - top5-acc: 0.9821 - val_loss: 0.7940 - val_acc: 0.7262 - val_top5-acc: 0.9820 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 0.7760 - acc: 0.7256 - top5-acc: 0.9819 - val_loss: 0.7912 - val_acc: 0.7236 - val_top5-acc: 0.9812 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 0.7691 - acc: 0.7296 - top5-acc: 0.9825 - val_loss: 0.7737 - val_acc: 0.7310 - val_top5-acc: 0.9812 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 0.7636 - acc: 0.7316 - top5-acc: 0.9823 - val_loss: 0.7698 - val_acc: 0.7300 - val_top5-acc: 0.9836 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 0.7604 - acc: 0.7331 - top5-acc: 0.9824 - val_loss: 0.7611 - val_acc: 0.7380 - val_top5-acc: 0.9830 - lr: 0.0012\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.7844 - acc: 0.7242 - top5-acc: 0.9809\n",
      "Test accuracy: 72.42%\n",
      "Test top 5 accuracy: 98.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn, layer_normalization_1_layer_call_and_return_conditional_losses, layer_normalization_2_layer_call_fn while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_8ly_384Dc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_8ly_384Dc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 242ms/step - loss: 4.2055 - acc: 0.2397 - top5-acc: 0.7318 - val_loss: 1.7413 - val_acc: 0.3678 - val_top5-acc: 0.8784 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.6059 - acc: 0.4214 - top5-acc: 0.8951 - val_loss: 1.4547 - val_acc: 0.4834 - val_top5-acc: 0.9282 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.4464 - acc: 0.4792 - top5-acc: 0.9228 - val_loss: 1.4160 - val_acc: 0.4954 - val_top5-acc: 0.9368 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.3520 - acc: 0.5137 - top5-acc: 0.9348 - val_loss: 1.3369 - val_acc: 0.5354 - val_top5-acc: 0.9392 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2914 - acc: 0.5375 - top5-acc: 0.9422 - val_loss: 1.1848 - val_acc: 0.5838 - val_top5-acc: 0.9580 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2412 - acc: 0.5567 - top5-acc: 0.9469 - val_loss: 1.1624 - val_acc: 0.5814 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1997 - acc: 0.5749 - top5-acc: 0.9493 - val_loss: 1.1636 - val_acc: 0.5944 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1606 - acc: 0.5865 - top5-acc: 0.9551 - val_loss: 1.0771 - val_acc: 0.6166 - val_top5-acc: 0.9670 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1385 - acc: 0.5937 - top5-acc: 0.9570 - val_loss: 1.1204 - val_acc: 0.6082 - val_top5-acc: 0.9598 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1078 - acc: 0.6052 - top5-acc: 0.9593 - val_loss: 1.0362 - val_acc: 0.6366 - val_top5-acc: 0.9694 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0764 - acc: 0.6184 - top5-acc: 0.9614 - val_loss: 1.0086 - val_acc: 0.6498 - val_top5-acc: 0.9680 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0573 - acc: 0.6251 - top5-acc: 0.9627 - val_loss: 0.9695 - val_acc: 0.6594 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0275 - acc: 0.6360 - top5-acc: 0.9652 - val_loss: 0.9594 - val_acc: 0.6672 - val_top5-acc: 0.9694 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0179 - acc: 0.6398 - top5-acc: 0.9666 - val_loss: 0.9670 - val_acc: 0.6556 - val_top5-acc: 0.9722 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9982 - acc: 0.6473 - top5-acc: 0.9682 - val_loss: 0.9397 - val_acc: 0.6728 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9719 - acc: 0.6552 - top5-acc: 0.9707 - val_loss: 0.9091 - val_acc: 0.6862 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9596 - acc: 0.6599 - top5-acc: 0.9706 - val_loss: 0.9196 - val_acc: 0.6800 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9466 - acc: 0.6655 - top5-acc: 0.9711 - val_loss: 0.9562 - val_acc: 0.6718 - val_top5-acc: 0.9738 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9241 - acc: 0.6730 - top5-acc: 0.9735 - val_loss: 0.8927 - val_acc: 0.6866 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9254 - acc: 0.6721 - top5-acc: 0.9729 - val_loss: 0.8795 - val_acc: 0.6954 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9053 - acc: 0.6819 - top5-acc: 0.9744 - val_loss: 0.8612 - val_acc: 0.6978 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9020 - acc: 0.6798 - top5-acc: 0.9750 - val_loss: 0.8245 - val_acc: 0.7144 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8861 - acc: 0.6860 - top5-acc: 0.9761 - val_loss: 0.8530 - val_acc: 0.6982 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8696 - acc: 0.6938 - top5-acc: 0.9772 - val_loss: 0.8205 - val_acc: 0.7122 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8591 - acc: 0.6981 - top5-acc: 0.9760 - val_loss: 0.7965 - val_acc: 0.7214 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8435 - acc: 0.7034 - top5-acc: 0.9778 - val_loss: 0.8383 - val_acc: 0.7202 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8534 - acc: 0.6996 - top5-acc: 0.9779 - val_loss: 0.8319 - val_acc: 0.7108 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8323 - acc: 0.7076 - top5-acc: 0.9790 - val_loss: 0.8442 - val_acc: 0.7078 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8312 - acc: 0.7076 - top5-acc: 0.9786 - val_loss: 0.8012 - val_acc: 0.7242 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8227 - acc: 0.7115 - top5-acc: 0.9793 - val_loss: 0.8084 - val_acc: 0.7168 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.7280 - acc: 0.7456 - top5-acc: 0.9842 - val_loss: 0.7446 - val_acc: 0.7458 - val_top5-acc: 0.9838 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.7002 - acc: 0.7517 - top5-acc: 0.9850 - val_loss: 0.7501 - val_acc: 0.7404 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.7006 - acc: 0.7522 - top5-acc: 0.9860 - val_loss: 0.7446 - val_acc: 0.7486 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.6892 - acc: 0.7591 - top5-acc: 0.9861 - val_loss: 0.7213 - val_acc: 0.7512 - val_top5-acc: 0.9844 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.6763 - acc: 0.7605 - top5-acc: 0.9870 - val_loss: 0.7280 - val_acc: 0.7500 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.6872 - acc: 0.7550 - top5-acc: 0.9867 - val_loss: 0.7432 - val_acc: 0.7488 - val_top5-acc: 0.9820 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.6698 - acc: 0.7651 - top5-acc: 0.9870 - val_loss: 0.7319 - val_acc: 0.7500 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.6677 - acc: 0.7637 - top5-acc: 0.9872 - val_loss: 0.7260 - val_acc: 0.7584 - val_top5-acc: 0.9848 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.6711 - acc: 0.7619 - top5-acc: 0.9878 - val_loss: 0.7471 - val_acc: 0.7466 - val_top5-acc: 0.9846 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.6063 - acc: 0.7877 - top5-acc: 0.9889 - val_loss: 0.6999 - val_acc: 0.7654 - val_top5-acc: 0.9856 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5908 - acc: 0.7912 - top5-acc: 0.9896 - val_loss: 0.6969 - val_acc: 0.7644 - val_top5-acc: 0.9852 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5810 - acc: 0.7930 - top5-acc: 0.9905 - val_loss: 0.7014 - val_acc: 0.7624 - val_top5-acc: 0.9830 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5778 - acc: 0.7959 - top5-acc: 0.9906 - val_loss: 0.7038 - val_acc: 0.7664 - val_top5-acc: 0.9844 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5806 - acc: 0.7955 - top5-acc: 0.9906 - val_loss: 0.6942 - val_acc: 0.7684 - val_top5-acc: 0.9862 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5664 - acc: 0.7993 - top5-acc: 0.9913 - val_loss: 0.7310 - val_acc: 0.7570 - val_top5-acc: 0.9830 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5658 - acc: 0.8005 - top5-acc: 0.9912 - val_loss: 0.7139 - val_acc: 0.7654 - val_top5-acc: 0.9842 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5619 - acc: 0.8005 - top5-acc: 0.9909 - val_loss: 0.6963 - val_acc: 0.7692 - val_top5-acc: 0.9868 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5604 - acc: 0.8021 - top5-acc: 0.9910 - val_loss: 0.6984 - val_acc: 0.7724 - val_top5-acc: 0.9860 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5610 - acc: 0.8023 - top5-acc: 0.9912 - val_loss: 0.6984 - val_acc: 0.7694 - val_top5-acc: 0.9864 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.5196 - acc: 0.8153 - top5-acc: 0.9933 - val_loss: 0.6981 - val_acc: 0.7686 - val_top5-acc: 0.9868 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 0.7194 - acc: 0.7651 - top5-acc: 0.9849\n",
      "Test accuracy: 76.51%\n",
      "Test top 5 accuracy: 98.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_8_layer_call_fn, layer_normalization_8_layer_call_and_return_conditional_losses, layer_normalization_9_layer_call_fn, layer_normalization_9_layer_call_and_return_conditional_losses, layer_normalization_10_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_12ly_384Dc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_12ly_384Dc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 56s 463ms/step - loss: 3.6085 - acc: 0.2612 - top5-acc: 0.7520 - val_loss: 1.6743 - val_acc: 0.4118 - val_top5-acc: 0.8916 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 1.5429 - acc: 0.4472 - top5-acc: 0.9069 - val_loss: 1.3398 - val_acc: 0.5286 - val_top5-acc: 0.9356 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.3940 - acc: 0.5005 - top5-acc: 0.9280 - val_loss: 1.3187 - val_acc: 0.5300 - val_top5-acc: 0.9426 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.3134 - acc: 0.5292 - top5-acc: 0.9406 - val_loss: 1.2215 - val_acc: 0.5608 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.2533 - acc: 0.5512 - top5-acc: 0.9471 - val_loss: 1.1349 - val_acc: 0.5928 - val_top5-acc: 0.9592 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.2163 - acc: 0.5665 - top5-acc: 0.9484 - val_loss: 1.1011 - val_acc: 0.6086 - val_top5-acc: 0.9624 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.1614 - acc: 0.5858 - top5-acc: 0.9538 - val_loss: 1.0686 - val_acc: 0.6240 - val_top5-acc: 0.9608 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.1368 - acc: 0.5959 - top5-acc: 0.9572 - val_loss: 1.0927 - val_acc: 0.6184 - val_top5-acc: 0.9592 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 38s 438ms/step - loss: 1.1033 - acc: 0.6084 - top5-acc: 0.9588 - val_loss: 1.0374 - val_acc: 0.6344 - val_top5-acc: 0.9672 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.0760 - acc: 0.6171 - top5-acc: 0.9622 - val_loss: 0.9890 - val_acc: 0.6488 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.0515 - acc: 0.6256 - top5-acc: 0.9635 - val_loss: 0.9834 - val_acc: 0.6494 - val_top5-acc: 0.9740 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.0233 - acc: 0.6349 - top5-acc: 0.9674 - val_loss: 0.9684 - val_acc: 0.6578 - val_top5-acc: 0.9734 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.0040 - acc: 0.6435 - top5-acc: 0.9676 - val_loss: 0.9747 - val_acc: 0.6604 - val_top5-acc: 0.9742 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.9872 - acc: 0.6506 - top5-acc: 0.9697 - val_loss: 0.9439 - val_acc: 0.6712 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.9610 - acc: 0.6622 - top5-acc: 0.9708 - val_loss: 0.8830 - val_acc: 0.6938 - val_top5-acc: 0.9760 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 0.9434 - acc: 0.6669 - top5-acc: 0.9717 - val_loss: 0.8758 - val_acc: 0.6976 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.9179 - acc: 0.6778 - top5-acc: 0.9731 - val_loss: 0.8910 - val_acc: 0.6938 - val_top5-acc: 0.9758 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.8899 - acc: 0.6870 - top5-acc: 0.9755 - val_loss: 0.8593 - val_acc: 0.6996 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.8765 - acc: 0.6901 - top5-acc: 0.9773 - val_loss: 0.8549 - val_acc: 0.7020 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 0.8688 - acc: 0.6930 - top5-acc: 0.9778 - val_loss: 0.8452 - val_acc: 0.7094 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.8604 - acc: 0.6977 - top5-acc: 0.9778 - val_loss: 0.8772 - val_acc: 0.7024 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 0.8477 - acc: 0.7011 - top5-acc: 0.9782 - val_loss: 0.9034 - val_acc: 0.6912 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 22.9842 - acc: 0.3596 - top5-acc: 0.7509 - val_loss: 2.2918 - val_acc: 0.3268 - val_top5-acc: 0.8270 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.7790 - acc: 0.3869 - top5-acc: 0.8712 - val_loss: 1.4100 - val_acc: 0.4990 - val_top5-acc: 0.9268 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.4704 - acc: 0.4749 - top5-acc: 0.9179 - val_loss: 1.3246 - val_acc: 0.5292 - val_top5-acc: 0.9350 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 39s 440ms/step - loss: 1.3548 - acc: 0.5172 - top5-acc: 0.9337 - val_loss: 1.2802 - val_acc: 0.5410 - val_top5-acc: 0.9430 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.3141 - acc: 0.5334 - top5-acc: 0.9378 - val_loss: 1.2096 - val_acc: 0.5670 - val_top5-acc: 0.9482 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 39s 440ms/step - loss: 1.2650 - acc: 0.5534 - top5-acc: 0.9414 - val_loss: 1.1725 - val_acc: 0.5832 - val_top5-acc: 0.9556 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 39s 441ms/step - loss: 1.2334 - acc: 0.5624 - top5-acc: 0.9465 - val_loss: 1.1306 - val_acc: 0.5946 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.1867 - acc: 0.5804 - top5-acc: 0.9521 - val_loss: 1.1037 - val_acc: 0.6100 - val_top5-acc: 0.9598 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.1567 - acc: 0.5918 - top5-acc: 0.9537 - val_loss: 1.0615 - val_acc: 0.6194 - val_top5-acc: 0.9622 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.1347 - acc: 0.5978 - top5-acc: 0.9556 - val_loss: 1.0488 - val_acc: 0.6232 - val_top5-acc: 0.9624 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.1206 - acc: 0.6030 - top5-acc: 0.9569 - val_loss: 1.0383 - val_acc: 0.6354 - val_top5-acc: 0.9638 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.1032 - acc: 0.6100 - top5-acc: 0.9594 - val_loss: 1.0095 - val_acc: 0.6380 - val_top5-acc: 0.9654 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.0872 - acc: 0.6127 - top5-acc: 0.9611 - val_loss: 1.0219 - val_acc: 0.6374 - val_top5-acc: 0.9666 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.0683 - acc: 0.6225 - top5-acc: 0.9623 - val_loss: 0.9889 - val_acc: 0.6508 - val_top5-acc: 0.9670 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.0511 - acc: 0.6286 - top5-acc: 0.9640 - val_loss: 0.9871 - val_acc: 0.6512 - val_top5-acc: 0.9680 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.0474 - acc: 0.6308 - top5-acc: 0.9633 - val_loss: 0.9779 - val_acc: 0.6536 - val_top5-acc: 0.9682 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.0385 - acc: 0.6334 - top5-acc: 0.9653 - val_loss: 0.9638 - val_acc: 0.6592 - val_top5-acc: 0.9690 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.0276 - acc: 0.6388 - top5-acc: 0.9668 - val_loss: 0.9595 - val_acc: 0.6590 - val_top5-acc: 0.9698 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 1.0167 - acc: 0.6428 - top5-acc: 0.9669 - val_loss: 0.9558 - val_acc: 0.6598 - val_top5-acc: 0.9692 - lr: 3.1250e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.0148 - acc: 0.6431 - top5-acc: 0.9671 - val_loss: 0.9547 - val_acc: 0.6614 - val_top5-acc: 0.9692 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.0067 - acc: 0.6443 - top5-acc: 0.9692 - val_loss: 0.9400 - val_acc: 0.6680 - val_top5-acc: 0.9710 - lr: 3.1250e-04\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 39s 439ms/step - loss: 1.0026 - acc: 0.6470 - top5-acc: 0.9680 - val_loss: 0.9405 - val_acc: 0.6688 - val_top5-acc: 0.9712 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 0.9993 - acc: 0.6488 - top5-acc: 0.9689 - val_loss: 0.9317 - val_acc: 0.6684 - val_top5-acc: 0.9722 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.9947 - acc: 0.6503 - top5-acc: 0.9692 - val_loss: 0.9420 - val_acc: 0.6624 - val_top5-acc: 0.9698 - lr: 1.5625e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.9996 - acc: 0.6501 - top5-acc: 0.9687 - val_loss: 0.9297 - val_acc: 0.6670 - val_top5-acc: 0.9720 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.9973 - acc: 0.6499 - top5-acc: 0.9686 - val_loss: 0.9341 - val_acc: 0.6648 - val_top5-acc: 0.9714 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.9974 - acc: 0.6488 - top5-acc: 0.9682 - val_loss: 0.9329 - val_acc: 0.6666 - val_top5-acc: 0.9712 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 0.9949 - acc: 0.6520 - top5-acc: 0.9688 - val_loss: 0.9374 - val_acc: 0.6636 - val_top5-acc: 0.9704 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 0.9667 - acc: 0.6604 - top5-acc: 0.9723\n",
      "Test accuracy: 66.04%\n",
      "Test top 5 accuracy: 97.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_20_layer_call_fn, layer_normalization_20_layer_call_and_return_conditional_losses, layer_normalization_21_layer_call_fn, layer_normalization_21_layer_call_and_return_conditional_losses, layer_normalization_22_layer_call_fn while saving (showing 5 of 48). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_24ly_384Dc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_24ly_384Dc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 74s 611ms/step - loss: 3.9783 - acc: 0.2592 - top5-acc: 0.7514 - val_loss: 1.5866 - val_acc: 0.4260 - val_top5-acc: 0.8970 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 1.5603 - acc: 0.4364 - top5-acc: 0.9033 - val_loss: 1.3998 - val_acc: 0.5068 - val_top5-acc: 0.9296 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.3946 - acc: 0.5014 - top5-acc: 0.9287 - val_loss: 1.2180 - val_acc: 0.5592 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.2980 - acc: 0.5381 - top5-acc: 0.9400 - val_loss: 1.1842 - val_acc: 0.5698 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.2470 - acc: 0.5557 - top5-acc: 0.9458 - val_loss: 1.1703 - val_acc: 0.5858 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 1.1822 - acc: 0.5796 - top5-acc: 0.9528 - val_loss: 1.1065 - val_acc: 0.5972 - val_top5-acc: 0.9656 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.1480 - acc: 0.5908 - top5-acc: 0.9562 - val_loss: 1.1508 - val_acc: 0.6022 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.1135 - acc: 0.6046 - top5-acc: 0.9589 - val_loss: 1.1069 - val_acc: 0.6202 - val_top5-acc: 0.9600 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.0817 - acc: 0.6161 - top5-acc: 0.9625 - val_loss: 1.0024 - val_acc: 0.6442 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 1.0485 - acc: 0.6271 - top5-acc: 0.9633 - val_loss: 0.9724 - val_acc: 0.6608 - val_top5-acc: 0.9722 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 1.0280 - acc: 0.6327 - top5-acc: 0.9659 - val_loss: 0.9972 - val_acc: 0.6486 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.9969 - acc: 0.6473 - top5-acc: 0.9675 - val_loss: 0.9767 - val_acc: 0.6674 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.9837 - acc: 0.6517 - top5-acc: 0.9685 - val_loss: 0.9472 - val_acc: 0.6722 - val_top5-acc: 0.9744 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.9682 - acc: 0.6595 - top5-acc: 0.9701 - val_loss: 0.9436 - val_acc: 0.6676 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 51s 580ms/step - loss: 0.9432 - acc: 0.6673 - top5-acc: 0.9709 - val_loss: 0.9332 - val_acc: 0.6778 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 51s 580ms/step - loss: 0.9146 - acc: 0.6758 - top5-acc: 0.9743 - val_loss: 1.0305 - val_acc: 0.6476 - val_top5-acc: 0.9654 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 51s 580ms/step - loss: 0.9013 - acc: 0.6832 - top5-acc: 0.9754 - val_loss: 0.9565 - val_acc: 0.6690 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 51s 581ms/step - loss: 18.3166 - acc: 0.3503 - top5-acc: 0.7384 - val_loss: 4.3761 - val_acc: 0.2362 - val_top5-acc: 0.7076 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 1.8680 - acc: 0.3944 - top5-acc: 0.8751 - val_loss: 1.3711 - val_acc: 0.5040 - val_top5-acc: 0.9362 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 1.3777 - acc: 0.5045 - top5-acc: 0.9306 - val_loss: 1.2213 - val_acc: 0.5638 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.2638 - acc: 0.5476 - top5-acc: 0.9426 - val_loss: 1.1488 - val_acc: 0.5868 - val_top5-acc: 0.9570 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 1.2070 - acc: 0.5701 - top5-acc: 0.9490 - val_loss: 1.1035 - val_acc: 0.6034 - val_top5-acc: 0.9606 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.1629 - acc: 0.5880 - top5-acc: 0.9547 - val_loss: 1.0631 - val_acc: 0.6176 - val_top5-acc: 0.9628 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 1.1207 - acc: 0.6000 - top5-acc: 0.9577 - val_loss: 1.0299 - val_acc: 0.6258 - val_top5-acc: 0.9688 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 1.0849 - acc: 0.6147 - top5-acc: 0.9607 - val_loss: 1.0051 - val_acc: 0.6406 - val_top5-acc: 0.9672 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 51s 580ms/step - loss: 1.0502 - acc: 0.6266 - top5-acc: 0.9636 - val_loss: 0.9802 - val_acc: 0.6478 - val_top5-acc: 0.9704 - lr: 0.0012\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 1.0309 - acc: 0.6329 - top5-acc: 0.9656 - val_loss: 0.9615 - val_acc: 0.6520 - val_top5-acc: 0.9704 - lr: 0.0012\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.0109 - acc: 0.6434 - top5-acc: 0.9666 - val_loss: 0.9643 - val_acc: 0.6526 - val_top5-acc: 0.9730 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.9964 - acc: 0.6489 - top5-acc: 0.9676 - val_loss: 0.9574 - val_acc: 0.6554 - val_top5-acc: 0.9728 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 51s 580ms/step - loss: 0.9858 - acc: 0.6528 - top5-acc: 0.9679 - val_loss: 0.9348 - val_acc: 0.6640 - val_top5-acc: 0.9746 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.9656 - acc: 0.6587 - top5-acc: 0.9693 - val_loss: 0.9165 - val_acc: 0.6726 - val_top5-acc: 0.9764 - lr: 6.2500e-04\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.9542 - acc: 0.6647 - top5-acc: 0.9708 - val_loss: 0.9138 - val_acc: 0.6732 - val_top5-acc: 0.9760 - lr: 6.2500e-04\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.9457 - acc: 0.6681 - top5-acc: 0.9713 - val_loss: 0.9073 - val_acc: 0.6758 - val_top5-acc: 0.9772 - lr: 6.2500e-04\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.9381 - acc: 0.6690 - top5-acc: 0.9706 - val_loss: 0.8988 - val_acc: 0.6764 - val_top5-acc: 0.9760 - lr: 6.2500e-04\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.9301 - acc: 0.6713 - top5-acc: 0.9726 - val_loss: 0.8962 - val_acc: 0.6796 - val_top5-acc: 0.9772 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.9242 - acc: 0.6745 - top5-acc: 0.9716 - val_loss: 0.8898 - val_acc: 0.6854 - val_top5-acc: 0.9794 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 51s 576ms/step - loss: 0.9157 - acc: 0.6785 - top5-acc: 0.9738 - val_loss: 0.8805 - val_acc: 0.6892 - val_top5-acc: 0.9774 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.9013 - acc: 0.6814 - top5-acc: 0.9746 - val_loss: 0.8781 - val_acc: 0.6886 - val_top5-acc: 0.9770 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.8966 - acc: 0.6837 - top5-acc: 0.9746 - val_loss: 0.8694 - val_acc: 0.6960 - val_top5-acc: 0.9784 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 51s 576ms/step - loss: 0.8879 - acc: 0.6886 - top5-acc: 0.9750 - val_loss: 0.8630 - val_acc: 0.6920 - val_top5-acc: 0.9796 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.8831 - acc: 0.6907 - top5-acc: 0.9755 - val_loss: 0.8590 - val_acc: 0.6986 - val_top5-acc: 0.9792 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.8705 - acc: 0.6949 - top5-acc: 0.9754 - val_loss: 0.8547 - val_acc: 0.6962 - val_top5-acc: 0.9798 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.8680 - acc: 0.6937 - top5-acc: 0.9764 - val_loss: 0.8590 - val_acc: 0.6982 - val_top5-acc: 0.9788 - lr: 6.2500e-04\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 51s 577ms/step - loss: 0.8543 - acc: 0.6999 - top5-acc: 0.9779 - val_loss: 0.8479 - val_acc: 0.6994 - val_top5-acc: 0.9792 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.8484 - acc: 0.7020 - top5-acc: 0.9770 - val_loss: 0.8294 - val_acc: 0.7104 - val_top5-acc: 0.9794 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.8390 - acc: 0.7050 - top5-acc: 0.9781 - val_loss: 0.8345 - val_acc: 0.7104 - val_top5-acc: 0.9802 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.8322 - acc: 0.7060 - top5-acc: 0.9782 - val_loss: 0.8248 - val_acc: 0.7096 - val_top5-acc: 0.9806 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.8195 - acc: 0.7111 - top5-acc: 0.9804 - val_loss: 0.8141 - val_acc: 0.7134 - val_top5-acc: 0.9808 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 51s 580ms/step - loss: 0.8149 - acc: 0.7134 - top5-acc: 0.9793 - val_loss: 0.8180 - val_acc: 0.7116 - val_top5-acc: 0.9804 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 51s 581ms/step - loss: 0.8018 - acc: 0.7161 - top5-acc: 0.9800 - val_loss: 0.8184 - val_acc: 0.7166 - val_top5-acc: 0.9816 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 37s 119ms/step - loss: 0.8543 - acc: 0.7018 - top5-acc: 0.9775\n",
      "Test accuracy: 70.18%\n",
      "Test top 5 accuracy: 97.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_44_layer_call_fn, layer_normalization_44_layer_call_and_return_conditional_losses, layer_normalization_45_layer_call_fn, layer_normalization_45_layer_call_and_return_conditional_losses, layer_normalization_46_layer_call_fn while saving (showing 5 of 64). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_32ly_384Dc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_32ly_384Dc\\assets\n"
     ]
    }
   ],
   "source": [
    "mlpmixer_iterations(num_patches,'1A', embedding_dim,num_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Average of layer's activation\n",
    "sigma = 1\n",
    "type = 'kernel'\n",
    "embedding_dim = 384\n",
    "blocks_total = num_blocks\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run separtely once to avoid randomness \n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CEB43994C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CEB43994C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A2668B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C8A2668B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    }
   ],
   "source": [
    "for item in blocks_total:\n",
    "    path = 'Results_Article/1A/mlpmixer_'+ str(item) +'ly_384Dc'\n",
    "    #Call the folder\n",
    "    tested_model = tf.keras.models.load_model(path)\n",
    "    num_blocks = item\n",
    "    A1_ave_mixer_activations = Prom_Mixer_Activations_Blocks(tested_model,batch_prepro)\n",
    "    A1_global_heatmap = Heatmap(A1_ave_mixer_activations,type,sigma)\n",
    "    with open(path + '/heatmap_'+ type + '_Sg'+ str(sigma) +'_'+ str(item)+'ly_384Dc.pkl','wb') as file:\n",
    "                pickle.dump(A1_global_heatmap,file)\n",
    "    with open(path + '/activations_'+ str(item)+'ly_384Dc.pkl','wb') as file:\n",
    "                pickle.dump(A1_ave_mixer_activations,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the only parameter that have to be initialized since, all the parameters are shared with Experiment 1A\n",
    "embedding_d = embedding_dim  # Fixed Embedding Dimension from experiment 1A\n",
    "path_1B = 'Results_Article/1B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evol_accuracy(all_models,num_blocks):\n",
    "    total_plots=list()\n",
    "    for f in range(len(all_models)) :\n",
    "        testing_model = all_models[f]\n",
    "        partial_plots = list()\n",
    "        for j in range(num_blocks[f]):\n",
    "            #Define the Mixer Block that are going to participate (Cumulative Approach)\n",
    "            inter_input = testing_model.layers[4].layers[0].input\n",
    "            inter_output = testing_model.layers[4].layers[j].output\n",
    "            partial_models=tf.keras.models.Model(inputs=inter_input,outputs=inter_output, name = 'Mixer_Blocks')\n",
    "            #Create the structure of the model\n",
    "            inputs = layers.Input(shape=input_shape)\n",
    "            augmented = data_augmentation(inputs)\n",
    "            patches = Patches(patch_size, num_patches)(augmented)\n",
    "            x = testing_model.layers[3](patches)\n",
    "            intermediate_output  =  partial_models(x)\n",
    "            representation = layers.GlobalAveragePooling1D()(intermediate_output)\n",
    "            output =  layers.Dense(units=num_classes, activation='softmax')(representation) # Linear regression that is going to be trained\n",
    "            final_modelx =   keras.Model(inputs=inputs, outputs=output)\n",
    "            #Set the condition to not trainable\n",
    "            final_modelx.layers[3].trainable = False\n",
    "            final_modelx.layers[4].trainable = False\n",
    "            __,accuracy,__= run_experiment(final_modelx)\n",
    "            with open(path_1B + '/accuracy_Blocks_'+ str(num_blocks[f]) + '_L' + str(j+1)+ '.pkl','wb') as file:\n",
    "                        pickle.dump(accuracy,file)\n",
    "            partial_plots.append(accuracy)\n",
    "        total_plots.append(partial_plots)\n",
    "    return total_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the path in this cell (loading results from the experiment 1A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "listnumblocks = [num_blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = list()\n",
    "for layer in listnumblocks:\n",
    "    #Call the folder\n",
    "    pwd1 = 'Results_Article/1A/mlpmixer_'+ str(layer) + 'ly_' + str(embedding_d) + 'Dc' \n",
    "    layers_models = tf.keras.models.load_model(pwd1, compile=False)\n",
    "    all_models.append(layers_models)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 3s 26ms/step - loss: 2.3622 - acc: 0.2527 - top5-acc: 0.7645 - val_loss: 1.8861 - val_acc: 0.3286 - val_top5-acc: 0.8286 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.9316 - acc: 0.3142 - top5-acc: 0.8222 - val_loss: 1.8939 - val_acc: 0.3266 - val_top5-acc: 0.8320 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.8905 - acc: 0.3268 - top5-acc: 0.8349 - val_loss: 1.8715 - val_acc: 0.3248 - val_top5-acc: 0.8452 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.8649 - acc: 0.3356 - top5-acc: 0.8438 - val_loss: 1.7941 - val_acc: 0.3656 - val_top5-acc: 0.8588 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.8557 - acc: 0.3418 - top5-acc: 0.8468 - val_loss: 1.7954 - val_acc: 0.3608 - val_top5-acc: 0.8562 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.8320 - acc: 0.3515 - top5-acc: 0.8498 - val_loss: 1.7415 - val_acc: 0.3710 - val_top5-acc: 0.8682 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.8078 - acc: 0.3582 - top5-acc: 0.8566 - val_loss: 1.7380 - val_acc: 0.3776 - val_top5-acc: 0.8668 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.7932 - acc: 0.3640 - top5-acc: 0.8610 - val_loss: 1.7905 - val_acc: 0.3638 - val_top5-acc: 0.8580 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.8182 - acc: 0.3556 - top5-acc: 0.8571 - val_loss: 1.7093 - val_acc: 0.3866 - val_top5-acc: 0.8724 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.7941 - acc: 0.3634 - top5-acc: 0.8597 - val_loss: 1.7715 - val_acc: 0.3778 - val_top5-acc: 0.8626 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.7744 - acc: 0.3661 - top5-acc: 0.8652 - val_loss: 1.7298 - val_acc: 0.3846 - val_top5-acc: 0.8650 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.7848 - acc: 0.3695 - top5-acc: 0.8604 - val_loss: 1.7311 - val_acc: 0.3778 - val_top5-acc: 0.8690 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.7559 - acc: 0.3736 - top5-acc: 0.8672 - val_loss: 1.7167 - val_acc: 0.3954 - val_top5-acc: 0.8682 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.7881 - acc: 0.3664 - top5-acc: 0.8642 - val_loss: 1.7171 - val_acc: 0.3796 - val_top5-acc: 0.8828 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6969 - acc: 0.3930 - top5-acc: 0.8749 - val_loss: 1.6280 - val_acc: 0.4164 - val_top5-acc: 0.8894 - lr: 0.0025\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.7028 - acc: 0.3918 - top5-acc: 0.8757 - val_loss: 1.6555 - val_acc: 0.4096 - val_top5-acc: 0.8840 - lr: 0.0025\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6869 - acc: 0.3964 - top5-acc: 0.8808 - val_loss: 1.6443 - val_acc: 0.4214 - val_top5-acc: 0.8814 - lr: 0.0025\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6960 - acc: 0.3950 - top5-acc: 0.8774 - val_loss: 1.6453 - val_acc: 0.4116 - val_top5-acc: 0.8868 - lr: 0.0025\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6920 - acc: 0.3960 - top5-acc: 0.8757 - val_loss: 1.6647 - val_acc: 0.4070 - val_top5-acc: 0.8820 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6945 - acc: 0.3936 - top5-acc: 0.8759 - val_loss: 1.6797 - val_acc: 0.3984 - val_top5-acc: 0.8798 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6722 - acc: 0.4036 - top5-acc: 0.8802 - val_loss: 1.6229 - val_acc: 0.4068 - val_top5-acc: 0.8934 - lr: 0.0012\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6630 - acc: 0.4067 - top5-acc: 0.8814 - val_loss: 1.6068 - val_acc: 0.4216 - val_top5-acc: 0.8936 - lr: 0.0012\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6633 - acc: 0.4050 - top5-acc: 0.8816 - val_loss: 1.6106 - val_acc: 0.4304 - val_top5-acc: 0.8898 - lr: 0.0012\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6668 - acc: 0.4068 - top5-acc: 0.8803 - val_loss: 1.6144 - val_acc: 0.4172 - val_top5-acc: 0.8908 - lr: 0.0012\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6655 - acc: 0.4058 - top5-acc: 0.8814 - val_loss: 1.6093 - val_acc: 0.4248 - val_top5-acc: 0.8926 - lr: 0.0012\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6656 - acc: 0.4045 - top5-acc: 0.8821 - val_loss: 1.6043 - val_acc: 0.4288 - val_top5-acc: 0.8872 - lr: 0.0012\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6662 - acc: 0.4053 - top5-acc: 0.8821 - val_loss: 1.6357 - val_acc: 0.4034 - val_top5-acc: 0.8898 - lr: 0.0012\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6706 - acc: 0.3994 - top5-acc: 0.8813 - val_loss: 1.6339 - val_acc: 0.4226 - val_top5-acc: 0.8762 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6639 - acc: 0.4035 - top5-acc: 0.8812 - val_loss: 1.6290 - val_acc: 0.4086 - val_top5-acc: 0.8852 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6622 - acc: 0.4044 - top5-acc: 0.8811 - val_loss: 1.6161 - val_acc: 0.4220 - val_top5-acc: 0.8916 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6601 - acc: 0.4081 - top5-acc: 0.8820 - val_loss: 1.6304 - val_acc: 0.4188 - val_top5-acc: 0.8862 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6471 - acc: 0.4127 - top5-acc: 0.8842 - val_loss: 1.6121 - val_acc: 0.4224 - val_top5-acc: 0.8918 - lr: 6.2500e-04\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6492 - acc: 0.4128 - top5-acc: 0.8840 - val_loss: 1.5993 - val_acc: 0.4302 - val_top5-acc: 0.8932 - lr: 6.2500e-04\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6499 - acc: 0.4116 - top5-acc: 0.8834 - val_loss: 1.6061 - val_acc: 0.4196 - val_top5-acc: 0.8886 - lr: 6.2500e-04\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6510 - acc: 0.4108 - top5-acc: 0.8834 - val_loss: 1.5999 - val_acc: 0.4306 - val_top5-acc: 0.8962 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6478 - acc: 0.4124 - top5-acc: 0.8852 - val_loss: 1.5982 - val_acc: 0.4288 - val_top5-acc: 0.8922 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6483 - acc: 0.4132 - top5-acc: 0.8842 - val_loss: 1.6150 - val_acc: 0.4208 - val_top5-acc: 0.8854 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6499 - acc: 0.4120 - top5-acc: 0.8849 - val_loss: 1.6106 - val_acc: 0.4206 - val_top5-acc: 0.8966 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6481 - acc: 0.4130 - top5-acc: 0.8848 - val_loss: 1.6196 - val_acc: 0.4210 - val_top5-acc: 0.8910 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6543 - acc: 0.4089 - top5-acc: 0.8832 - val_loss: 1.6038 - val_acc: 0.4270 - val_top5-acc: 0.8892 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6505 - acc: 0.4106 - top5-acc: 0.8837 - val_loss: 1.6035 - val_acc: 0.4334 - val_top5-acc: 0.8952 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6425 - acc: 0.4155 - top5-acc: 0.8852 - val_loss: 1.6057 - val_acc: 0.4272 - val_top5-acc: 0.8904 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6425 - acc: 0.4174 - top5-acc: 0.8853 - val_loss: 1.5986 - val_acc: 0.4346 - val_top5-acc: 0.8950 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6455 - acc: 0.4148 - top5-acc: 0.8852 - val_loss: 1.6015 - val_acc: 0.4324 - val_top5-acc: 0.8918 - lr: 3.1250e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6427 - acc: 0.4137 - top5-acc: 0.8866 - val_loss: 1.6065 - val_acc: 0.4292 - val_top5-acc: 0.8920 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6477 - acc: 0.4116 - top5-acc: 0.8846 - val_loss: 1.6080 - val_acc: 0.4310 - val_top5-acc: 0.8928 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6426 - acc: 0.4188 - top5-acc: 0.8854 - val_loss: 1.6010 - val_acc: 0.4350 - val_top5-acc: 0.8932 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6410 - acc: 0.4174 - top5-acc: 0.8854 - val_loss: 1.6012 - val_acc: 0.4348 - val_top5-acc: 0.8928 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6417 - acc: 0.4170 - top5-acc: 0.8871 - val_loss: 1.6001 - val_acc: 0.4324 - val_top5-acc: 0.8920 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 1.6412 - acc: 0.4205 - top5-acc: 0.8860 - val_loss: 1.6047 - val_acc: 0.4286 - val_top5-acc: 0.8916 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.6176 - acc: 0.4314 - top5-acc: 0.8957\n",
      "Test accuracy: 43.14%\n",
      "Test top 5 accuracy: 89.57%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 4s 34ms/step - loss: 2.1174 - acc: 0.2937 - top5-acc: 0.8125 - val_loss: 1.7642 - val_acc: 0.3644 - val_top5-acc: 0.8652 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.8202 - acc: 0.3467 - top5-acc: 0.8579 - val_loss: 1.7686 - val_acc: 0.3638 - val_top5-acc: 0.8732 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.7752 - acc: 0.3630 - top5-acc: 0.8657 - val_loss: 1.6741 - val_acc: 0.3934 - val_top5-acc: 0.8862 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.7508 - acc: 0.3715 - top5-acc: 0.8708 - val_loss: 1.6402 - val_acc: 0.3964 - val_top5-acc: 0.8946 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.7494 - acc: 0.3786 - top5-acc: 0.8748 - val_loss: 1.6319 - val_acc: 0.4080 - val_top5-acc: 0.8864 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.7155 - acc: 0.3843 - top5-acc: 0.8786 - val_loss: 1.6481 - val_acc: 0.3902 - val_top5-acc: 0.8956 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.7011 - acc: 0.3891 - top5-acc: 0.8808 - val_loss: 1.6531 - val_acc: 0.4004 - val_top5-acc: 0.8934 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6938 - acc: 0.3959 - top5-acc: 0.8836 - val_loss: 1.6162 - val_acc: 0.4110 - val_top5-acc: 0.9012 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6994 - acc: 0.3944 - top5-acc: 0.8847 - val_loss: 1.5799 - val_acc: 0.4334 - val_top5-acc: 0.8984 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6815 - acc: 0.4004 - top5-acc: 0.8869 - val_loss: 1.6025 - val_acc: 0.4122 - val_top5-acc: 0.9048 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6726 - acc: 0.4039 - top5-acc: 0.8861 - val_loss: 1.6005 - val_acc: 0.4260 - val_top5-acc: 0.8990 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 1.6764 - acc: 0.4022 - top5-acc: 0.8872 - val_loss: 1.5999 - val_acc: 0.4172 - val_top5-acc: 0.9066 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6778 - acc: 0.4027 - top5-acc: 0.8866 - val_loss: 1.5585 - val_acc: 0.4320 - val_top5-acc: 0.9048 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6640 - acc: 0.4077 - top5-acc: 0.8876 - val_loss: 1.5651 - val_acc: 0.4306 - val_top5-acc: 0.9004 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6626 - acc: 0.4091 - top5-acc: 0.8892 - val_loss: 1.5960 - val_acc: 0.4260 - val_top5-acc: 0.9042 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6574 - acc: 0.4084 - top5-acc: 0.8893 - val_loss: 1.6064 - val_acc: 0.4194 - val_top5-acc: 0.9006 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6487 - acc: 0.4114 - top5-acc: 0.8907 - val_loss: 1.5844 - val_acc: 0.4334 - val_top5-acc: 0.9042 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6494 - acc: 0.4103 - top5-acc: 0.8927 - val_loss: 1.5832 - val_acc: 0.4166 - val_top5-acc: 0.9094 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5978 - acc: 0.4261 - top5-acc: 0.8984 - val_loss: 1.5144 - val_acc: 0.4474 - val_top5-acc: 0.9120 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5875 - acc: 0.4323 - top5-acc: 0.8997 - val_loss: 1.5282 - val_acc: 0.4448 - val_top5-acc: 0.9108 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5944 - acc: 0.4298 - top5-acc: 0.9004 - val_loss: 1.5520 - val_acc: 0.4252 - val_top5-acc: 0.9068 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5861 - acc: 0.4282 - top5-acc: 0.8992 - val_loss: 1.5187 - val_acc: 0.4450 - val_top5-acc: 0.9158 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5842 - acc: 0.4308 - top5-acc: 0.8987 - val_loss: 1.5167 - val_acc: 0.4486 - val_top5-acc: 0.9086 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5957 - acc: 0.4290 - top5-acc: 0.8992 - val_loss: 1.5622 - val_acc: 0.4308 - val_top5-acc: 0.9066 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5687 - acc: 0.4357 - top5-acc: 0.9003 - val_loss: 1.4962 - val_acc: 0.4592 - val_top5-acc: 0.9144 - lr: 0.0012\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5678 - acc: 0.4365 - top5-acc: 0.9022 - val_loss: 1.4920 - val_acc: 0.4652 - val_top5-acc: 0.9160 - lr: 0.0012\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5717 - acc: 0.4389 - top5-acc: 0.9016 - val_loss: 1.4938 - val_acc: 0.4628 - val_top5-acc: 0.9164 - lr: 0.0012\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5673 - acc: 0.4382 - top5-acc: 0.9014 - val_loss: 1.4930 - val_acc: 0.4668 - val_top5-acc: 0.9148 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5695 - acc: 0.4366 - top5-acc: 0.9029 - val_loss: 1.5026 - val_acc: 0.4540 - val_top5-acc: 0.9188 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5717 - acc: 0.4350 - top5-acc: 0.9018 - val_loss: 1.5167 - val_acc: 0.4560 - val_top5-acc: 0.9098 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5644 - acc: 0.4386 - top5-acc: 0.9019 - val_loss: 1.5000 - val_acc: 0.4584 - val_top5-acc: 0.9134 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5522 - acc: 0.4424 - top5-acc: 0.9044 - val_loss: 1.4819 - val_acc: 0.4666 - val_top5-acc: 0.9204 - lr: 6.2500e-04\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5526 - acc: 0.4455 - top5-acc: 0.9037 - val_loss: 1.4841 - val_acc: 0.4652 - val_top5-acc: 0.9190 - lr: 6.2500e-04\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5546 - acc: 0.4432 - top5-acc: 0.9046 - val_loss: 1.4935 - val_acc: 0.4630 - val_top5-acc: 0.9190 - lr: 6.2500e-04\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5541 - acc: 0.4428 - top5-acc: 0.9047 - val_loss: 1.4902 - val_acc: 0.4582 - val_top5-acc: 0.9222 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5559 - acc: 0.4459 - top5-acc: 0.9041 - val_loss: 1.4915 - val_acc: 0.4638 - val_top5-acc: 0.9154 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5567 - acc: 0.4407 - top5-acc: 0.9040 - val_loss: 1.4922 - val_acc: 0.4640 - val_top5-acc: 0.9136 - lr: 6.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5496 - acc: 0.4450 - top5-acc: 0.9049 - val_loss: 1.4899 - val_acc: 0.4614 - val_top5-acc: 0.9172 - lr: 3.1250e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5501 - acc: 0.4471 - top5-acc: 0.9046 - val_loss: 1.4858 - val_acc: 0.4706 - val_top5-acc: 0.9202 - lr: 3.1250e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5505 - acc: 0.4474 - top5-acc: 0.9052 - val_loss: 1.4875 - val_acc: 0.4660 - val_top5-acc: 0.9222 - lr: 3.1250e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5496 - acc: 0.4455 - top5-acc: 0.9037 - val_loss: 1.4892 - val_acc: 0.4676 - val_top5-acc: 0.9164 - lr: 3.1250e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5501 - acc: 0.4468 - top5-acc: 0.9061 - val_loss: 1.4887 - val_acc: 0.4678 - val_top5-acc: 0.9174 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5489 - acc: 0.4462 - top5-acc: 0.9058 - val_loss: 1.4858 - val_acc: 0.4648 - val_top5-acc: 0.9204 - lr: 1.5625e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 1.5493 - acc: 0.4474 - top5-acc: 0.9058 - val_loss: 1.4863 - val_acc: 0.4660 - val_top5-acc: 0.9218 - lr: 1.5625e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5482 - acc: 0.4489 - top5-acc: 0.9058 - val_loss: 1.4881 - val_acc: 0.4656 - val_top5-acc: 0.9196 - lr: 1.5625e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5464 - acc: 0.4470 - top5-acc: 0.9057 - val_loss: 1.4893 - val_acc: 0.4668 - val_top5-acc: 0.9166 - lr: 1.5625e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 1.5507 - acc: 0.4479 - top5-acc: 0.9058 - val_loss: 1.4879 - val_acc: 0.4674 - val_top5-acc: 0.9188 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5500 - acc: 0.4489 - top5-acc: 0.9049 - val_loss: 1.4883 - val_acc: 0.4666 - val_top5-acc: 0.9204 - lr: 7.8125e-05\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5508 - acc: 0.4480 - top5-acc: 0.9063 - val_loss: 1.4905 - val_acc: 0.4700 - val_top5-acc: 0.9176 - lr: 7.8125e-05\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.5493 - acc: 0.4476 - top5-acc: 0.9060 - val_loss: 1.4901 - val_acc: 0.4656 - val_top5-acc: 0.9200 - lr: 7.8125e-05\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 1.5158 - acc: 0.4670 - top5-acc: 0.9131\n",
      "Test accuracy: 46.7%\n",
      "Test top 5 accuracy: 91.31%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 5s 43ms/step - loss: 2.1166 - acc: 0.3001 - top5-acc: 0.8102 - val_loss: 1.7102 - val_acc: 0.3836 - val_top5-acc: 0.8778 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.7683 - acc: 0.3638 - top5-acc: 0.8686 - val_loss: 1.6512 - val_acc: 0.3898 - val_top5-acc: 0.8960 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.7414 - acc: 0.3782 - top5-acc: 0.8774 - val_loss: 1.6452 - val_acc: 0.4170 - val_top5-acc: 0.8948 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6973 - acc: 0.3925 - top5-acc: 0.8836 - val_loss: 1.6333 - val_acc: 0.4098 - val_top5-acc: 0.8994 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.7098 - acc: 0.3931 - top5-acc: 0.8848 - val_loss: 1.6613 - val_acc: 0.4062 - val_top5-acc: 0.9004 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6578 - acc: 0.4086 - top5-acc: 0.8893 - val_loss: 1.5637 - val_acc: 0.4338 - val_top5-acc: 0.9048 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6590 - acc: 0.4079 - top5-acc: 0.8909 - val_loss: 1.5547 - val_acc: 0.4356 - val_top5-acc: 0.9122 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6561 - acc: 0.4121 - top5-acc: 0.8926 - val_loss: 1.5610 - val_acc: 0.4460 - val_top5-acc: 0.9114 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6361 - acc: 0.4196 - top5-acc: 0.8956 - val_loss: 1.5813 - val_acc: 0.4266 - val_top5-acc: 0.9092 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6388 - acc: 0.4185 - top5-acc: 0.8942 - val_loss: 1.5242 - val_acc: 0.4564 - val_top5-acc: 0.9088 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6204 - acc: 0.4246 - top5-acc: 0.8977 - val_loss: 1.5329 - val_acc: 0.4504 - val_top5-acc: 0.9114 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6214 - acc: 0.4238 - top5-acc: 0.8974 - val_loss: 1.5541 - val_acc: 0.4418 - val_top5-acc: 0.9146 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6180 - acc: 0.4246 - top5-acc: 0.8966 - val_loss: 1.5154 - val_acc: 0.4546 - val_top5-acc: 0.9096 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5945 - acc: 0.4336 - top5-acc: 0.9010 - val_loss: 1.5052 - val_acc: 0.4628 - val_top5-acc: 0.9188 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6122 - acc: 0.4254 - top5-acc: 0.8992 - val_loss: 1.5414 - val_acc: 0.4518 - val_top5-acc: 0.9100 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6147 - acc: 0.4276 - top5-acc: 0.8990 - val_loss: 1.5224 - val_acc: 0.4538 - val_top5-acc: 0.9146 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5979 - acc: 0.4324 - top5-acc: 0.9010 - val_loss: 1.5088 - val_acc: 0.4644 - val_top5-acc: 0.9168 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.6041 - acc: 0.4328 - top5-acc: 0.9028 - val_loss: 1.5220 - val_acc: 0.4668 - val_top5-acc: 0.9130 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5834 - acc: 0.4355 - top5-acc: 0.9032 - val_loss: 1.5199 - val_acc: 0.4528 - val_top5-acc: 0.9128 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5465 - acc: 0.4448 - top5-acc: 0.9087 - val_loss: 1.4694 - val_acc: 0.4716 - val_top5-acc: 0.9232 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5378 - acc: 0.4532 - top5-acc: 0.9081 - val_loss: 1.4592 - val_acc: 0.4798 - val_top5-acc: 0.9218 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5354 - acc: 0.4505 - top5-acc: 0.9090 - val_loss: 1.4688 - val_acc: 0.4794 - val_top5-acc: 0.9204 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5447 - acc: 0.4475 - top5-acc: 0.9092 - val_loss: 1.4887 - val_acc: 0.4664 - val_top5-acc: 0.9222 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5429 - acc: 0.4495 - top5-acc: 0.9078 - val_loss: 1.4735 - val_acc: 0.4722 - val_top5-acc: 0.9258 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5429 - acc: 0.4480 - top5-acc: 0.9093 - val_loss: 1.4641 - val_acc: 0.4750 - val_top5-acc: 0.9190 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5450 - acc: 0.4498 - top5-acc: 0.9086 - val_loss: 1.4660 - val_acc: 0.4682 - val_top5-acc: 0.9248 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5178 - acc: 0.4571 - top5-acc: 0.9110 - val_loss: 1.4367 - val_acc: 0.4874 - val_top5-acc: 0.9262 - lr: 0.0012\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5153 - acc: 0.4607 - top5-acc: 0.9121 - val_loss: 1.4427 - val_acc: 0.4852 - val_top5-acc: 0.9260 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5170 - acc: 0.4590 - top5-acc: 0.9126 - val_loss: 1.4462 - val_acc: 0.4830 - val_top5-acc: 0.9252 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5155 - acc: 0.4598 - top5-acc: 0.9128 - val_loss: 1.4476 - val_acc: 0.4812 - val_top5-acc: 0.9230 - lr: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5164 - acc: 0.4594 - top5-acc: 0.9130 - val_loss: 1.4428 - val_acc: 0.4876 - val_top5-acc: 0.9258 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5216 - acc: 0.4576 - top5-acc: 0.9111 - val_loss: 1.4466 - val_acc: 0.4896 - val_top5-acc: 0.9268 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5097 - acc: 0.4628 - top5-acc: 0.9133 - val_loss: 1.4321 - val_acc: 0.4898 - val_top5-acc: 0.9264 - lr: 6.2500e-04\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5079 - acc: 0.4632 - top5-acc: 0.9116 - val_loss: 1.4353 - val_acc: 0.4898 - val_top5-acc: 0.9260 - lr: 6.2500e-04\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5033 - acc: 0.4661 - top5-acc: 0.9134 - val_loss: 1.4310 - val_acc: 0.4948 - val_top5-acc: 0.9282 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5099 - acc: 0.4636 - top5-acc: 0.9110 - val_loss: 1.4316 - val_acc: 0.4916 - val_top5-acc: 0.9272 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5094 - acc: 0.4618 - top5-acc: 0.9131 - val_loss: 1.4350 - val_acc: 0.4932 - val_top5-acc: 0.9270 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5077 - acc: 0.4623 - top5-acc: 0.9129 - val_loss: 1.4452 - val_acc: 0.4814 - val_top5-acc: 0.9288 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5087 - acc: 0.4640 - top5-acc: 0.9133 - val_loss: 1.4352 - val_acc: 0.4858 - val_top5-acc: 0.9282 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5104 - acc: 0.4632 - top5-acc: 0.9129 - val_loss: 1.4467 - val_acc: 0.4842 - val_top5-acc: 0.9304 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5066 - acc: 0.4661 - top5-acc: 0.9119 - val_loss: 1.4358 - val_acc: 0.4946 - val_top5-acc: 0.9256 - lr: 3.1250e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5013 - acc: 0.4687 - top5-acc: 0.9127 - val_loss: 1.4320 - val_acc: 0.4924 - val_top5-acc: 0.9278 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5024 - acc: 0.4682 - top5-acc: 0.9136 - val_loss: 1.4375 - val_acc: 0.4920 - val_top5-acc: 0.9278 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5043 - acc: 0.4652 - top5-acc: 0.9153 - val_loss: 1.4325 - val_acc: 0.4910 - val_top5-acc: 0.9274 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5015 - acc: 0.4668 - top5-acc: 0.9139 - val_loss: 1.4349 - val_acc: 0.4934 - val_top5-acc: 0.9282 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.4982 - acc: 0.4710 - top5-acc: 0.9135 - val_loss: 1.4346 - val_acc: 0.4914 - val_top5-acc: 0.9266 - lr: 1.5625e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.4985 - acc: 0.4701 - top5-acc: 0.9139 - val_loss: 1.4346 - val_acc: 0.4936 - val_top5-acc: 0.9260 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.4985 - acc: 0.4715 - top5-acc: 0.9123 - val_loss: 1.4382 - val_acc: 0.4894 - val_top5-acc: 0.9272 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5053 - acc: 0.4666 - top5-acc: 0.9136 - val_loss: 1.4381 - val_acc: 0.4900 - val_top5-acc: 0.9266 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 3s 39ms/step - loss: 1.5040 - acc: 0.4684 - top5-acc: 0.9133 - val_loss: 1.4379 - val_acc: 0.4860 - val_top5-acc: 0.9276 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.4635 - acc: 0.4826 - top5-acc: 0.9233\n",
      "Test accuracy: 48.26%\n",
      "Test top 5 accuracy: 92.33%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 6s 52ms/step - loss: 1.9897 - acc: 0.3155 - top5-acc: 0.8289 - val_loss: 1.6942 - val_acc: 0.3892 - val_top5-acc: 0.8886 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.7368 - acc: 0.3807 - top5-acc: 0.8775 - val_loss: 1.6271 - val_acc: 0.4080 - val_top5-acc: 0.9040 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.6866 - acc: 0.3969 - top5-acc: 0.8888 - val_loss: 1.6023 - val_acc: 0.4286 - val_top5-acc: 0.9062 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.6568 - acc: 0.4098 - top5-acc: 0.8919 - val_loss: 1.6015 - val_acc: 0.4360 - val_top5-acc: 0.9058 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.6346 - acc: 0.4170 - top5-acc: 0.8950 - val_loss: 1.5273 - val_acc: 0.4464 - val_top5-acc: 0.9230 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.6200 - acc: 0.4228 - top5-acc: 0.9013 - val_loss: 1.5454 - val_acc: 0.4446 - val_top5-acc: 0.9134 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.6025 - acc: 0.4309 - top5-acc: 0.9006 - val_loss: 1.5111 - val_acc: 0.4596 - val_top5-acc: 0.9192 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5935 - acc: 0.4315 - top5-acc: 0.9017 - val_loss: 1.5104 - val_acc: 0.4652 - val_top5-acc: 0.9198 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5928 - acc: 0.4343 - top5-acc: 0.9037 - val_loss: 1.5213 - val_acc: 0.4494 - val_top5-acc: 0.9166 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5865 - acc: 0.4367 - top5-acc: 0.9034 - val_loss: 1.4990 - val_acc: 0.4682 - val_top5-acc: 0.9202 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5717 - acc: 0.4409 - top5-acc: 0.9061 - val_loss: 1.4987 - val_acc: 0.4568 - val_top5-acc: 0.9206 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5637 - acc: 0.4431 - top5-acc: 0.9077 - val_loss: 1.4895 - val_acc: 0.4570 - val_top5-acc: 0.9264 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5720 - acc: 0.4398 - top5-acc: 0.9058 - val_loss: 1.4987 - val_acc: 0.4568 - val_top5-acc: 0.9220 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5643 - acc: 0.4428 - top5-acc: 0.9067 - val_loss: 1.4901 - val_acc: 0.4600 - val_top5-acc: 0.9256 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5605 - acc: 0.4451 - top5-acc: 0.9092 - val_loss: 1.4992 - val_acc: 0.4496 - val_top5-acc: 0.9226 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5639 - acc: 0.4457 - top5-acc: 0.9072 - val_loss: 1.4445 - val_acc: 0.4916 - val_top5-acc: 0.9276 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5592 - acc: 0.4464 - top5-acc: 0.9078 - val_loss: 1.5060 - val_acc: 0.4590 - val_top5-acc: 0.9262 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5503 - acc: 0.4498 - top5-acc: 0.9104 - val_loss: 1.5267 - val_acc: 0.4340 - val_top5-acc: 0.9192 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5673 - acc: 0.4460 - top5-acc: 0.9077 - val_loss: 1.4852 - val_acc: 0.4708 - val_top5-acc: 0.9192 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5468 - acc: 0.4524 - top5-acc: 0.9101 - val_loss: 1.4343 - val_acc: 0.4824 - val_top5-acc: 0.9282 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5462 - acc: 0.4500 - top5-acc: 0.9096 - val_loss: 1.4476 - val_acc: 0.4800 - val_top5-acc: 0.9250 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5435 - acc: 0.4525 - top5-acc: 0.9116 - val_loss: 1.5331 - val_acc: 0.4610 - val_top5-acc: 0.9200 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5406 - acc: 0.4524 - top5-acc: 0.9099 - val_loss: 1.4465 - val_acc: 0.4832 - val_top5-acc: 0.9276 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5359 - acc: 0.4557 - top5-acc: 0.9112 - val_loss: 1.4271 - val_acc: 0.4868 - val_top5-acc: 0.9282 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5421 - acc: 0.4536 - top5-acc: 0.9100 - val_loss: 1.4635 - val_acc: 0.4740 - val_top5-acc: 0.9260 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5385 - acc: 0.4547 - top5-acc: 0.9113 - val_loss: 1.4538 - val_acc: 0.4776 - val_top5-acc: 0.9256 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5346 - acc: 0.4567 - top5-acc: 0.9119 - val_loss: 1.4623 - val_acc: 0.4720 - val_top5-acc: 0.9284 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5437 - acc: 0.4535 - top5-acc: 0.9107 - val_loss: 1.4279 - val_acc: 0.4852 - val_top5-acc: 0.9318 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5348 - acc: 0.4567 - top5-acc: 0.9128 - val_loss: 1.4532 - val_acc: 0.4784 - val_top5-acc: 0.9270 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4996 - acc: 0.4689 - top5-acc: 0.9173 - val_loss: 1.4088 - val_acc: 0.4982 - val_top5-acc: 0.9310 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4861 - acc: 0.4707 - top5-acc: 0.9170 - val_loss: 1.4045 - val_acc: 0.5008 - val_top5-acc: 0.9308 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4933 - acc: 0.4690 - top5-acc: 0.9162 - val_loss: 1.4154 - val_acc: 0.4914 - val_top5-acc: 0.9290 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4958 - acc: 0.4704 - top5-acc: 0.9175 - val_loss: 1.4083 - val_acc: 0.4918 - val_top5-acc: 0.9326 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4954 - acc: 0.4665 - top5-acc: 0.9171 - val_loss: 1.4124 - val_acc: 0.5004 - val_top5-acc: 0.9290 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4986 - acc: 0.4682 - top5-acc: 0.9169 - val_loss: 1.4156 - val_acc: 0.4948 - val_top5-acc: 0.9322 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4928 - acc: 0.4705 - top5-acc: 0.9163 - val_loss: 1.3986 - val_acc: 0.5062 - val_top5-acc: 0.9324 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4866 - acc: 0.4717 - top5-acc: 0.9158 - val_loss: 1.4004 - val_acc: 0.5018 - val_top5-acc: 0.9336 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4921 - acc: 0.4679 - top5-acc: 0.9174 - val_loss: 1.4149 - val_acc: 0.4988 - val_top5-acc: 0.9300 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4937 - acc: 0.4682 - top5-acc: 0.9163 - val_loss: 1.4191 - val_acc: 0.4936 - val_top5-acc: 0.9320 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4936 - acc: 0.4660 - top5-acc: 0.9163 - val_loss: 1.4092 - val_acc: 0.4944 - val_top5-acc: 0.9350 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4859 - acc: 0.4708 - top5-acc: 0.9180 - val_loss: 1.4232 - val_acc: 0.4874 - val_top5-acc: 0.9314 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4726 - acc: 0.4780 - top5-acc: 0.9189 - val_loss: 1.3972 - val_acc: 0.5040 - val_top5-acc: 0.9374 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4722 - acc: 0.4762 - top5-acc: 0.9205 - val_loss: 1.3924 - val_acc: 0.5080 - val_top5-acc: 0.9372 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4726 - acc: 0.4764 - top5-acc: 0.9206 - val_loss: 1.3961 - val_acc: 0.4986 - val_top5-acc: 0.9326 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4688 - acc: 0.4763 - top5-acc: 0.9212 - val_loss: 1.3940 - val_acc: 0.5012 - val_top5-acc: 0.9336 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4709 - acc: 0.4772 - top5-acc: 0.9207 - val_loss: 1.4034 - val_acc: 0.4982 - val_top5-acc: 0.9342 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4712 - acc: 0.4748 - top5-acc: 0.9189 - val_loss: 1.4067 - val_acc: 0.5052 - val_top5-acc: 0.9296 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4729 - acc: 0.4772 - top5-acc: 0.9186 - val_loss: 1.3931 - val_acc: 0.5048 - val_top5-acc: 0.9346 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4634 - acc: 0.4823 - top5-acc: 0.9200 - val_loss: 1.3890 - val_acc: 0.5038 - val_top5-acc: 0.9358 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.4574 - acc: 0.4826 - top5-acc: 0.9214 - val_loss: 1.3927 - val_acc: 0.5102 - val_top5-acc: 0.9358 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.4153 - acc: 0.4991 - top5-acc: 0.9326\n",
      "Test accuracy: 49.91%\n",
      "Test top 5 accuracy: 93.26%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 7s 60ms/step - loss: 1.9184 - acc: 0.3281 - top5-acc: 0.8451 - val_loss: 1.6381 - val_acc: 0.4004 - val_top5-acc: 0.8984 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.6739 - acc: 0.3950 - top5-acc: 0.8892 - val_loss: 1.6131 - val_acc: 0.4154 - val_top5-acc: 0.9034 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.6366 - acc: 0.4144 - top5-acc: 0.8978 - val_loss: 1.5642 - val_acc: 0.4348 - val_top5-acc: 0.9086 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.6061 - acc: 0.4305 - top5-acc: 0.9010 - val_loss: 1.5347 - val_acc: 0.4452 - val_top5-acc: 0.9146 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5821 - acc: 0.4357 - top5-acc: 0.9049 - val_loss: 1.5244 - val_acc: 0.4528 - val_top5-acc: 0.9152 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5733 - acc: 0.4400 - top5-acc: 0.9079 - val_loss: 1.4772 - val_acc: 0.4608 - val_top5-acc: 0.9260 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5669 - acc: 0.4430 - top5-acc: 0.9082 - val_loss: 1.5015 - val_acc: 0.4652 - val_top5-acc: 0.9094 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5488 - acc: 0.4497 - top5-acc: 0.9110 - val_loss: 1.4963 - val_acc: 0.4670 - val_top5-acc: 0.9212 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5426 - acc: 0.4538 - top5-acc: 0.9100 - val_loss: 1.4603 - val_acc: 0.4728 - val_top5-acc: 0.9278 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5270 - acc: 0.4583 - top5-acc: 0.9142 - val_loss: 1.4623 - val_acc: 0.4710 - val_top5-acc: 0.9260 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5240 - acc: 0.4558 - top5-acc: 0.9145 - val_loss: 1.4387 - val_acc: 0.4792 - val_top5-acc: 0.9256 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5235 - acc: 0.4581 - top5-acc: 0.9143 - val_loss: 1.4314 - val_acc: 0.4878 - val_top5-acc: 0.9252 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5196 - acc: 0.4583 - top5-acc: 0.9149 - val_loss: 1.4205 - val_acc: 0.4908 - val_top5-acc: 0.9262 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5176 - acc: 0.4612 - top5-acc: 0.9153 - val_loss: 1.4356 - val_acc: 0.4782 - val_top5-acc: 0.9286 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5111 - acc: 0.4617 - top5-acc: 0.9163 - val_loss: 1.4056 - val_acc: 0.4976 - val_top5-acc: 0.9292 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5147 - acc: 0.4614 - top5-acc: 0.9150 - val_loss: 1.4281 - val_acc: 0.4872 - val_top5-acc: 0.9272 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4984 - acc: 0.4661 - top5-acc: 0.9180 - val_loss: 1.4571 - val_acc: 0.4718 - val_top5-acc: 0.9248 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5008 - acc: 0.4666 - top5-acc: 0.9180 - val_loss: 1.4210 - val_acc: 0.4804 - val_top5-acc: 0.9304 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5039 - acc: 0.4648 - top5-acc: 0.9165 - val_loss: 1.4228 - val_acc: 0.4848 - val_top5-acc: 0.9290 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4962 - acc: 0.4683 - top5-acc: 0.9180 - val_loss: 1.3968 - val_acc: 0.4982 - val_top5-acc: 0.9278 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4929 - acc: 0.4692 - top5-acc: 0.9166 - val_loss: 1.4225 - val_acc: 0.4898 - val_top5-acc: 0.9294 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.4899 - acc: 0.4722 - top5-acc: 0.9197 - val_loss: 1.3897 - val_acc: 0.4994 - val_top5-acc: 0.9336 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.4926 - acc: 0.4670 - top5-acc: 0.9188 - val_loss: 1.3966 - val_acc: 0.4934 - val_top5-acc: 0.9252 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4903 - acc: 0.4726 - top5-acc: 0.9184 - val_loss: 1.4028 - val_acc: 0.4954 - val_top5-acc: 0.9306 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4850 - acc: 0.4720 - top5-acc: 0.9198 - val_loss: 1.3995 - val_acc: 0.4878 - val_top5-acc: 0.9306 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4856 - acc: 0.4722 - top5-acc: 0.9209 - val_loss: 1.4087 - val_acc: 0.4936 - val_top5-acc: 0.9278 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4884 - acc: 0.4710 - top5-acc: 0.9196 - val_loss: 1.4070 - val_acc: 0.4874 - val_top5-acc: 0.9312 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4523 - acc: 0.4834 - top5-acc: 0.9225 - val_loss: 1.3674 - val_acc: 0.5070 - val_top5-acc: 0.9332 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.4473 - acc: 0.4862 - top5-acc: 0.9240 - val_loss: 1.3594 - val_acc: 0.5126 - val_top5-acc: 0.9318 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.4625 - acc: 0.4782 - top5-acc: 0.9228 - val_loss: 1.3784 - val_acc: 0.5060 - val_top5-acc: 0.9334 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4558 - acc: 0.4827 - top5-acc: 0.9218 - val_loss: 1.3801 - val_acc: 0.4972 - val_top5-acc: 0.9362 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.4495 - acc: 0.4865 - top5-acc: 0.9225 - val_loss: 1.3829 - val_acc: 0.5048 - val_top5-acc: 0.9296 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4523 - acc: 0.4824 - top5-acc: 0.9224 - val_loss: 1.3869 - val_acc: 0.4996 - val_top5-acc: 0.9332 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4533 - acc: 0.4838 - top5-acc: 0.9228 - val_loss: 1.3659 - val_acc: 0.5068 - val_top5-acc: 0.9340 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.4329 - acc: 0.4911 - top5-acc: 0.9263 - val_loss: 1.3617 - val_acc: 0.5074 - val_top5-acc: 0.9370 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4382 - acc: 0.4896 - top5-acc: 0.9258 - val_loss: 1.3629 - val_acc: 0.5068 - val_top5-acc: 0.9354 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4304 - acc: 0.4913 - top5-acc: 0.9258 - val_loss: 1.3553 - val_acc: 0.5156 - val_top5-acc: 0.9340 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4320 - acc: 0.4924 - top5-acc: 0.9245 - val_loss: 1.3590 - val_acc: 0.5104 - val_top5-acc: 0.9340 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4377 - acc: 0.4908 - top5-acc: 0.9241 - val_loss: 1.3531 - val_acc: 0.5176 - val_top5-acc: 0.9342 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4308 - acc: 0.4906 - top5-acc: 0.9270 - val_loss: 1.3573 - val_acc: 0.5142 - val_top5-acc: 0.9354 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4349 - acc: 0.4905 - top5-acc: 0.9257 - val_loss: 1.3741 - val_acc: 0.5086 - val_top5-acc: 0.9344 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4357 - acc: 0.4931 - top5-acc: 0.9246 - val_loss: 1.3548 - val_acc: 0.5102 - val_top5-acc: 0.9376 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4363 - acc: 0.4887 - top5-acc: 0.9227 - val_loss: 1.3548 - val_acc: 0.5138 - val_top5-acc: 0.9352 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4332 - acc: 0.4908 - top5-acc: 0.9253 - val_loss: 1.3602 - val_acc: 0.5138 - val_top5-acc: 0.9338 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4268 - acc: 0.4938 - top5-acc: 0.9259 - val_loss: 1.3516 - val_acc: 0.5160 - val_top5-acc: 0.9380 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4288 - acc: 0.4934 - top5-acc: 0.9254 - val_loss: 1.3586 - val_acc: 0.5132 - val_top5-acc: 0.9324 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4307 - acc: 0.4903 - top5-acc: 0.9264 - val_loss: 1.3568 - val_acc: 0.5130 - val_top5-acc: 0.9350 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4259 - acc: 0.4946 - top5-acc: 0.9255 - val_loss: 1.3603 - val_acc: 0.5154 - val_top5-acc: 0.9322 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4234 - acc: 0.4980 - top5-acc: 0.9257 - val_loss: 1.3558 - val_acc: 0.5112 - val_top5-acc: 0.9368 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4274 - acc: 0.4916 - top5-acc: 0.9253 - val_loss: 1.3569 - val_acc: 0.5166 - val_top5-acc: 0.9348 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 1.3845 - acc: 0.5102 - top5-acc: 0.9347\n",
      "Test accuracy: 51.02%\n",
      "Test top 5 accuracy: 93.47%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 8s 68ms/step - loss: 1.9104 - acc: 0.3364 - top5-acc: 0.8417 - val_loss: 1.6073 - val_acc: 0.4198 - val_top5-acc: 0.9024 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.6321 - acc: 0.4165 - top5-acc: 0.8985 - val_loss: 1.5174 - val_acc: 0.4602 - val_top5-acc: 0.9196 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5740 - acc: 0.4368 - top5-acc: 0.9053 - val_loss: 1.4988 - val_acc: 0.4558 - val_top5-acc: 0.9218 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5457 - acc: 0.4515 - top5-acc: 0.9100 - val_loss: 1.4333 - val_acc: 0.4900 - val_top5-acc: 0.9258 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.5220 - acc: 0.4601 - top5-acc: 0.9155 - val_loss: 1.4686 - val_acc: 0.4712 - val_top5-acc: 0.9306 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5162 - acc: 0.4642 - top5-acc: 0.9150 - val_loss: 1.4292 - val_acc: 0.4952 - val_top5-acc: 0.9290 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.5020 - acc: 0.4674 - top5-acc: 0.9179 - val_loss: 1.4178 - val_acc: 0.5012 - val_top5-acc: 0.9296 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4849 - acc: 0.4725 - top5-acc: 0.9181 - val_loss: 1.4148 - val_acc: 0.4962 - val_top5-acc: 0.9276 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4788 - acc: 0.4769 - top5-acc: 0.9218 - val_loss: 1.3977 - val_acc: 0.4982 - val_top5-acc: 0.9348 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4769 - acc: 0.4770 - top5-acc: 0.9202 - val_loss: 1.3954 - val_acc: 0.5008 - val_top5-acc: 0.9310 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4625 - acc: 0.4790 - top5-acc: 0.9242 - val_loss: 1.3811 - val_acc: 0.5064 - val_top5-acc: 0.9370 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4577 - acc: 0.4837 - top5-acc: 0.9232 - val_loss: 1.3561 - val_acc: 0.5172 - val_top5-acc: 0.9360 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4562 - acc: 0.4858 - top5-acc: 0.9238 - val_loss: 1.4150 - val_acc: 0.4956 - val_top5-acc: 0.9262 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4571 - acc: 0.4858 - top5-acc: 0.9246 - val_loss: 1.3680 - val_acc: 0.5130 - val_top5-acc: 0.9370 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4510 - acc: 0.4844 - top5-acc: 0.9238 - val_loss: 1.3682 - val_acc: 0.5084 - val_top5-acc: 0.9378 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4446 - acc: 0.4867 - top5-acc: 0.9248 - val_loss: 1.3564 - val_acc: 0.5188 - val_top5-acc: 0.9346 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4452 - acc: 0.4882 - top5-acc: 0.9238 - val_loss: 1.3499 - val_acc: 0.5216 - val_top5-acc: 0.9376 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4418 - acc: 0.4907 - top5-acc: 0.9252 - val_loss: 1.3195 - val_acc: 0.5330 - val_top5-acc: 0.9378 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4448 - acc: 0.4870 - top5-acc: 0.9251 - val_loss: 1.3322 - val_acc: 0.5238 - val_top5-acc: 0.9378 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4371 - acc: 0.4920 - top5-acc: 0.9269 - val_loss: 1.3440 - val_acc: 0.5198 - val_top5-acc: 0.9368 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4472 - acc: 0.4882 - top5-acc: 0.9273 - val_loss: 1.3503 - val_acc: 0.5206 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4392 - acc: 0.4926 - top5-acc: 0.9266 - val_loss: 1.3266 - val_acc: 0.5224 - val_top5-acc: 0.9426 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4358 - acc: 0.4932 - top5-acc: 0.9261 - val_loss: 1.3572 - val_acc: 0.5172 - val_top5-acc: 0.9386 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4096 - acc: 0.4997 - top5-acc: 0.9274 - val_loss: 1.3233 - val_acc: 0.5286 - val_top5-acc: 0.9412 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3975 - acc: 0.5024 - top5-acc: 0.9301 - val_loss: 1.3200 - val_acc: 0.5232 - val_top5-acc: 0.9436 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3969 - acc: 0.5052 - top5-acc: 0.9305 - val_loss: 1.3294 - val_acc: 0.5232 - val_top5-acc: 0.9366 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4014 - acc: 0.5030 - top5-acc: 0.9295 - val_loss: 1.3267 - val_acc: 0.5296 - val_top5-acc: 0.9398 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3970 - acc: 0.5031 - top5-acc: 0.9301 - val_loss: 1.3159 - val_acc: 0.5286 - val_top5-acc: 0.9400 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4021 - acc: 0.5038 - top5-acc: 0.9305 - val_loss: 1.3075 - val_acc: 0.5374 - val_top5-acc: 0.9424 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3955 - acc: 0.5058 - top5-acc: 0.9307 - val_loss: 1.3053 - val_acc: 0.5362 - val_top5-acc: 0.9428 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4025 - acc: 0.5010 - top5-acc: 0.9296 - val_loss: 1.3092 - val_acc: 0.5380 - val_top5-acc: 0.9408 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3981 - acc: 0.5037 - top5-acc: 0.9284 - val_loss: 1.3043 - val_acc: 0.5372 - val_top5-acc: 0.9422 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3939 - acc: 0.5036 - top5-acc: 0.9308 - val_loss: 1.3044 - val_acc: 0.5402 - val_top5-acc: 0.9412 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3999 - acc: 0.5016 - top5-acc: 0.9303 - val_loss: 1.3234 - val_acc: 0.5276 - val_top5-acc: 0.9404 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3972 - acc: 0.5035 - top5-acc: 0.9303 - val_loss: 1.3220 - val_acc: 0.5278 - val_top5-acc: 0.9418 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3980 - acc: 0.5051 - top5-acc: 0.9288 - val_loss: 1.3039 - val_acc: 0.5344 - val_top5-acc: 0.9448 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3964 - acc: 0.5034 - top5-acc: 0.9316 - val_loss: 1.3097 - val_acc: 0.5366 - val_top5-acc: 0.9394 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3960 - acc: 0.5073 - top5-acc: 0.9305 - val_loss: 1.3033 - val_acc: 0.5432 - val_top5-acc: 0.9410 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3974 - acc: 0.5034 - top5-acc: 0.9293 - val_loss: 1.3071 - val_acc: 0.5310 - val_top5-acc: 0.9426 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4014 - acc: 0.5026 - top5-acc: 0.9293 - val_loss: 1.3281 - val_acc: 0.5232 - val_top5-acc: 0.9388 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4012 - acc: 0.5007 - top5-acc: 0.9276 - val_loss: 1.2989 - val_acc: 0.5420 - val_top5-acc: 0.9436 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4024 - acc: 0.5013 - top5-acc: 0.9292 - val_loss: 1.3162 - val_acc: 0.5328 - val_top5-acc: 0.9382 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4010 - acc: 0.5021 - top5-acc: 0.9301 - val_loss: 1.2997 - val_acc: 0.5394 - val_top5-acc: 0.9428 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3975 - acc: 0.5071 - top5-acc: 0.9284 - val_loss: 1.3017 - val_acc: 0.5394 - val_top5-acc: 0.9450 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3926 - acc: 0.5065 - top5-acc: 0.9301 - val_loss: 1.3047 - val_acc: 0.5350 - val_top5-acc: 0.9424 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3920 - acc: 0.5087 - top5-acc: 0.9311 - val_loss: 1.2939 - val_acc: 0.5408 - val_top5-acc: 0.9432 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3934 - acc: 0.5085 - top5-acc: 0.9311 - val_loss: 1.3078 - val_acc: 0.5274 - val_top5-acc: 0.9416 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3978 - acc: 0.5051 - top5-acc: 0.9310 - val_loss: 1.3165 - val_acc: 0.5278 - val_top5-acc: 0.9398 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4009 - acc: 0.5034 - top5-acc: 0.9295 - val_loss: 1.3215 - val_acc: 0.5242 - val_top5-acc: 0.9408 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.3908 - acc: 0.5066 - top5-acc: 0.9311 - val_loss: 1.3081 - val_acc: 0.5372 - val_top5-acc: 0.9398 - lr: 0.0025\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 1.3306 - acc: 0.5286 - top5-acc: 0.9397\n",
      "Test accuracy: 52.86%\n",
      "Test top 5 accuracy: 93.97%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 9s 76ms/step - loss: 1.8608 - acc: 0.3489 - top5-acc: 0.8502 - val_loss: 1.5784 - val_acc: 0.4282 - val_top5-acc: 0.9108 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.6049 - acc: 0.4302 - top5-acc: 0.9027 - val_loss: 1.5223 - val_acc: 0.4550 - val_top5-acc: 0.9208 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.5597 - acc: 0.4458 - top5-acc: 0.9086 - val_loss: 1.4478 - val_acc: 0.4898 - val_top5-acc: 0.9282 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.5147 - acc: 0.4620 - top5-acc: 0.9159 - val_loss: 1.4234 - val_acc: 0.4966 - val_top5-acc: 0.9336 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.5082 - acc: 0.4660 - top5-acc: 0.9164 - val_loss: 1.4171 - val_acc: 0.4922 - val_top5-acc: 0.9314 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4852 - acc: 0.4736 - top5-acc: 0.9188 - val_loss: 1.4074 - val_acc: 0.5072 - val_top5-acc: 0.9304 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4637 - acc: 0.4816 - top5-acc: 0.9210 - val_loss: 1.3862 - val_acc: 0.5072 - val_top5-acc: 0.9326 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4675 - acc: 0.4792 - top5-acc: 0.9226 - val_loss: 1.3799 - val_acc: 0.5050 - val_top5-acc: 0.9334 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4595 - acc: 0.4855 - top5-acc: 0.9230 - val_loss: 1.3801 - val_acc: 0.5092 - val_top5-acc: 0.9360 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 6s 71ms/step - loss: 1.4453 - acc: 0.4905 - top5-acc: 0.9232 - val_loss: 1.3562 - val_acc: 0.5174 - val_top5-acc: 0.9416 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 6s 71ms/step - loss: 1.4415 - acc: 0.4906 - top5-acc: 0.9244 - val_loss: 1.3726 - val_acc: 0.5158 - val_top5-acc: 0.9338 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4543 - acc: 0.4886 - top5-acc: 0.9226 - val_loss: 1.3560 - val_acc: 0.5222 - val_top5-acc: 0.9344 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4440 - acc: 0.4897 - top5-acc: 0.9240 - val_loss: 1.3512 - val_acc: 0.5226 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4376 - acc: 0.4890 - top5-acc: 0.9260 - val_loss: 1.3759 - val_acc: 0.5154 - val_top5-acc: 0.9348 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4279 - acc: 0.4940 - top5-acc: 0.9272 - val_loss: 1.3664 - val_acc: 0.5124 - val_top5-acc: 0.9340 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4369 - acc: 0.4909 - top5-acc: 0.9259 - val_loss: 1.3549 - val_acc: 0.5220 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4254 - acc: 0.4956 - top5-acc: 0.9272 - val_loss: 1.3173 - val_acc: 0.5328 - val_top5-acc: 0.9448 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4252 - acc: 0.4964 - top5-acc: 0.9271 - val_loss: 1.3391 - val_acc: 0.5244 - val_top5-acc: 0.9406 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4300 - acc: 0.4958 - top5-acc: 0.9262 - val_loss: 1.3249 - val_acc: 0.5300 - val_top5-acc: 0.9380 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4243 - acc: 0.4969 - top5-acc: 0.9286 - val_loss: 1.3093 - val_acc: 0.5366 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4200 - acc: 0.4973 - top5-acc: 0.9293 - val_loss: 1.3166 - val_acc: 0.5394 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4149 - acc: 0.5001 - top5-acc: 0.9288 - val_loss: 1.3376 - val_acc: 0.5210 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4211 - acc: 0.4964 - top5-acc: 0.9287 - val_loss: 1.3305 - val_acc: 0.5276 - val_top5-acc: 0.9386 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4084 - acc: 0.5008 - top5-acc: 0.9294 - val_loss: 1.3420 - val_acc: 0.5264 - val_top5-acc: 0.9422 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4086 - acc: 0.5010 - top5-acc: 0.9295 - val_loss: 1.3010 - val_acc: 0.5398 - val_top5-acc: 0.9450 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 6s 71ms/step - loss: 1.4121 - acc: 0.4990 - top5-acc: 0.9267 - val_loss: 1.3070 - val_acc: 0.5402 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4075 - acc: 0.5003 - top5-acc: 0.9286 - val_loss: 1.2959 - val_acc: 0.5448 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4107 - acc: 0.4998 - top5-acc: 0.9310 - val_loss: 1.3253 - val_acc: 0.5346 - val_top5-acc: 0.9422 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4139 - acc: 0.5000 - top5-acc: 0.9300 - val_loss: 1.3196 - val_acc: 0.5354 - val_top5-acc: 0.9374 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4114 - acc: 0.5010 - top5-acc: 0.9281 - val_loss: 1.3515 - val_acc: 0.5160 - val_top5-acc: 0.9360 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4082 - acc: 0.4996 - top5-acc: 0.9299 - val_loss: 1.3135 - val_acc: 0.5334 - val_top5-acc: 0.9404 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4077 - acc: 0.5012 - top5-acc: 0.9294 - val_loss: 1.3118 - val_acc: 0.5400 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3785 - acc: 0.5128 - top5-acc: 0.9312 - val_loss: 1.2802 - val_acc: 0.5496 - val_top5-acc: 0.9452 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3777 - acc: 0.5121 - top5-acc: 0.9334 - val_loss: 1.3087 - val_acc: 0.5464 - val_top5-acc: 0.9388 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3847 - acc: 0.5068 - top5-acc: 0.9308 - val_loss: 1.3057 - val_acc: 0.5478 - val_top5-acc: 0.9434 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3837 - acc: 0.5080 - top5-acc: 0.9330 - val_loss: 1.2796 - val_acc: 0.5502 - val_top5-acc: 0.9432 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3767 - acc: 0.5092 - top5-acc: 0.9336 - val_loss: 1.2901 - val_acc: 0.5516 - val_top5-acc: 0.9454 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3780 - acc: 0.5116 - top5-acc: 0.9319 - val_loss: 1.2764 - val_acc: 0.5526 - val_top5-acc: 0.9482 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3751 - acc: 0.5103 - top5-acc: 0.9338 - val_loss: 1.2836 - val_acc: 0.5476 - val_top5-acc: 0.9464 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 6s 71ms/step - loss: 1.3736 - acc: 0.5093 - top5-acc: 0.9324 - val_loss: 1.2833 - val_acc: 0.5532 - val_top5-acc: 0.9442 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 6s 71ms/step - loss: 1.3789 - acc: 0.5108 - top5-acc: 0.9318 - val_loss: 1.2856 - val_acc: 0.5466 - val_top5-acc: 0.9460 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 6s 71ms/step - loss: 1.3783 - acc: 0.5092 - top5-acc: 0.9321 - val_loss: 1.2826 - val_acc: 0.5522 - val_top5-acc: 0.9460 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3780 - acc: 0.5113 - top5-acc: 0.9334 - val_loss: 1.2934 - val_acc: 0.5492 - val_top5-acc: 0.9470 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3677 - acc: 0.5131 - top5-acc: 0.9335 - val_loss: 1.2716 - val_acc: 0.5584 - val_top5-acc: 0.9470 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3613 - acc: 0.5182 - top5-acc: 0.9333 - val_loss: 1.2741 - val_acc: 0.5548 - val_top5-acc: 0.9470 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3658 - acc: 0.5164 - top5-acc: 0.9326 - val_loss: 1.2770 - val_acc: 0.5560 - val_top5-acc: 0.9458 - lr: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3659 - acc: 0.5159 - top5-acc: 0.9336 - val_loss: 1.2789 - val_acc: 0.5540 - val_top5-acc: 0.9452 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3693 - acc: 0.5166 - top5-acc: 0.9332 - val_loss: 1.2798 - val_acc: 0.5568 - val_top5-acc: 0.9460 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3639 - acc: 0.5173 - top5-acc: 0.9335 - val_loss: 1.2745 - val_acc: 0.5622 - val_top5-acc: 0.9460 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3530 - acc: 0.5223 - top5-acc: 0.9339 - val_loss: 1.2725 - val_acc: 0.5600 - val_top5-acc: 0.9456 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 1.2916 - acc: 0.5469 - top5-acc: 0.9441\n",
      "Test accuracy: 54.69%\n",
      "Test top 5 accuracy: 94.41%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 10s 84ms/step - loss: 1.8660 - acc: 0.3459 - top5-acc: 0.8446 - val_loss: 1.5747 - val_acc: 0.4238 - val_top5-acc: 0.9134 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.5965 - acc: 0.4350 - top5-acc: 0.9027 - val_loss: 1.4874 - val_acc: 0.4744 - val_top5-acc: 0.9158 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.5507 - acc: 0.4461 - top5-acc: 0.9089 - val_loss: 1.4436 - val_acc: 0.4918 - val_top5-acc: 0.9232 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.5108 - acc: 0.4646 - top5-acc: 0.9151 - val_loss: 1.4170 - val_acc: 0.5088 - val_top5-acc: 0.9308 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4996 - acc: 0.4687 - top5-acc: 0.9165 - val_loss: 1.4144 - val_acc: 0.5002 - val_top5-acc: 0.9302 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4802 - acc: 0.4762 - top5-acc: 0.9193 - val_loss: 1.3682 - val_acc: 0.5146 - val_top5-acc: 0.9374 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4736 - acc: 0.4799 - top5-acc: 0.9189 - val_loss: 1.3736 - val_acc: 0.5140 - val_top5-acc: 0.9352 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4545 - acc: 0.4849 - top5-acc: 0.9233 - val_loss: 1.3520 - val_acc: 0.5184 - val_top5-acc: 0.9376 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4557 - acc: 0.4862 - top5-acc: 0.9232 - val_loss: 1.3651 - val_acc: 0.5202 - val_top5-acc: 0.9362 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4454 - acc: 0.4869 - top5-acc: 0.9245 - val_loss: 1.3615 - val_acc: 0.5240 - val_top5-acc: 0.9384 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4432 - acc: 0.4926 - top5-acc: 0.9231 - val_loss: 1.3439 - val_acc: 0.5174 - val_top5-acc: 0.9376 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4326 - acc: 0.4929 - top5-acc: 0.9256 - val_loss: 1.3336 - val_acc: 0.5306 - val_top5-acc: 0.9404 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4437 - acc: 0.4907 - top5-acc: 0.9236 - val_loss: 1.3586 - val_acc: 0.5174 - val_top5-acc: 0.9394 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4290 - acc: 0.4937 - top5-acc: 0.9276 - val_loss: 1.3321 - val_acc: 0.5314 - val_top5-acc: 0.9352 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4288 - acc: 0.4930 - top5-acc: 0.9257 - val_loss: 1.3458 - val_acc: 0.5230 - val_top5-acc: 0.9396 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4238 - acc: 0.4941 - top5-acc: 0.9272 - val_loss: 1.3323 - val_acc: 0.5312 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4214 - acc: 0.4973 - top5-acc: 0.9276 - val_loss: 1.3323 - val_acc: 0.5290 - val_top5-acc: 0.9392 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4156 - acc: 0.5037 - top5-acc: 0.9271 - val_loss: 1.3315 - val_acc: 0.5238 - val_top5-acc: 0.9366 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4319 - acc: 0.4939 - top5-acc: 0.9266 - val_loss: 1.3520 - val_acc: 0.5254 - val_top5-acc: 0.9372 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4141 - acc: 0.5018 - top5-acc: 0.9267 - val_loss: 1.3115 - val_acc: 0.5376 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4131 - acc: 0.4996 - top5-acc: 0.9290 - val_loss: 1.3065 - val_acc: 0.5380 - val_top5-acc: 0.9422 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4093 - acc: 0.5012 - top5-acc: 0.9281 - val_loss: 1.3437 - val_acc: 0.5262 - val_top5-acc: 0.9372 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4270 - acc: 0.4936 - top5-acc: 0.9294 - val_loss: 1.3256 - val_acc: 0.5338 - val_top5-acc: 0.9386 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4142 - acc: 0.4972 - top5-acc: 0.9290 - val_loss: 1.3124 - val_acc: 0.5346 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4081 - acc: 0.5010 - top5-acc: 0.9298 - val_loss: 1.3299 - val_acc: 0.5318 - val_top5-acc: 0.9366 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4068 - acc: 0.4998 - top5-acc: 0.9298 - val_loss: 1.3271 - val_acc: 0.5270 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3816 - acc: 0.5117 - top5-acc: 0.9302 - val_loss: 1.2818 - val_acc: 0.5504 - val_top5-acc: 0.9438 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3742 - acc: 0.5124 - top5-acc: 0.9314 - val_loss: 1.2839 - val_acc: 0.5478 - val_top5-acc: 0.9436 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3777 - acc: 0.5106 - top5-acc: 0.9320 - val_loss: 1.2757 - val_acc: 0.5534 - val_top5-acc: 0.9454 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3774 - acc: 0.5121 - top5-acc: 0.9303 - val_loss: 1.2783 - val_acc: 0.5532 - val_top5-acc: 0.9464 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3779 - acc: 0.5144 - top5-acc: 0.9315 - val_loss: 1.2825 - val_acc: 0.5542 - val_top5-acc: 0.9412 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3719 - acc: 0.5152 - top5-acc: 0.9334 - val_loss: 1.2850 - val_acc: 0.5494 - val_top5-acc: 0.9426 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3809 - acc: 0.5120 - top5-acc: 0.9317 - val_loss: 1.2823 - val_acc: 0.5436 - val_top5-acc: 0.9438 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3746 - acc: 0.5130 - top5-acc: 0.9320 - val_loss: 1.2945 - val_acc: 0.5406 - val_top5-acc: 0.9456 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3662 - acc: 0.5168 - top5-acc: 0.9328 - val_loss: 1.2707 - val_acc: 0.5590 - val_top5-acc: 0.9450 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3636 - acc: 0.5177 - top5-acc: 0.9340 - val_loss: 1.2675 - val_acc: 0.5574 - val_top5-acc: 0.9456 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3664 - acc: 0.5162 - top5-acc: 0.9334 - val_loss: 1.2793 - val_acc: 0.5548 - val_top5-acc: 0.9430 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3640 - acc: 0.5173 - top5-acc: 0.9330 - val_loss: 1.2708 - val_acc: 0.5582 - val_top5-acc: 0.9466 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3649 - acc: 0.5158 - top5-acc: 0.9347 - val_loss: 1.2686 - val_acc: 0.5580 - val_top5-acc: 0.9450 - lr: 0.0012\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3632 - acc: 0.5176 - top5-acc: 0.9351 - val_loss: 1.2737 - val_acc: 0.5560 - val_top5-acc: 0.9442 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3646 - acc: 0.5167 - top5-acc: 0.9330 - val_loss: 1.2701 - val_acc: 0.5610 - val_top5-acc: 0.9480 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3554 - acc: 0.5200 - top5-acc: 0.9339 - val_loss: 1.2661 - val_acc: 0.5620 - val_top5-acc: 0.9484 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3642 - acc: 0.5194 - top5-acc: 0.9336 - val_loss: 1.2703 - val_acc: 0.5596 - val_top5-acc: 0.9482 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3601 - acc: 0.5214 - top5-acc: 0.9345 - val_loss: 1.2721 - val_acc: 0.5560 - val_top5-acc: 0.9472 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3575 - acc: 0.5194 - top5-acc: 0.9348 - val_loss: 1.2673 - val_acc: 0.5620 - val_top5-acc: 0.9464 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3622 - acc: 0.5198 - top5-acc: 0.9342 - val_loss: 1.2667 - val_acc: 0.5612 - val_top5-acc: 0.9452 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3636 - acc: 0.5186 - top5-acc: 0.9334 - val_loss: 1.2745 - val_acc: 0.5568 - val_top5-acc: 0.9440 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3576 - acc: 0.5214 - top5-acc: 0.9338 - val_loss: 1.2686 - val_acc: 0.5632 - val_top5-acc: 0.9462 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3593 - acc: 0.5213 - top5-acc: 0.9319 - val_loss: 1.2670 - val_acc: 0.5652 - val_top5-acc: 0.9472 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3574 - acc: 0.5208 - top5-acc: 0.9366 - val_loss: 1.2734 - val_acc: 0.5610 - val_top5-acc: 0.9466 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 1.2936 - acc: 0.5500 - top5-acc: 0.9446\n",
      "Test accuracy: 55.0%\n",
      "Test top 5 accuracy: 94.46%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 11s 92ms/step - loss: 1.8573 - acc: 0.3468 - top5-acc: 0.8447 - val_loss: 1.5699 - val_acc: 0.4368 - val_top5-acc: 0.9100 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.5922 - acc: 0.4338 - top5-acc: 0.9022 - val_loss: 1.4958 - val_acc: 0.4610 - val_top5-acc: 0.9224 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.5403 - acc: 0.4502 - top5-acc: 0.9118 - val_loss: 1.4347 - val_acc: 0.4892 - val_top5-acc: 0.9304 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.5016 - acc: 0.4674 - top5-acc: 0.9169 - val_loss: 1.4490 - val_acc: 0.4816 - val_top5-acc: 0.9266 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4851 - acc: 0.4737 - top5-acc: 0.9196 - val_loss: 1.3879 - val_acc: 0.5084 - val_top5-acc: 0.9356 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4743 - acc: 0.4774 - top5-acc: 0.9215 - val_loss: 1.3678 - val_acc: 0.5168 - val_top5-acc: 0.9326 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4518 - acc: 0.4877 - top5-acc: 0.9237 - val_loss: 1.3878 - val_acc: 0.5072 - val_top5-acc: 0.9328 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4536 - acc: 0.4835 - top5-acc: 0.9226 - val_loss: 1.3382 - val_acc: 0.5232 - val_top5-acc: 0.9412 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4496 - acc: 0.4872 - top5-acc: 0.9244 - val_loss: 1.3520 - val_acc: 0.5210 - val_top5-acc: 0.9336 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.4295 - acc: 0.4943 - top5-acc: 0.9267 - val_loss: 1.3403 - val_acc: 0.5248 - val_top5-acc: 0.9362 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4277 - acc: 0.4928 - top5-acc: 0.9269 - val_loss: 1.3406 - val_acc: 0.5200 - val_top5-acc: 0.9370 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4206 - acc: 0.4963 - top5-acc: 0.9275 - val_loss: 1.3306 - val_acc: 0.5306 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.4264 - acc: 0.4959 - top5-acc: 0.9266 - val_loss: 1.3382 - val_acc: 0.5268 - val_top5-acc: 0.9372 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.4239 - acc: 0.4953 - top5-acc: 0.9251 - val_loss: 1.3503 - val_acc: 0.5238 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4135 - acc: 0.4993 - top5-acc: 0.9273 - val_loss: 1.3053 - val_acc: 0.5366 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4081 - acc: 0.5045 - top5-acc: 0.9278 - val_loss: 1.3152 - val_acc: 0.5400 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4096 - acc: 0.5005 - top5-acc: 0.9293 - val_loss: 1.2969 - val_acc: 0.5406 - val_top5-acc: 0.9394 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4084 - acc: 0.5031 - top5-acc: 0.9300 - val_loss: 1.3227 - val_acc: 0.5296 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4088 - acc: 0.5017 - top5-acc: 0.9274 - val_loss: 1.3210 - val_acc: 0.5302 - val_top5-acc: 0.9372 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4021 - acc: 0.5068 - top5-acc: 0.9271 - val_loss: 1.3244 - val_acc: 0.5232 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4093 - acc: 0.5030 - top5-acc: 0.9288 - val_loss: 1.3072 - val_acc: 0.5376 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.4022 - acc: 0.5051 - top5-acc: 0.9300 - val_loss: 1.3161 - val_acc: 0.5412 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3796 - acc: 0.5136 - top5-acc: 0.9316 - val_loss: 1.2818 - val_acc: 0.5524 - val_top5-acc: 0.9438 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3778 - acc: 0.5134 - top5-acc: 0.9318 - val_loss: 1.2872 - val_acc: 0.5442 - val_top5-acc: 0.9442 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3738 - acc: 0.5150 - top5-acc: 0.9322 - val_loss: 1.2792 - val_acc: 0.5476 - val_top5-acc: 0.9432 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3829 - acc: 0.5103 - top5-acc: 0.9318 - val_loss: 1.2918 - val_acc: 0.5436 - val_top5-acc: 0.9464 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3768 - acc: 0.5130 - top5-acc: 0.9324 - val_loss: 1.2830 - val_acc: 0.5466 - val_top5-acc: 0.9436 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3754 - acc: 0.5137 - top5-acc: 0.9317 - val_loss: 1.2863 - val_acc: 0.5448 - val_top5-acc: 0.9410 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3744 - acc: 0.5119 - top5-acc: 0.9328 - val_loss: 1.2870 - val_acc: 0.5436 - val_top5-acc: 0.9456 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3648 - acc: 0.5171 - top5-acc: 0.9338 - val_loss: 1.2897 - val_acc: 0.5364 - val_top5-acc: 0.9442 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3613 - acc: 0.5171 - top5-acc: 0.9333 - val_loss: 1.2742 - val_acc: 0.5520 - val_top5-acc: 0.9466 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3601 - acc: 0.5200 - top5-acc: 0.9345 - val_loss: 1.2705 - val_acc: 0.5540 - val_top5-acc: 0.9440 - lr: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3588 - acc: 0.5207 - top5-acc: 0.9346 - val_loss: 1.2740 - val_acc: 0.5478 - val_top5-acc: 0.9432 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3602 - acc: 0.5202 - top5-acc: 0.9333 - val_loss: 1.2615 - val_acc: 0.5648 - val_top5-acc: 0.9466 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3653 - acc: 0.5169 - top5-acc: 0.9330 - val_loss: 1.2684 - val_acc: 0.5554 - val_top5-acc: 0.9460 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3655 - acc: 0.5170 - top5-acc: 0.9330 - val_loss: 1.2672 - val_acc: 0.5566 - val_top5-acc: 0.9460 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3560 - acc: 0.5202 - top5-acc: 0.9328 - val_loss: 1.2605 - val_acc: 0.5590 - val_top5-acc: 0.9464 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3603 - acc: 0.5169 - top5-acc: 0.9340 - val_loss: 1.2704 - val_acc: 0.5582 - val_top5-acc: 0.9432 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3638 - acc: 0.5198 - top5-acc: 0.9329 - val_loss: 1.2833 - val_acc: 0.5466 - val_top5-acc: 0.9448 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3617 - acc: 0.5204 - top5-acc: 0.9351 - val_loss: 1.2658 - val_acc: 0.5598 - val_top5-acc: 0.9446 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3672 - acc: 0.5182 - top5-acc: 0.9325 - val_loss: 1.2700 - val_acc: 0.5492 - val_top5-acc: 0.9452 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3644 - acc: 0.5192 - top5-acc: 0.9325 - val_loss: 1.2640 - val_acc: 0.5600 - val_top5-acc: 0.9444 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3564 - acc: 0.5194 - top5-acc: 0.9333 - val_loss: 1.2679 - val_acc: 0.5570 - val_top5-acc: 0.9442 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3552 - acc: 0.5221 - top5-acc: 0.9350 - val_loss: 1.2643 - val_acc: 0.5628 - val_top5-acc: 0.9440 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3581 - acc: 0.5230 - top5-acc: 0.9344 - val_loss: 1.2670 - val_acc: 0.5600 - val_top5-acc: 0.9470 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3551 - acc: 0.5217 - top5-acc: 0.9353 - val_loss: 1.2726 - val_acc: 0.5578 - val_top5-acc: 0.9444 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3602 - acc: 0.5182 - top5-acc: 0.9340 - val_loss: 1.2648 - val_acc: 0.5600 - val_top5-acc: 0.9446 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3556 - acc: 0.5215 - top5-acc: 0.9334 - val_loss: 1.2693 - val_acc: 0.5576 - val_top5-acc: 0.9452 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3562 - acc: 0.5238 - top5-acc: 0.9337 - val_loss: 1.2734 - val_acc: 0.5578 - val_top5-acc: 0.9436 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3508 - acc: 0.5288 - top5-acc: 0.9355 - val_loss: 1.2660 - val_acc: 0.5608 - val_top5-acc: 0.9434 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.2961 - acc: 0.5497 - top5-acc: 0.9435\n",
      "Test accuracy: 54.97%\n",
      "Test top 5 accuracy: 94.35%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 12s 101ms/step - loss: 1.8308 - acc: 0.3502 - top5-acc: 0.8526 - val_loss: 1.5754 - val_acc: 0.4304 - val_top5-acc: 0.9068 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.5878 - acc: 0.4352 - top5-acc: 0.9045 - val_loss: 1.4784 - val_acc: 0.4752 - val_top5-acc: 0.9232 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.5375 - acc: 0.4512 - top5-acc: 0.9134 - val_loss: 1.4424 - val_acc: 0.4854 - val_top5-acc: 0.9282 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.5047 - acc: 0.4660 - top5-acc: 0.9149 - val_loss: 1.4022 - val_acc: 0.4980 - val_top5-acc: 0.9306 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4890 - acc: 0.4755 - top5-acc: 0.9186 - val_loss: 1.4045 - val_acc: 0.4976 - val_top5-acc: 0.9308 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4736 - acc: 0.4774 - top5-acc: 0.9207 - val_loss: 1.3938 - val_acc: 0.4976 - val_top5-acc: 0.9296 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4632 - acc: 0.4841 - top5-acc: 0.9227 - val_loss: 1.3696 - val_acc: 0.5182 - val_top5-acc: 0.9316 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4485 - acc: 0.4894 - top5-acc: 0.9245 - val_loss: 1.3504 - val_acc: 0.5188 - val_top5-acc: 0.9350 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4446 - acc: 0.4890 - top5-acc: 0.9235 - val_loss: 1.3914 - val_acc: 0.5090 - val_top5-acc: 0.9296 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4400 - acc: 0.4915 - top5-acc: 0.9241 - val_loss: 1.3553 - val_acc: 0.5324 - val_top5-acc: 0.9364 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4280 - acc: 0.4936 - top5-acc: 0.9267 - val_loss: 1.3684 - val_acc: 0.5198 - val_top5-acc: 0.9308 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.4335 - acc: 0.4955 - top5-acc: 0.9249 - val_loss: 1.3376 - val_acc: 0.5268 - val_top5-acc: 0.9360 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.4177 - acc: 0.4974 - top5-acc: 0.9290 - val_loss: 1.3615 - val_acc: 0.5156 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4336 - acc: 0.4920 - top5-acc: 0.9250 - val_loss: 1.3542 - val_acc: 0.5194 - val_top5-acc: 0.9324 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4206 - acc: 0.4979 - top5-acc: 0.9283 - val_loss: 1.3507 - val_acc: 0.5224 - val_top5-acc: 0.9344 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4163 - acc: 0.5008 - top5-acc: 0.9278 - val_loss: 1.3160 - val_acc: 0.5398 - val_top5-acc: 0.9446 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4102 - acc: 0.5019 - top5-acc: 0.9295 - val_loss: 1.3139 - val_acc: 0.5366 - val_top5-acc: 0.9402 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4028 - acc: 0.5040 - top5-acc: 0.9292 - val_loss: 1.3152 - val_acc: 0.5436 - val_top5-acc: 0.9396 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4104 - acc: 0.5022 - top5-acc: 0.9293 - val_loss: 1.3285 - val_acc: 0.5268 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4163 - acc: 0.5024 - top5-acc: 0.9268 - val_loss: 1.3036 - val_acc: 0.5362 - val_top5-acc: 0.9436 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4099 - acc: 0.5013 - top5-acc: 0.9298 - val_loss: 1.3055 - val_acc: 0.5436 - val_top5-acc: 0.9396 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3983 - acc: 0.5066 - top5-acc: 0.9307 - val_loss: 1.3361 - val_acc: 0.5236 - val_top5-acc: 0.9412 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4027 - acc: 0.5047 - top5-acc: 0.9300 - val_loss: 1.3348 - val_acc: 0.5208 - val_top5-acc: 0.9424 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.4093 - acc: 0.5020 - top5-acc: 0.9296 - val_loss: 1.3143 - val_acc: 0.5318 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.3978 - acc: 0.5064 - top5-acc: 0.9300 - val_loss: 1.3042 - val_acc: 0.5374 - val_top5-acc: 0.9394 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3744 - acc: 0.5181 - top5-acc: 0.9325 - val_loss: 1.2880 - val_acc: 0.5452 - val_top5-acc: 0.9446 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3704 - acc: 0.5141 - top5-acc: 0.9327 - val_loss: 1.2747 - val_acc: 0.5474 - val_top5-acc: 0.9438 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3757 - acc: 0.5136 - top5-acc: 0.9327 - val_loss: 1.2775 - val_acc: 0.5528 - val_top5-acc: 0.9436 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3751 - acc: 0.5113 - top5-acc: 0.9325 - val_loss: 1.2800 - val_acc: 0.5494 - val_top5-acc: 0.9446 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3725 - acc: 0.5144 - top5-acc: 0.9318 - val_loss: 1.2943 - val_acc: 0.5408 - val_top5-acc: 0.9432 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3756 - acc: 0.5132 - top5-acc: 0.9331 - val_loss: 1.2921 - val_acc: 0.5484 - val_top5-acc: 0.9416 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3742 - acc: 0.5168 - top5-acc: 0.9327 - val_loss: 1.2754 - val_acc: 0.5496 - val_top5-acc: 0.9422 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3638 - acc: 0.5181 - top5-acc: 0.9328 - val_loss: 1.2723 - val_acc: 0.5566 - val_top5-acc: 0.9444 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3596 - acc: 0.5181 - top5-acc: 0.9336 - val_loss: 1.2701 - val_acc: 0.5576 - val_top5-acc: 0.9434 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3612 - acc: 0.5202 - top5-acc: 0.9330 - val_loss: 1.2710 - val_acc: 0.5514 - val_top5-acc: 0.9442 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3632 - acc: 0.5189 - top5-acc: 0.9315 - val_loss: 1.2738 - val_acc: 0.5508 - val_top5-acc: 0.9438 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3600 - acc: 0.5206 - top5-acc: 0.9349 - val_loss: 1.2764 - val_acc: 0.5532 - val_top5-acc: 0.9452 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3619 - acc: 0.5200 - top5-acc: 0.9318 - val_loss: 1.2751 - val_acc: 0.5508 - val_top5-acc: 0.9422 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3609 - acc: 0.5186 - top5-acc: 0.9340 - val_loss: 1.2720 - val_acc: 0.5494 - val_top5-acc: 0.9448 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3591 - acc: 0.5205 - top5-acc: 0.9352 - val_loss: 1.2683 - val_acc: 0.5602 - val_top5-acc: 0.9434 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3579 - acc: 0.5219 - top5-acc: 0.9343 - val_loss: 1.2646 - val_acc: 0.5648 - val_top5-acc: 0.9428 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3566 - acc: 0.5208 - top5-acc: 0.9340 - val_loss: 1.2693 - val_acc: 0.5556 - val_top5-acc: 0.9442 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3571 - acc: 0.5228 - top5-acc: 0.9342 - val_loss: 1.2704 - val_acc: 0.5566 - val_top5-acc: 0.9450 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3597 - acc: 0.5228 - top5-acc: 0.9335 - val_loss: 1.2706 - val_acc: 0.5598 - val_top5-acc: 0.9432 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3594 - acc: 0.5221 - top5-acc: 0.9336 - val_loss: 1.2763 - val_acc: 0.5534 - val_top5-acc: 0.9442 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3580 - acc: 0.5206 - top5-acc: 0.9333 - val_loss: 1.2735 - val_acc: 0.5590 - val_top5-acc: 0.9420 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3583 - acc: 0.5224 - top5-acc: 0.9344 - val_loss: 1.2666 - val_acc: 0.5616 - val_top5-acc: 0.9424 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3601 - acc: 0.5215 - top5-acc: 0.9331 - val_loss: 1.2682 - val_acc: 0.5572 - val_top5-acc: 0.9442 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3588 - acc: 0.5211 - top5-acc: 0.9351 - val_loss: 1.2700 - val_acc: 0.5608 - val_top5-acc: 0.9444 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3564 - acc: 0.5250 - top5-acc: 0.9334 - val_loss: 1.2713 - val_acc: 0.5550 - val_top5-acc: 0.9436 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.3019 - acc: 0.5464 - top5-acc: 0.9439\n",
      "Test accuracy: 54.64%\n",
      "Test top 5 accuracy: 94.39%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 12s 108ms/step - loss: 1.8290 - acc: 0.3552 - top5-acc: 0.8494 - val_loss: 1.5546 - val_acc: 0.4406 - val_top5-acc: 0.9116 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.5844 - acc: 0.4370 - top5-acc: 0.9028 - val_loss: 1.4667 - val_acc: 0.4726 - val_top5-acc: 0.9232 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.5198 - acc: 0.4630 - top5-acc: 0.9138 - val_loss: 1.4226 - val_acc: 0.4940 - val_top5-acc: 0.9324 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.4948 - acc: 0.4721 - top5-acc: 0.9178 - val_loss: 1.4205 - val_acc: 0.4970 - val_top5-acc: 0.9274 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.4701 - acc: 0.4810 - top5-acc: 0.9196 - val_loss: 1.3707 - val_acc: 0.5162 - val_top5-acc: 0.9354 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.4501 - acc: 0.4882 - top5-acc: 0.9226 - val_loss: 1.3684 - val_acc: 0.5220 - val_top5-acc: 0.9364 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.4523 - acc: 0.4861 - top5-acc: 0.9242 - val_loss: 1.3430 - val_acc: 0.5264 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.4350 - acc: 0.4976 - top5-acc: 0.9240 - val_loss: 1.3516 - val_acc: 0.5242 - val_top5-acc: 0.9386 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.4159 - acc: 0.5012 - top5-acc: 0.9248 - val_loss: 1.3544 - val_acc: 0.5142 - val_top5-acc: 0.9372 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.4179 - acc: 0.4979 - top5-acc: 0.9275 - val_loss: 1.3065 - val_acc: 0.5350 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.4081 - acc: 0.5030 - top5-acc: 0.9275 - val_loss: 1.3284 - val_acc: 0.5286 - val_top5-acc: 0.9406 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.4013 - acc: 0.5026 - top5-acc: 0.9279 - val_loss: 1.3205 - val_acc: 0.5262 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.4043 - acc: 0.5043 - top5-acc: 0.9290 - val_loss: 1.2877 - val_acc: 0.5476 - val_top5-acc: 0.9426 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.4067 - acc: 0.5007 - top5-acc: 0.9292 - val_loss: 1.3390 - val_acc: 0.5142 - val_top5-acc: 0.9396 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.4037 - acc: 0.5027 - top5-acc: 0.9293 - val_loss: 1.3142 - val_acc: 0.5400 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3932 - acc: 0.5084 - top5-acc: 0.9293 - val_loss: 1.3116 - val_acc: 0.5312 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3879 - acc: 0.5119 - top5-acc: 0.9303 - val_loss: 1.2933 - val_acc: 0.5362 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3970 - acc: 0.5076 - top5-acc: 0.9287 - val_loss: 1.2901 - val_acc: 0.5458 - val_top5-acc: 0.9444 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3678 - acc: 0.5176 - top5-acc: 0.9322 - val_loss: 1.2679 - val_acc: 0.5536 - val_top5-acc: 0.9432 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.3642 - acc: 0.5223 - top5-acc: 0.9312 - val_loss: 1.2753 - val_acc: 0.5458 - val_top5-acc: 0.9454 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.3662 - acc: 0.5186 - top5-acc: 0.9326 - val_loss: 1.2778 - val_acc: 0.5424 - val_top5-acc: 0.9462 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.3624 - acc: 0.5204 - top5-acc: 0.9332 - val_loss: 1.2814 - val_acc: 0.5450 - val_top5-acc: 0.9444 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.3650 - acc: 0.5194 - top5-acc: 0.9312 - val_loss: 1.2642 - val_acc: 0.5540 - val_top5-acc: 0.9438 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3674 - acc: 0.5180 - top5-acc: 0.9316 - val_loss: 1.2724 - val_acc: 0.5476 - val_top5-acc: 0.9448 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3582 - acc: 0.5203 - top5-acc: 0.9331 - val_loss: 1.2859 - val_acc: 0.5428 - val_top5-acc: 0.9444 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3636 - acc: 0.5206 - top5-acc: 0.9334 - val_loss: 1.2701 - val_acc: 0.5482 - val_top5-acc: 0.9446 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3715 - acc: 0.5149 - top5-acc: 0.9314 - val_loss: 1.2675 - val_acc: 0.5514 - val_top5-acc: 0.9460 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3584 - acc: 0.5197 - top5-acc: 0.9335 - val_loss: 1.2716 - val_acc: 0.5554 - val_top5-acc: 0.9434 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3495 - acc: 0.5249 - top5-acc: 0.9343 - val_loss: 1.2578 - val_acc: 0.5588 - val_top5-acc: 0.9486 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3455 - acc: 0.5255 - top5-acc: 0.9359 - val_loss: 1.2552 - val_acc: 0.5612 - val_top5-acc: 0.9452 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3494 - acc: 0.5276 - top5-acc: 0.9349 - val_loss: 1.2548 - val_acc: 0.5550 - val_top5-acc: 0.9456 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.3532 - acc: 0.5256 - top5-acc: 0.9339 - val_loss: 1.2563 - val_acc: 0.5548 - val_top5-acc: 0.9474 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3499 - acc: 0.5260 - top5-acc: 0.9338 - val_loss: 1.2718 - val_acc: 0.5528 - val_top5-acc: 0.9456 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3490 - acc: 0.5264 - top5-acc: 0.9342 - val_loss: 1.2530 - val_acc: 0.5552 - val_top5-acc: 0.9462 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3534 - acc: 0.5244 - top5-acc: 0.9332 - val_loss: 1.2599 - val_acc: 0.5530 - val_top5-acc: 0.9484 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3468 - acc: 0.5246 - top5-acc: 0.9367 - val_loss: 1.2575 - val_acc: 0.5542 - val_top5-acc: 0.9464 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3490 - acc: 0.5247 - top5-acc: 0.9342 - val_loss: 1.2544 - val_acc: 0.5602 - val_top5-acc: 0.9464 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3514 - acc: 0.5236 - top5-acc: 0.9358 - val_loss: 1.2644 - val_acc: 0.5466 - val_top5-acc: 0.9472 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3497 - acc: 0.5250 - top5-acc: 0.9334 - val_loss: 1.2675 - val_acc: 0.5518 - val_top5-acc: 0.9470 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3441 - acc: 0.5286 - top5-acc: 0.9357 - val_loss: 1.2615 - val_acc: 0.5566 - val_top5-acc: 0.9444 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.3445 - acc: 0.5282 - top5-acc: 0.9346 - val_loss: 1.2551 - val_acc: 0.5592 - val_top5-acc: 0.9464 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.3413 - acc: 0.5282 - top5-acc: 0.9347 - val_loss: 1.2522 - val_acc: 0.5598 - val_top5-acc: 0.9448 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.3370 - acc: 0.5322 - top5-acc: 0.9369 - val_loss: 1.2555 - val_acc: 0.5560 - val_top5-acc: 0.9478 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 9s 101ms/step - loss: 1.3455 - acc: 0.5297 - top5-acc: 0.9338 - val_loss: 1.2510 - val_acc: 0.5586 - val_top5-acc: 0.9468 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3474 - acc: 0.5273 - top5-acc: 0.9364 - val_loss: 1.2537 - val_acc: 0.5604 - val_top5-acc: 0.9476 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3493 - acc: 0.5261 - top5-acc: 0.9356 - val_loss: 1.2628 - val_acc: 0.5588 - val_top5-acc: 0.9462 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3477 - acc: 0.5267 - top5-acc: 0.9352 - val_loss: 1.2623 - val_acc: 0.5544 - val_top5-acc: 0.9476 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3461 - acc: 0.5260 - top5-acc: 0.9346 - val_loss: 1.2598 - val_acc: 0.5578 - val_top5-acc: 0.9488 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3479 - acc: 0.5270 - top5-acc: 0.9338 - val_loss: 1.2604 - val_acc: 0.5556 - val_top5-acc: 0.9472 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 9s 100ms/step - loss: 1.3455 - acc: 0.5257 - top5-acc: 0.9350 - val_loss: 1.2551 - val_acc: 0.5602 - val_top5-acc: 0.9474 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 13s 40ms/step - loss: 1.2873 - acc: 0.5476 - top5-acc: 0.9430\n",
      "Test accuracy: 54.76%\n",
      "Test top 5 accuracy: 94.3%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 13s 116ms/step - loss: 1.8485 - acc: 0.3555 - top5-acc: 0.8447 - val_loss: 1.5511 - val_acc: 0.4454 - val_top5-acc: 0.9130 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.5696 - acc: 0.4426 - top5-acc: 0.9056 - val_loss: 1.4545 - val_acc: 0.4824 - val_top5-acc: 0.9258 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.5197 - acc: 0.4647 - top5-acc: 0.9113 - val_loss: 1.4243 - val_acc: 0.4990 - val_top5-acc: 0.9234 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.4868 - acc: 0.4720 - top5-acc: 0.9181 - val_loss: 1.4137 - val_acc: 0.5026 - val_top5-acc: 0.9284 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.4601 - acc: 0.4839 - top5-acc: 0.9196 - val_loss: 1.3846 - val_acc: 0.5134 - val_top5-acc: 0.9312 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.4476 - acc: 0.4856 - top5-acc: 0.9225 - val_loss: 1.3652 - val_acc: 0.5144 - val_top5-acc: 0.9334 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.4407 - acc: 0.4937 - top5-acc: 0.9234 - val_loss: 1.3608 - val_acc: 0.5234 - val_top5-acc: 0.9316 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.4270 - acc: 0.4974 - top5-acc: 0.9248 - val_loss: 1.3212 - val_acc: 0.5328 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.4233 - acc: 0.4970 - top5-acc: 0.9253 - val_loss: 1.3321 - val_acc: 0.5278 - val_top5-acc: 0.9368 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.4155 - acc: 0.5011 - top5-acc: 0.9262 - val_loss: 1.3327 - val_acc: 0.5284 - val_top5-acc: 0.9368 - lr: 0.0050\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 9s 108ms/step - loss: 1.4038 - acc: 0.5059 - top5-acc: 0.9282 - val_loss: 1.3234 - val_acc: 0.5346 - val_top5-acc: 0.9380 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.4000 - acc: 0.5038 - top5-acc: 0.9294 - val_loss: 1.3031 - val_acc: 0.5394 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.4071 - acc: 0.5033 - top5-acc: 0.9281 - val_loss: 1.3088 - val_acc: 0.5406 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3974 - acc: 0.5050 - top5-acc: 0.9292 - val_loss: 1.3008 - val_acc: 0.5476 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3942 - acc: 0.5056 - top5-acc: 0.9304 - val_loss: 1.2896 - val_acc: 0.5416 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3907 - acc: 0.5085 - top5-acc: 0.9306 - val_loss: 1.3111 - val_acc: 0.5320 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3906 - acc: 0.5102 - top5-acc: 0.9291 - val_loss: 1.3094 - val_acc: 0.5268 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.3938 - acc: 0.5052 - top5-acc: 0.9291 - val_loss: 1.2850 - val_acc: 0.5428 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.3836 - acc: 0.5146 - top5-acc: 0.9293 - val_loss: 1.2919 - val_acc: 0.5408 - val_top5-acc: 0.9430 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3825 - acc: 0.5085 - top5-acc: 0.9314 - val_loss: 1.2797 - val_acc: 0.5454 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.3896 - acc: 0.5092 - top5-acc: 0.9298 - val_loss: 1.3155 - val_acc: 0.5308 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3805 - acc: 0.5146 - top5-acc: 0.9297 - val_loss: 1.2775 - val_acc: 0.5410 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3855 - acc: 0.5110 - top5-acc: 0.9297 - val_loss: 1.2970 - val_acc: 0.5370 - val_top5-acc: 0.9424 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3816 - acc: 0.5110 - top5-acc: 0.9311 - val_loss: 1.2791 - val_acc: 0.5468 - val_top5-acc: 0.9424 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3762 - acc: 0.5140 - top5-acc: 0.9322 - val_loss: 1.2656 - val_acc: 0.5528 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3719 - acc: 0.5140 - top5-acc: 0.9308 - val_loss: 1.2641 - val_acc: 0.5466 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3704 - acc: 0.5148 - top5-acc: 0.9320 - val_loss: 1.2817 - val_acc: 0.5446 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3721 - acc: 0.5155 - top5-acc: 0.9313 - val_loss: 1.2956 - val_acc: 0.5396 - val_top5-acc: 0.9424 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.3715 - acc: 0.5155 - top5-acc: 0.9315 - val_loss: 1.2678 - val_acc: 0.5516 - val_top5-acc: 0.9422 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.3698 - acc: 0.5170 - top5-acc: 0.9314 - val_loss: 1.2517 - val_acc: 0.5538 - val_top5-acc: 0.9480 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 1.3757 - acc: 0.5150 - top5-acc: 0.9314 - val_loss: 1.2778 - val_acc: 0.5420 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3721 - acc: 0.5135 - top5-acc: 0.9315 - val_loss: 1.2733 - val_acc: 0.5432 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3755 - acc: 0.5128 - top5-acc: 0.9311 - val_loss: 1.2652 - val_acc: 0.5482 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3766 - acc: 0.5111 - top5-acc: 0.9334 - val_loss: 1.2655 - val_acc: 0.5476 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3816 - acc: 0.5106 - top5-acc: 0.9302 - val_loss: 1.2510 - val_acc: 0.5516 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3784 - acc: 0.5122 - top5-acc: 0.9315 - val_loss: 1.2557 - val_acc: 0.5504 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3708 - acc: 0.5160 - top5-acc: 0.9321 - val_loss: 1.2533 - val_acc: 0.5558 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3752 - acc: 0.5123 - top5-acc: 0.9323 - val_loss: 1.2869 - val_acc: 0.5416 - val_top5-acc: 0.9406 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3797 - acc: 0.5122 - top5-acc: 0.9306 - val_loss: 1.2578 - val_acc: 0.5570 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3659 - acc: 0.5176 - top5-acc: 0.9340 - val_loss: 1.2428 - val_acc: 0.5566 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.3641 - acc: 0.5176 - top5-acc: 0.9330 - val_loss: 1.2395 - val_acc: 0.5664 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3618 - acc: 0.5196 - top5-acc: 0.9324 - val_loss: 1.2533 - val_acc: 0.5564 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3721 - acc: 0.5147 - top5-acc: 0.9316 - val_loss: 1.2883 - val_acc: 0.5358 - val_top5-acc: 0.9414 - lr: 0.0050\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3640 - acc: 0.5177 - top5-acc: 0.9326 - val_loss: 1.2524 - val_acc: 0.5558 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3663 - acc: 0.5166 - top5-acc: 0.9315 - val_loss: 1.2579 - val_acc: 0.5500 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3730 - acc: 0.5139 - top5-acc: 0.9336 - val_loss: 1.2790 - val_acc: 0.5484 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3466 - acc: 0.5255 - top5-acc: 0.9345 - val_loss: 1.2242 - val_acc: 0.5692 - val_top5-acc: 0.9490 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3434 - acc: 0.5221 - top5-acc: 0.9346 - val_loss: 1.2444 - val_acc: 0.5566 - val_top5-acc: 0.9464 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3470 - acc: 0.5231 - top5-acc: 0.9347 - val_loss: 1.2274 - val_acc: 0.5686 - val_top5-acc: 0.9468 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3468 - acc: 0.5255 - top5-acc: 0.9346 - val_loss: 1.2270 - val_acc: 0.5700 - val_top5-acc: 0.9482 - lr: 0.0025\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 1.2707 - acc: 0.5528 - top5-acc: 0.9472\n",
      "Test accuracy: 55.28%\n",
      "Test top 5 accuracy: 94.72%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 13s 125ms/step - loss: 1.8381 - acc: 0.3552 - top5-acc: 0.8492 - val_loss: 1.5590 - val_acc: 0.4192 - val_top5-acc: 0.9164 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.5547 - acc: 0.4476 - top5-acc: 0.9050 - val_loss: 1.4367 - val_acc: 0.4880 - val_top5-acc: 0.9236 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.4943 - acc: 0.4680 - top5-acc: 0.9156 - val_loss: 1.4258 - val_acc: 0.4894 - val_top5-acc: 0.9278 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.4716 - acc: 0.4766 - top5-acc: 0.9193 - val_loss: 1.3699 - val_acc: 0.5078 - val_top5-acc: 0.9340 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.4391 - acc: 0.4910 - top5-acc: 0.9227 - val_loss: 1.3423 - val_acc: 0.5168 - val_top5-acc: 0.9412 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.4298 - acc: 0.4938 - top5-acc: 0.9250 - val_loss: 1.3344 - val_acc: 0.5226 - val_top5-acc: 0.9372 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.4208 - acc: 0.5003 - top5-acc: 0.9272 - val_loss: 1.3316 - val_acc: 0.5288 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.4090 - acc: 0.5004 - top5-acc: 0.9261 - val_loss: 1.3145 - val_acc: 0.5342 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3932 - acc: 0.5043 - top5-acc: 0.9291 - val_loss: 1.3035 - val_acc: 0.5356 - val_top5-acc: 0.9396 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3865 - acc: 0.5102 - top5-acc: 0.9295 - val_loss: 1.2849 - val_acc: 0.5460 - val_top5-acc: 0.9402 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.3935 - acc: 0.5078 - top5-acc: 0.9283 - val_loss: 1.3142 - val_acc: 0.5336 - val_top5-acc: 0.9380 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.3785 - acc: 0.5107 - top5-acc: 0.9305 - val_loss: 1.2971 - val_acc: 0.5370 - val_top5-acc: 0.9406 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.3697 - acc: 0.5144 - top5-acc: 0.9316 - val_loss: 1.2992 - val_acc: 0.5424 - val_top5-acc: 0.9376 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3782 - acc: 0.5126 - top5-acc: 0.9311 - val_loss: 1.2817 - val_acc: 0.5412 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3652 - acc: 0.5188 - top5-acc: 0.9309 - val_loss: 1.2610 - val_acc: 0.5550 - val_top5-acc: 0.9446 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3647 - acc: 0.5172 - top5-acc: 0.9326 - val_loss: 1.2711 - val_acc: 0.5516 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3676 - acc: 0.5169 - top5-acc: 0.9323 - val_loss: 1.2863 - val_acc: 0.5422 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3727 - acc: 0.5136 - top5-acc: 0.9322 - val_loss: 1.2499 - val_acc: 0.5640 - val_top5-acc: 0.9450 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3635 - acc: 0.5173 - top5-acc: 0.9325 - val_loss: 1.2546 - val_acc: 0.5554 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3608 - acc: 0.5178 - top5-acc: 0.9320 - val_loss: 1.2417 - val_acc: 0.5558 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3602 - acc: 0.5198 - top5-acc: 0.9332 - val_loss: 1.2503 - val_acc: 0.5522 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3536 - acc: 0.5199 - top5-acc: 0.9344 - val_loss: 1.2488 - val_acc: 0.5516 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3543 - acc: 0.5210 - top5-acc: 0.9334 - val_loss: 1.2541 - val_acc: 0.5510 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3537 - acc: 0.5224 - top5-acc: 0.9335 - val_loss: 1.2540 - val_acc: 0.5606 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3548 - acc: 0.5203 - top5-acc: 0.9337 - val_loss: 1.2496 - val_acc: 0.5592 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3311 - acc: 0.5297 - top5-acc: 0.9364 - val_loss: 1.2204 - val_acc: 0.5706 - val_top5-acc: 0.9480 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3241 - acc: 0.5332 - top5-acc: 0.9377 - val_loss: 1.2166 - val_acc: 0.5754 - val_top5-acc: 0.9514 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3284 - acc: 0.5309 - top5-acc: 0.9360 - val_loss: 1.2375 - val_acc: 0.5686 - val_top5-acc: 0.9482 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 10s 115ms/step - loss: 1.3234 - acc: 0.5325 - top5-acc: 0.9368 - val_loss: 1.2248 - val_acc: 0.5668 - val_top5-acc: 0.9498 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3341 - acc: 0.5284 - top5-acc: 0.9358 - val_loss: 1.2232 - val_acc: 0.5672 - val_top5-acc: 0.9482 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3328 - acc: 0.5292 - top5-acc: 0.9356 - val_loss: 1.2251 - val_acc: 0.5658 - val_top5-acc: 0.9476 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.3315 - acc: 0.5288 - top5-acc: 0.9354 - val_loss: 1.2353 - val_acc: 0.5656 - val_top5-acc: 0.9496 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3146 - acc: 0.5372 - top5-acc: 0.9385 - val_loss: 1.2190 - val_acc: 0.5692 - val_top5-acc: 0.9488 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3154 - acc: 0.5378 - top5-acc: 0.9376 - val_loss: 1.2242 - val_acc: 0.5674 - val_top5-acc: 0.9480 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3195 - acc: 0.5349 - top5-acc: 0.9377 - val_loss: 1.2143 - val_acc: 0.5710 - val_top5-acc: 0.9518 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3234 - acc: 0.5332 - top5-acc: 0.9358 - val_loss: 1.2152 - val_acc: 0.5720 - val_top5-acc: 0.9508 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3184 - acc: 0.5347 - top5-acc: 0.9372 - val_loss: 1.2183 - val_acc: 0.5768 - val_top5-acc: 0.9490 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 10s 115ms/step - loss: 1.3240 - acc: 0.5298 - top5-acc: 0.9366 - val_loss: 1.2196 - val_acc: 0.5706 - val_top5-acc: 0.9496 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3200 - acc: 0.5351 - top5-acc: 0.9360 - val_loss: 1.2150 - val_acc: 0.5742 - val_top5-acc: 0.9486 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3209 - acc: 0.5324 - top5-acc: 0.9384 - val_loss: 1.2205 - val_acc: 0.5654 - val_top5-acc: 0.9482 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3154 - acc: 0.5364 - top5-acc: 0.9371 - val_loss: 1.2156 - val_acc: 0.5776 - val_top5-acc: 0.9498 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3133 - acc: 0.5380 - top5-acc: 0.9374 - val_loss: 1.2120 - val_acc: 0.5764 - val_top5-acc: 0.9516 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3136 - acc: 0.5384 - top5-acc: 0.9373 - val_loss: 1.2154 - val_acc: 0.5754 - val_top5-acc: 0.9522 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3179 - acc: 0.5363 - top5-acc: 0.9394 - val_loss: 1.2150 - val_acc: 0.5720 - val_top5-acc: 0.9506 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3186 - acc: 0.5351 - top5-acc: 0.9375 - val_loss: 1.2221 - val_acc: 0.5694 - val_top5-acc: 0.9506 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3137 - acc: 0.5348 - top5-acc: 0.9378 - val_loss: 1.2128 - val_acc: 0.5762 - val_top5-acc: 0.9498 - lr: 6.2500e-04\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3113 - acc: 0.5384 - top5-acc: 0.9384 - val_loss: 1.2229 - val_acc: 0.5726 - val_top5-acc: 0.9502 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 10s 115ms/step - loss: 1.3135 - acc: 0.5390 - top5-acc: 0.9383 - val_loss: 1.2168 - val_acc: 0.5768 - val_top5-acc: 0.9504 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3122 - acc: 0.5402 - top5-acc: 0.9378 - val_loss: 1.2187 - val_acc: 0.5796 - val_top5-acc: 0.9498 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3125 - acc: 0.5373 - top5-acc: 0.9366 - val_loss: 1.2173 - val_acc: 0.5730 - val_top5-acc: 0.9524 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 1.2456 - acc: 0.5651 - top5-acc: 0.9505\n",
      "Test accuracy: 56.51%\n",
      "Test top 5 accuracy: 95.05%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 15s 134ms/step - loss: 1.7858 - acc: 0.3731 - top5-acc: 0.8515 - val_loss: 1.5043 - val_acc: 0.4536 - val_top5-acc: 0.9244 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.5294 - acc: 0.4570 - top5-acc: 0.9115 - val_loss: 1.4233 - val_acc: 0.4918 - val_top5-acc: 0.9258 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.4716 - acc: 0.4761 - top5-acc: 0.9213 - val_loss: 1.3606 - val_acc: 0.5144 - val_top5-acc: 0.9376 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.4360 - acc: 0.4936 - top5-acc: 0.9234 - val_loss: 1.3382 - val_acc: 0.5232 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.4141 - acc: 0.4993 - top5-acc: 0.9252 - val_loss: 1.3178 - val_acc: 0.5372 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.4108 - acc: 0.5002 - top5-acc: 0.9275 - val_loss: 1.3152 - val_acc: 0.5306 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3863 - acc: 0.5114 - top5-acc: 0.9288 - val_loss: 1.2965 - val_acc: 0.5512 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3728 - acc: 0.5140 - top5-acc: 0.9339 - val_loss: 1.2803 - val_acc: 0.5526 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3739 - acc: 0.5175 - top5-acc: 0.9316 - val_loss: 1.2728 - val_acc: 0.5446 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3658 - acc: 0.5178 - top5-acc: 0.9330 - val_loss: 1.2781 - val_acc: 0.5436 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3565 - acc: 0.5185 - top5-acc: 0.9340 - val_loss: 1.2699 - val_acc: 0.5574 - val_top5-acc: 0.9458 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3534 - acc: 0.5216 - top5-acc: 0.9336 - val_loss: 1.2687 - val_acc: 0.5400 - val_top5-acc: 0.9458 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3569 - acc: 0.5201 - top5-acc: 0.9324 - val_loss: 1.2650 - val_acc: 0.5454 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3561 - acc: 0.5198 - top5-acc: 0.9348 - val_loss: 1.2740 - val_acc: 0.5484 - val_top5-acc: 0.9480 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3466 - acc: 0.5204 - top5-acc: 0.9343 - val_loss: 1.2561 - val_acc: 0.5594 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3409 - acc: 0.5279 - top5-acc: 0.9344 - val_loss: 1.2489 - val_acc: 0.5552 - val_top5-acc: 0.9484 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3410 - acc: 0.5239 - top5-acc: 0.9338 - val_loss: 1.2448 - val_acc: 0.5582 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3330 - acc: 0.5283 - top5-acc: 0.9356 - val_loss: 1.2427 - val_acc: 0.5594 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3258 - acc: 0.5318 - top5-acc: 0.9358 - val_loss: 1.2351 - val_acc: 0.5604 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3396 - acc: 0.5266 - top5-acc: 0.9356 - val_loss: 1.2702 - val_acc: 0.5510 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3445 - acc: 0.5228 - top5-acc: 0.9352 - val_loss: 1.2428 - val_acc: 0.5588 - val_top5-acc: 0.9450 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3348 - acc: 0.5283 - top5-acc: 0.9353 - val_loss: 1.2268 - val_acc: 0.5634 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3324 - acc: 0.5298 - top5-acc: 0.9364 - val_loss: 1.2354 - val_acc: 0.5594 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3266 - acc: 0.5320 - top5-acc: 0.9361 - val_loss: 1.2404 - val_acc: 0.5694 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3269 - acc: 0.5307 - top5-acc: 0.9371 - val_loss: 1.2481 - val_acc: 0.5676 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3325 - acc: 0.5284 - top5-acc: 0.9370 - val_loss: 1.2265 - val_acc: 0.5674 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3268 - acc: 0.5298 - top5-acc: 0.9370 - val_loss: 1.2368 - val_acc: 0.5672 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 11s 125ms/step - loss: 1.3231 - acc: 0.5323 - top5-acc: 0.9374 - val_loss: 1.2172 - val_acc: 0.5678 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3256 - acc: 0.5320 - top5-acc: 0.9378 - val_loss: 1.2076 - val_acc: 0.5776 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3233 - acc: 0.5336 - top5-acc: 0.9364 - val_loss: 1.2086 - val_acc: 0.5744 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3249 - acc: 0.5307 - top5-acc: 0.9382 - val_loss: 1.2163 - val_acc: 0.5650 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3146 - acc: 0.5359 - top5-acc: 0.9382 - val_loss: 1.2096 - val_acc: 0.5794 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3219 - acc: 0.5316 - top5-acc: 0.9374 - val_loss: 1.2182 - val_acc: 0.5692 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3260 - acc: 0.5313 - top5-acc: 0.9358 - val_loss: 1.2229 - val_acc: 0.5664 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3044 - acc: 0.5396 - top5-acc: 0.9405 - val_loss: 1.1951 - val_acc: 0.5774 - val_top5-acc: 0.9502 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3032 - acc: 0.5363 - top5-acc: 0.9404 - val_loss: 1.2054 - val_acc: 0.5746 - val_top5-acc: 0.9526 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.2968 - acc: 0.5430 - top5-acc: 0.9390 - val_loss: 1.2045 - val_acc: 0.5720 - val_top5-acc: 0.9516 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3045 - acc: 0.5354 - top5-acc: 0.9394 - val_loss: 1.1983 - val_acc: 0.5842 - val_top5-acc: 0.9486 - lr: 0.0025\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3097 - acc: 0.5354 - top5-acc: 0.9387 - val_loss: 1.2016 - val_acc: 0.5786 - val_top5-acc: 0.9532 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.2994 - acc: 0.5404 - top5-acc: 0.9391 - val_loss: 1.1915 - val_acc: 0.5784 - val_top5-acc: 0.9522 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3004 - acc: 0.5389 - top5-acc: 0.9392 - val_loss: 1.2005 - val_acc: 0.5760 - val_top5-acc: 0.9506 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3016 - acc: 0.5387 - top5-acc: 0.9396 - val_loss: 1.1967 - val_acc: 0.5742 - val_top5-acc: 0.9536 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3043 - acc: 0.5378 - top5-acc: 0.9400 - val_loss: 1.1925 - val_acc: 0.5822 - val_top5-acc: 0.9514 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3070 - acc: 0.5401 - top5-acc: 0.9381 - val_loss: 1.2228 - val_acc: 0.5700 - val_top5-acc: 0.9488 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3022 - acc: 0.5433 - top5-acc: 0.9402 - val_loss: 1.2153 - val_acc: 0.5648 - val_top5-acc: 0.9510 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.2902 - acc: 0.5450 - top5-acc: 0.9393 - val_loss: 1.1979 - val_acc: 0.5802 - val_top5-acc: 0.9504 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.2916 - acc: 0.5411 - top5-acc: 0.9416 - val_loss: 1.1919 - val_acc: 0.5770 - val_top5-acc: 0.9496 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.2880 - acc: 0.5444 - top5-acc: 0.9407 - val_loss: 1.1986 - val_acc: 0.5818 - val_top5-acc: 0.9516 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.2905 - acc: 0.5453 - top5-acc: 0.9391 - val_loss: 1.1918 - val_acc: 0.5796 - val_top5-acc: 0.9530 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.2977 - acc: 0.5409 - top5-acc: 0.9398 - val_loss: 1.1974 - val_acc: 0.5812 - val_top5-acc: 0.9512 - lr: 0.0012\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 1.2264 - acc: 0.5616 - top5-acc: 0.9543\n",
      "Test accuracy: 56.16%\n",
      "Test top 5 accuracy: 95.43%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 16s 142ms/step - loss: 1.8263 - acc: 0.3635 - top5-acc: 0.8450 - val_loss: 1.5275 - val_acc: 0.4440 - val_top5-acc: 0.9172 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 1.5236 - acc: 0.4602 - top5-acc: 0.9120 - val_loss: 1.4011 - val_acc: 0.4934 - val_top5-acc: 0.9334 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 13s 150ms/step - loss: 1.4668 - acc: 0.4783 - top5-acc: 0.9187 - val_loss: 1.3981 - val_acc: 0.4984 - val_top5-acc: 0.9362 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 1.4404 - acc: 0.4899 - top5-acc: 0.9220 - val_loss: 1.3618 - val_acc: 0.5172 - val_top5-acc: 0.9324 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.4219 - acc: 0.4952 - top5-acc: 0.9257 - val_loss: 1.3419 - val_acc: 0.5238 - val_top5-acc: 0.9384 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3953 - acc: 0.5032 - top5-acc: 0.9288 - val_loss: 1.3147 - val_acc: 0.5462 - val_top5-acc: 0.9386 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.3809 - acc: 0.5112 - top5-acc: 0.9304 - val_loss: 1.2849 - val_acc: 0.5506 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.3767 - acc: 0.5119 - top5-acc: 0.9309 - val_loss: 1.2959 - val_acc: 0.5336 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3657 - acc: 0.5184 - top5-acc: 0.9313 - val_loss: 1.2681 - val_acc: 0.5574 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.3670 - acc: 0.5143 - top5-acc: 0.9317 - val_loss: 1.2787 - val_acc: 0.5444 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 13s 148ms/step - loss: 1.3511 - acc: 0.5226 - top5-acc: 0.9336 - val_loss: 1.2437 - val_acc: 0.5676 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 1.3582 - acc: 0.5193 - top5-acc: 0.9342 - val_loss: 1.2371 - val_acc: 0.5682 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.3440 - acc: 0.5254 - top5-acc: 0.9342 - val_loss: 1.2624 - val_acc: 0.5582 - val_top5-acc: 0.9472 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3432 - acc: 0.5216 - top5-acc: 0.9348 - val_loss: 1.2477 - val_acc: 0.5642 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3476 - acc: 0.5233 - top5-acc: 0.9348 - val_loss: 1.2315 - val_acc: 0.5640 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3409 - acc: 0.5273 - top5-acc: 0.9338 - val_loss: 1.2411 - val_acc: 0.5578 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 12s 138ms/step - loss: 1.3471 - acc: 0.5244 - top5-acc: 0.9344 - val_loss: 1.2440 - val_acc: 0.5574 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3364 - acc: 0.5270 - top5-acc: 0.9360 - val_loss: 1.2331 - val_acc: 0.5668 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3329 - acc: 0.5292 - top5-acc: 0.9368 - val_loss: 1.2344 - val_acc: 0.5642 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 12s 138ms/step - loss: 1.3275 - acc: 0.5281 - top5-acc: 0.9362 - val_loss: 1.2328 - val_acc: 0.5716 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.3106 - acc: 0.5369 - top5-acc: 0.9381 - val_loss: 1.2156 - val_acc: 0.5726 - val_top5-acc: 0.9492 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.3136 - acc: 0.5350 - top5-acc: 0.9380 - val_loss: 1.2179 - val_acc: 0.5762 - val_top5-acc: 0.9522 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.3125 - acc: 0.5350 - top5-acc: 0.9385 - val_loss: 1.2097 - val_acc: 0.5730 - val_top5-acc: 0.9502 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.3139 - acc: 0.5361 - top5-acc: 0.9376 - val_loss: 1.1963 - val_acc: 0.5808 - val_top5-acc: 0.9520 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 13s 142ms/step - loss: 1.3083 - acc: 0.5370 - top5-acc: 0.9376 - val_loss: 1.2043 - val_acc: 0.5808 - val_top5-acc: 0.9538 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.3081 - acc: 0.5362 - top5-acc: 0.9368 - val_loss: 1.2111 - val_acc: 0.5786 - val_top5-acc: 0.9516 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 12s 138ms/step - loss: 1.3166 - acc: 0.5325 - top5-acc: 0.9384 - val_loss: 1.2177 - val_acc: 0.5760 - val_top5-acc: 0.9502 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3045 - acc: 0.5420 - top5-acc: 0.9383 - val_loss: 1.2055 - val_acc: 0.5790 - val_top5-acc: 0.9528 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 13s 150ms/step - loss: 1.3072 - acc: 0.5385 - top5-acc: 0.9389 - val_loss: 1.2047 - val_acc: 0.5818 - val_top5-acc: 0.9526 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.2982 - acc: 0.5414 - top5-acc: 0.9402 - val_loss: 1.2035 - val_acc: 0.5776 - val_top5-acc: 0.9528 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.3012 - acc: 0.5385 - top5-acc: 0.9397 - val_loss: 1.2003 - val_acc: 0.5850 - val_top5-acc: 0.9540 - lr: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.2935 - acc: 0.5424 - top5-acc: 0.9404 - val_loss: 1.2055 - val_acc: 0.5736 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2975 - acc: 0.5436 - top5-acc: 0.9390 - val_loss: 1.1966 - val_acc: 0.5850 - val_top5-acc: 0.9526 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2995 - acc: 0.5421 - top5-acc: 0.9396 - val_loss: 1.2017 - val_acc: 0.5754 - val_top5-acc: 0.9500 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 12s 138ms/step - loss: 1.2927 - acc: 0.5442 - top5-acc: 0.9405 - val_loss: 1.1967 - val_acc: 0.5820 - val_top5-acc: 0.9554 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 12s 138ms/step - loss: 1.2920 - acc: 0.5452 - top5-acc: 0.9385 - val_loss: 1.1934 - val_acc: 0.5868 - val_top5-acc: 0.9534 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 12s 138ms/step - loss: 1.2956 - acc: 0.5441 - top5-acc: 0.9400 - val_loss: 1.1934 - val_acc: 0.5854 - val_top5-acc: 0.9524 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2899 - acc: 0.5469 - top5-acc: 0.9399 - val_loss: 1.1989 - val_acc: 0.5810 - val_top5-acc: 0.9516 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 1.2938 - acc: 0.5452 - top5-acc: 0.9383 - val_loss: 1.1978 - val_acc: 0.5870 - val_top5-acc: 0.9506 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 1.2905 - acc: 0.5451 - top5-acc: 0.9414 - val_loss: 1.1944 - val_acc: 0.5884 - val_top5-acc: 0.9548 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 1.2893 - acc: 0.5488 - top5-acc: 0.9398 - val_loss: 1.1969 - val_acc: 0.5836 - val_top5-acc: 0.9544 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 1.2945 - acc: 0.5463 - top5-acc: 0.9385 - val_loss: 1.1977 - val_acc: 0.5868 - val_top5-acc: 0.9540 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 12s 134ms/step - loss: 1.2902 - acc: 0.5465 - top5-acc: 0.9412 - val_loss: 1.2010 - val_acc: 0.5854 - val_top5-acc: 0.9536 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 12s 134ms/step - loss: 1.2934 - acc: 0.5468 - top5-acc: 0.9400 - val_loss: 1.1996 - val_acc: 0.5868 - val_top5-acc: 0.9524 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 1.2899 - acc: 0.5478 - top5-acc: 0.9405 - val_loss: 1.2007 - val_acc: 0.5834 - val_top5-acc: 0.9528 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 12s 134ms/step - loss: 1.2887 - acc: 0.5476 - top5-acc: 0.9404 - val_loss: 1.2026 - val_acc: 0.5812 - val_top5-acc: 0.9506 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 13s 148ms/step - loss: 1.2910 - acc: 0.5468 - top5-acc: 0.9404 - val_loss: 1.2008 - val_acc: 0.5832 - val_top5-acc: 0.9550 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2928 - acc: 0.5463 - top5-acc: 0.9399 - val_loss: 1.2006 - val_acc: 0.5824 - val_top5-acc: 0.9546 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2963 - acc: 0.5463 - top5-acc: 0.9401 - val_loss: 1.2032 - val_acc: 0.5818 - val_top5-acc: 0.9534 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 1.2907 - acc: 0.5476 - top5-acc: 0.9420 - val_loss: 1.2044 - val_acc: 0.5870 - val_top5-acc: 0.9524 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 18s 58ms/step - loss: 1.2346 - acc: 0.5656 - top5-acc: 0.9536\n",
      "Test accuracy: 56.56%\n",
      "Test top 5 accuracy: 95.36%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 18s 153ms/step - loss: 1.7872 - acc: 0.3724 - top5-acc: 0.8544 - val_loss: 1.4833 - val_acc: 0.4582 - val_top5-acc: 0.9274 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.5110 - acc: 0.4618 - top5-acc: 0.9113 - val_loss: 1.3885 - val_acc: 0.4954 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.4462 - acc: 0.4867 - top5-acc: 0.9229 - val_loss: 1.3555 - val_acc: 0.5054 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.4171 - acc: 0.4977 - top5-acc: 0.9253 - val_loss: 1.3306 - val_acc: 0.5192 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 13s 147ms/step - loss: 1.3881 - acc: 0.5078 - top5-acc: 0.9298 - val_loss: 1.3061 - val_acc: 0.5332 - val_top5-acc: 0.9480 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3764 - acc: 0.5119 - top5-acc: 0.9305 - val_loss: 1.2908 - val_acc: 0.5500 - val_top5-acc: 0.9448 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.3676 - acc: 0.5136 - top5-acc: 0.9340 - val_loss: 1.3097 - val_acc: 0.5306 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.3648 - acc: 0.5180 - top5-acc: 0.9325 - val_loss: 1.2749 - val_acc: 0.5508 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.3489 - acc: 0.5237 - top5-acc: 0.9348 - val_loss: 1.2516 - val_acc: 0.5578 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3437 - acc: 0.5240 - top5-acc: 0.9367 - val_loss: 1.2383 - val_acc: 0.5662 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 13s 142ms/step - loss: 1.3358 - acc: 0.5303 - top5-acc: 0.9372 - val_loss: 1.2352 - val_acc: 0.5632 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.3323 - acc: 0.5283 - top5-acc: 0.9359 - val_loss: 1.2364 - val_acc: 0.5640 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.3357 - acc: 0.5285 - top5-acc: 0.9364 - val_loss: 1.2630 - val_acc: 0.5506 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.3272 - acc: 0.5291 - top5-acc: 0.9373 - val_loss: 1.2329 - val_acc: 0.5594 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.3189 - acc: 0.5347 - top5-acc: 0.9379 - val_loss: 1.2442 - val_acc: 0.5582 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.3252 - acc: 0.5306 - top5-acc: 0.9381 - val_loss: 1.2207 - val_acc: 0.5680 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 13s 142ms/step - loss: 1.3190 - acc: 0.5369 - top5-acc: 0.9370 - val_loss: 1.2157 - val_acc: 0.5792 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.3208 - acc: 0.5362 - top5-acc: 0.9366 - val_loss: 1.2261 - val_acc: 0.5696 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.3135 - acc: 0.5352 - top5-acc: 0.9388 - val_loss: 1.1990 - val_acc: 0.5796 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3198 - acc: 0.5333 - top5-acc: 0.9382 - val_loss: 1.2226 - val_acc: 0.5702 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 13s 142ms/step - loss: 1.3137 - acc: 0.5375 - top5-acc: 0.9396 - val_loss: 1.2023 - val_acc: 0.5788 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.3071 - acc: 0.5375 - top5-acc: 0.9387 - val_loss: 1.2113 - val_acc: 0.5762 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 13s 142ms/step - loss: 1.3082 - acc: 0.5366 - top5-acc: 0.9390 - val_loss: 1.2065 - val_acc: 0.5812 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 13s 145ms/step - loss: 1.3110 - acc: 0.5365 - top5-acc: 0.9381 - val_loss: 1.2077 - val_acc: 0.5764 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2920 - acc: 0.5419 - top5-acc: 0.9402 - val_loss: 1.1920 - val_acc: 0.5784 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2907 - acc: 0.5406 - top5-acc: 0.9421 - val_loss: 1.1855 - val_acc: 0.5922 - val_top5-acc: 0.9546 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2880 - acc: 0.5460 - top5-acc: 0.9421 - val_loss: 1.2161 - val_acc: 0.5670 - val_top5-acc: 0.9518 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.2928 - acc: 0.5425 - top5-acc: 0.9406 - val_loss: 1.1856 - val_acc: 0.5888 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 13s 149ms/step - loss: 1.2882 - acc: 0.5464 - top5-acc: 0.9411 - val_loss: 1.1777 - val_acc: 0.5866 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2911 - acc: 0.5440 - top5-acc: 0.9418 - val_loss: 1.1785 - val_acc: 0.5906 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 13s 149ms/step - loss: 1.2946 - acc: 0.5407 - top5-acc: 0.9420 - val_loss: 1.1785 - val_acc: 0.5920 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 13s 147ms/step - loss: 1.2904 - acc: 0.5463 - top5-acc: 0.9400 - val_loss: 1.1697 - val_acc: 0.5936 - val_top5-acc: 0.9580 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 13s 148ms/step - loss: 1.2881 - acc: 0.5423 - top5-acc: 0.9415 - val_loss: 1.1924 - val_acc: 0.5826 - val_top5-acc: 0.9568 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 13s 149ms/step - loss: 1.2850 - acc: 0.5463 - top5-acc: 0.9421 - val_loss: 1.1876 - val_acc: 0.5834 - val_top5-acc: 0.9542 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2884 - acc: 0.5446 - top5-acc: 0.9409 - val_loss: 1.1795 - val_acc: 0.5916 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2872 - acc: 0.5458 - top5-acc: 0.9404 - val_loss: 1.2017 - val_acc: 0.5760 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2884 - acc: 0.5470 - top5-acc: 0.9405 - val_loss: 1.1972 - val_acc: 0.5750 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 13s 150ms/step - loss: 1.2767 - acc: 0.5470 - top5-acc: 0.9422 - val_loss: 1.1847 - val_acc: 0.5836 - val_top5-acc: 0.9536 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2725 - acc: 0.5507 - top5-acc: 0.9427 - val_loss: 1.1779 - val_acc: 0.5872 - val_top5-acc: 0.9582 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2723 - acc: 0.5506 - top5-acc: 0.9436 - val_loss: 1.1762 - val_acc: 0.5878 - val_top5-acc: 0.9554 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 13s 150ms/step - loss: 1.2769 - acc: 0.5495 - top5-acc: 0.9426 - val_loss: 1.1785 - val_acc: 0.5888 - val_top5-acc: 0.9550 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2789 - acc: 0.5486 - top5-acc: 0.9414 - val_loss: 1.1678 - val_acc: 0.5938 - val_top5-acc: 0.9550 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2775 - acc: 0.5478 - top5-acc: 0.9417 - val_loss: 1.1758 - val_acc: 0.5896 - val_top5-acc: 0.9536 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2773 - acc: 0.5489 - top5-acc: 0.9411 - val_loss: 1.1738 - val_acc: 0.5902 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2787 - acc: 0.5494 - top5-acc: 0.9407 - val_loss: 1.1749 - val_acc: 0.5908 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2765 - acc: 0.5498 - top5-acc: 0.9421 - val_loss: 1.1761 - val_acc: 0.5882 - val_top5-acc: 0.9556 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2795 - acc: 0.5475 - top5-acc: 0.9414 - val_loss: 1.1835 - val_acc: 0.5870 - val_top5-acc: 0.9562 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2704 - acc: 0.5529 - top5-acc: 0.9448 - val_loss: 1.1695 - val_acc: 0.5922 - val_top5-acc: 0.9572 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2691 - acc: 0.5514 - top5-acc: 0.9438 - val_loss: 1.1706 - val_acc: 0.5946 - val_top5-acc: 0.9558 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2704 - acc: 0.5546 - top5-acc: 0.9431 - val_loss: 1.1729 - val_acc: 0.5918 - val_top5-acc: 0.9576 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 20s 63ms/step - loss: 1.2059 - acc: 0.5703 - top5-acc: 0.9556\n",
      "Test accuracy: 57.03%\n",
      "Test top 5 accuracy: 95.56%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 19s 170ms/step - loss: 1.7997 - acc: 0.3674 - top5-acc: 0.8484 - val_loss: 1.4986 - val_acc: 0.4582 - val_top5-acc: 0.9228 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.5007 - acc: 0.4662 - top5-acc: 0.9152 - val_loss: 1.3764 - val_acc: 0.5076 - val_top5-acc: 0.9330 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.4322 - acc: 0.4953 - top5-acc: 0.9247 - val_loss: 1.3281 - val_acc: 0.5290 - val_top5-acc: 0.9406 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.3958 - acc: 0.5066 - top5-acc: 0.9300 - val_loss: 1.2904 - val_acc: 0.5382 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.3705 - acc: 0.5132 - top5-acc: 0.9330 - val_loss: 1.2887 - val_acc: 0.5436 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.3543 - acc: 0.5192 - top5-acc: 0.9338 - val_loss: 1.3001 - val_acc: 0.5386 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.3554 - acc: 0.5215 - top5-acc: 0.9343 - val_loss: 1.2465 - val_acc: 0.5566 - val_top5-acc: 0.9470 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.3455 - acc: 0.5247 - top5-acc: 0.9340 - val_loss: 1.2561 - val_acc: 0.5604 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 14s 154ms/step - loss: 1.3250 - acc: 0.5316 - top5-acc: 0.9361 - val_loss: 1.2528 - val_acc: 0.5572 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.3230 - acc: 0.5332 - top5-acc: 0.9379 - val_loss: 1.2414 - val_acc: 0.5606 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.3207 - acc: 0.5366 - top5-acc: 0.9379 - val_loss: 1.2524 - val_acc: 0.5530 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.3183 - acc: 0.5369 - top5-acc: 0.9379 - val_loss: 1.2280 - val_acc: 0.5654 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.3154 - acc: 0.5330 - top5-acc: 0.9385 - val_loss: 1.2137 - val_acc: 0.5764 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.3145 - acc: 0.5313 - top5-acc: 0.9391 - val_loss: 1.2000 - val_acc: 0.5828 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 13s 150ms/step - loss: 1.3064 - acc: 0.5382 - top5-acc: 0.9401 - val_loss: 1.2091 - val_acc: 0.5722 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 13s 150ms/step - loss: 1.3084 - acc: 0.5337 - top5-acc: 0.9396 - val_loss: 1.1939 - val_acc: 0.5830 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.3057 - acc: 0.5387 - top5-acc: 0.9411 - val_loss: 1.1941 - val_acc: 0.5834 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.3013 - acc: 0.5415 - top5-acc: 0.9404 - val_loss: 1.2133 - val_acc: 0.5716 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.3037 - acc: 0.5415 - top5-acc: 0.9400 - val_loss: 1.2173 - val_acc: 0.5680 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.3019 - acc: 0.5389 - top5-acc: 0.9400 - val_loss: 1.1853 - val_acc: 0.5856 - val_top5-acc: 0.9572 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2986 - acc: 0.5413 - top5-acc: 0.9405 - val_loss: 1.1975 - val_acc: 0.5792 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2904 - acc: 0.5415 - top5-acc: 0.9411 - val_loss: 1.1906 - val_acc: 0.5804 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2976 - acc: 0.5401 - top5-acc: 0.9393 - val_loss: 1.1888 - val_acc: 0.5888 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2982 - acc: 0.5400 - top5-acc: 0.9403 - val_loss: 1.2031 - val_acc: 0.5798 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2978 - acc: 0.5412 - top5-acc: 0.9413 - val_loss: 1.2006 - val_acc: 0.5768 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2758 - acc: 0.5476 - top5-acc: 0.9426 - val_loss: 1.1821 - val_acc: 0.5808 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.2680 - acc: 0.5529 - top5-acc: 0.9427 - val_loss: 1.1561 - val_acc: 0.5986 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2669 - acc: 0.5511 - top5-acc: 0.9451 - val_loss: 1.1667 - val_acc: 0.5922 - val_top5-acc: 0.9576 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2686 - acc: 0.5526 - top5-acc: 0.9438 - val_loss: 1.1619 - val_acc: 0.5946 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 14s 156ms/step - loss: 1.2650 - acc: 0.5524 - top5-acc: 0.9434 - val_loss: 1.1641 - val_acc: 0.5958 - val_top5-acc: 0.9580 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2669 - acc: 0.5499 - top5-acc: 0.9448 - val_loss: 1.1719 - val_acc: 0.5864 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2718 - acc: 0.5510 - top5-acc: 0.9414 - val_loss: 1.1688 - val_acc: 0.5956 - val_top5-acc: 0.9550 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2600 - acc: 0.5537 - top5-acc: 0.9435 - val_loss: 1.1569 - val_acc: 0.5990 - val_top5-acc: 0.9562 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2592 - acc: 0.5570 - top5-acc: 0.9444 - val_loss: 1.1612 - val_acc: 0.5974 - val_top5-acc: 0.9566 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 14s 156ms/step - loss: 1.2611 - acc: 0.5544 - top5-acc: 0.9433 - val_loss: 1.1745 - val_acc: 0.5880 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 14s 155ms/step - loss: 1.2592 - acc: 0.5563 - top5-acc: 0.9425 - val_loss: 1.1556 - val_acc: 0.6000 - val_top5-acc: 0.9590 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2624 - acc: 0.5520 - top5-acc: 0.9448 - val_loss: 1.1541 - val_acc: 0.5988 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.2673 - acc: 0.5519 - top5-acc: 0.9434 - val_loss: 1.1628 - val_acc: 0.5928 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2573 - acc: 0.5552 - top5-acc: 0.9446 - val_loss: 1.1689 - val_acc: 0.5910 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.2646 - acc: 0.5532 - top5-acc: 0.9430 - val_loss: 1.1675 - val_acc: 0.5950 - val_top5-acc: 0.9556 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.2663 - acc: 0.5527 - top5-acc: 0.9431 - val_loss: 1.1646 - val_acc: 0.5946 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2629 - acc: 0.5527 - top5-acc: 0.9441 - val_loss: 1.1632 - val_acc: 0.5974 - val_top5-acc: 0.9566 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2570 - acc: 0.5578 - top5-acc: 0.9458 - val_loss: 1.1554 - val_acc: 0.6024 - val_top5-acc: 0.9580 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2608 - acc: 0.5562 - top5-acc: 0.9432 - val_loss: 1.1622 - val_acc: 0.5976 - val_top5-acc: 0.9572 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2591 - acc: 0.5555 - top5-acc: 0.9444 - val_loss: 1.1580 - val_acc: 0.6000 - val_top5-acc: 0.9598 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.2519 - acc: 0.5598 - top5-acc: 0.9449 - val_loss: 1.1589 - val_acc: 0.5982 - val_top5-acc: 0.9588 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2608 - acc: 0.5570 - top5-acc: 0.9423 - val_loss: 1.1592 - val_acc: 0.5948 - val_top5-acc: 0.9576 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.2532 - acc: 0.5607 - top5-acc: 0.9448 - val_loss: 1.1593 - val_acc: 0.5986 - val_top5-acc: 0.9572 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2585 - acc: 0.5585 - top5-acc: 0.9438 - val_loss: 1.1591 - val_acc: 0.5990 - val_top5-acc: 0.9596 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2538 - acc: 0.5610 - top5-acc: 0.9438 - val_loss: 1.1647 - val_acc: 0.5936 - val_top5-acc: 0.9604 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 21s 69ms/step - loss: 1.1931 - acc: 0.5781 - top5-acc: 0.9570\n",
      "Test accuracy: 57.81%\n",
      "Test top 5 accuracy: 95.7%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 20s 179ms/step - loss: 1.7316 - acc: 0.3867 - top5-acc: 0.8631 - val_loss: 1.4382 - val_acc: 0.4814 - val_top5-acc: 0.9298 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 1.4804 - acc: 0.4733 - top5-acc: 0.9177 - val_loss: 1.3549 - val_acc: 0.5166 - val_top5-acc: 0.9436 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.4145 - acc: 0.5019 - top5-acc: 0.9261 - val_loss: 1.3238 - val_acc: 0.5268 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3782 - acc: 0.5129 - top5-acc: 0.9318 - val_loss: 1.2961 - val_acc: 0.5410 - val_top5-acc: 0.9450 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.3636 - acc: 0.5214 - top5-acc: 0.9329 - val_loss: 1.2763 - val_acc: 0.5478 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.3557 - acc: 0.5191 - top5-acc: 0.9333 - val_loss: 1.2666 - val_acc: 0.5486 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 1.3371 - acc: 0.5254 - top5-acc: 0.9374 - val_loss: 1.2342 - val_acc: 0.5622 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 14s 158ms/step - loss: 1.3277 - acc: 0.5296 - top5-acc: 0.9381 - val_loss: 1.2565 - val_acc: 0.5522 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.3166 - acc: 0.5346 - top5-acc: 0.9365 - val_loss: 1.2607 - val_acc: 0.5548 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.3216 - acc: 0.5333 - top5-acc: 0.9359 - val_loss: 1.2110 - val_acc: 0.5710 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.3165 - acc: 0.5323 - top5-acc: 0.9388 - val_loss: 1.2559 - val_acc: 0.5530 - val_top5-acc: 0.9484 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.3039 - acc: 0.5384 - top5-acc: 0.9384 - val_loss: 1.2295 - val_acc: 0.5612 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2978 - acc: 0.5403 - top5-acc: 0.9399 - val_loss: 1.2163 - val_acc: 0.5694 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.2982 - acc: 0.5398 - top5-acc: 0.9405 - val_loss: 1.2154 - val_acc: 0.5716 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.2979 - acc: 0.5418 - top5-acc: 0.9394 - val_loss: 1.2403 - val_acc: 0.5590 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2805 - acc: 0.5480 - top5-acc: 0.9426 - val_loss: 1.1894 - val_acc: 0.5780 - val_top5-acc: 0.9540 - lr: 0.0025\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.2780 - acc: 0.5457 - top5-acc: 0.9421 - val_loss: 1.1825 - val_acc: 0.5802 - val_top5-acc: 0.9592 - lr: 0.0025\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2751 - acc: 0.5479 - top5-acc: 0.9409 - val_loss: 1.1957 - val_acc: 0.5764 - val_top5-acc: 0.9536 - lr: 0.0025\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2805 - acc: 0.5471 - top5-acc: 0.9420 - val_loss: 1.2006 - val_acc: 0.5768 - val_top5-acc: 0.9570 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2739 - acc: 0.5489 - top5-acc: 0.9430 - val_loss: 1.1933 - val_acc: 0.5788 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.2711 - acc: 0.5487 - top5-acc: 0.9432 - val_loss: 1.1888 - val_acc: 0.5826 - val_top5-acc: 0.9556 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.2690 - acc: 0.5514 - top5-acc: 0.9439 - val_loss: 1.1906 - val_acc: 0.5814 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2575 - acc: 0.5582 - top5-acc: 0.9437 - val_loss: 1.1774 - val_acc: 0.5840 - val_top5-acc: 0.9550 - lr: 0.0012\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2649 - acc: 0.5518 - top5-acc: 0.9445 - val_loss: 1.1725 - val_acc: 0.5852 - val_top5-acc: 0.9578 - lr: 0.0012\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 14s 160ms/step - loss: 1.2589 - acc: 0.5558 - top5-acc: 0.9438 - val_loss: 1.1709 - val_acc: 0.5894 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.2619 - acc: 0.5547 - top5-acc: 0.9443 - val_loss: 1.1794 - val_acc: 0.5900 - val_top5-acc: 0.9562 - lr: 0.0012\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 1.2615 - acc: 0.5542 - top5-acc: 0.9446 - val_loss: 1.1784 - val_acc: 0.5876 - val_top5-acc: 0.9548 - lr: 0.0012\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2641 - acc: 0.5544 - top5-acc: 0.9438 - val_loss: 1.1758 - val_acc: 0.5898 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2628 - acc: 0.5518 - top5-acc: 0.9452 - val_loss: 1.1767 - val_acc: 0.5862 - val_top5-acc: 0.9562 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2643 - acc: 0.5550 - top5-acc: 0.9437 - val_loss: 1.1708 - val_acc: 0.5880 - val_top5-acc: 0.9564 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 14s 160ms/step - loss: 1.2583 - acc: 0.5539 - top5-acc: 0.9459 - val_loss: 1.1705 - val_acc: 0.5902 - val_top5-acc: 0.9568 - lr: 6.2500e-04\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 14s 160ms/step - loss: 1.2589 - acc: 0.5579 - top5-acc: 0.9453 - val_loss: 1.1714 - val_acc: 0.5900 - val_top5-acc: 0.9572 - lr: 6.2500e-04\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2602 - acc: 0.5572 - top5-acc: 0.9459 - val_loss: 1.1734 - val_acc: 0.5894 - val_top5-acc: 0.9560 - lr: 6.2500e-04\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2573 - acc: 0.5570 - top5-acc: 0.9444 - val_loss: 1.1754 - val_acc: 0.5872 - val_top5-acc: 0.9578 - lr: 6.2500e-04\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2599 - acc: 0.5563 - top5-acc: 0.9450 - val_loss: 1.1757 - val_acc: 0.5868 - val_top5-acc: 0.9572 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2654 - acc: 0.5527 - top5-acc: 0.9444 - val_loss: 1.1730 - val_acc: 0.5870 - val_top5-acc: 0.9574 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2593 - acc: 0.5576 - top5-acc: 0.9450 - val_loss: 1.1736 - val_acc: 0.5872 - val_top5-acc: 0.9576 - lr: 3.1250e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2599 - acc: 0.5567 - top5-acc: 0.9446 - val_loss: 1.1738 - val_acc: 0.5886 - val_top5-acc: 0.9570 - lr: 3.1250e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2547 - acc: 0.5585 - top5-acc: 0.9446 - val_loss: 1.1749 - val_acc: 0.5860 - val_top5-acc: 0.9574 - lr: 3.1250e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2572 - acc: 0.5564 - top5-acc: 0.9450 - val_loss: 1.1802 - val_acc: 0.5866 - val_top5-acc: 0.9586 - lr: 3.1250e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2616 - acc: 0.5549 - top5-acc: 0.9457 - val_loss: 1.1813 - val_acc: 0.5854 - val_top5-acc: 0.9572 - lr: 3.1250e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2545 - acc: 0.5605 - top5-acc: 0.9446 - val_loss: 1.1759 - val_acc: 0.5874 - val_top5-acc: 0.9562 - lr: 1.5625e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2589 - acc: 0.5586 - top5-acc: 0.9448 - val_loss: 1.1802 - val_acc: 0.5842 - val_top5-acc: 0.9552 - lr: 1.5625e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2552 - acc: 0.5602 - top5-acc: 0.9456 - val_loss: 1.1775 - val_acc: 0.5890 - val_top5-acc: 0.9562 - lr: 1.5625e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2557 - acc: 0.5584 - top5-acc: 0.9448 - val_loss: 1.1801 - val_acc: 0.5870 - val_top5-acc: 0.9554 - lr: 1.5625e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2650 - acc: 0.5532 - top5-acc: 0.9440 - val_loss: 1.1808 - val_acc: 0.5892 - val_top5-acc: 0.9558 - lr: 1.5625e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2587 - acc: 0.5586 - top5-acc: 0.9448 - val_loss: 1.1810 - val_acc: 0.5874 - val_top5-acc: 0.9552 - lr: 7.8125e-05\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2550 - acc: 0.5570 - top5-acc: 0.9449 - val_loss: 1.1816 - val_acc: 0.5866 - val_top5-acc: 0.9554 - lr: 7.8125e-05\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.2631 - acc: 0.5584 - top5-acc: 0.9443 - val_loss: 1.1833 - val_acc: 0.5876 - val_top5-acc: 0.9548 - lr: 7.8125e-05\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.2618 - acc: 0.5563 - top5-acc: 0.9443 - val_loss: 1.1843 - val_acc: 0.5870 - val_top5-acc: 0.9560 - lr: 7.8125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 22s 71ms/step - loss: 1.2082 - acc: 0.5751 - top5-acc: 0.9561\n",
      "Test accuracy: 57.51%\n",
      "Test top 5 accuracy: 95.61%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 20s 181ms/step - loss: 1.7834 - acc: 0.3751 - top5-acc: 0.8528 - val_loss: 1.4861 - val_acc: 0.4720 - val_top5-acc: 0.9218 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.4984 - acc: 0.4691 - top5-acc: 0.9149 - val_loss: 1.3821 - val_acc: 0.5222 - val_top5-acc: 0.9304 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.4294 - acc: 0.4966 - top5-acc: 0.9226 - val_loss: 1.3691 - val_acc: 0.5124 - val_top5-acc: 0.9308 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.4011 - acc: 0.5077 - top5-acc: 0.9276 - val_loss: 1.3036 - val_acc: 0.5418 - val_top5-acc: 0.9376 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3753 - acc: 0.5176 - top5-acc: 0.9306 - val_loss: 1.3104 - val_acc: 0.5418 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3620 - acc: 0.5210 - top5-acc: 0.9324 - val_loss: 1.2807 - val_acc: 0.5470 - val_top5-acc: 0.9436 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3485 - acc: 0.5248 - top5-acc: 0.9340 - val_loss: 1.2683 - val_acc: 0.5550 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3385 - acc: 0.5291 - top5-acc: 0.9367 - val_loss: 1.2815 - val_acc: 0.5468 - val_top5-acc: 0.9384 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.3361 - acc: 0.5282 - top5-acc: 0.9357 - val_loss: 1.2715 - val_acc: 0.5504 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.3226 - acc: 0.5342 - top5-acc: 0.9372 - val_loss: 1.2629 - val_acc: 0.5520 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.3249 - acc: 0.5323 - top5-acc: 0.9364 - val_loss: 1.2262 - val_acc: 0.5724 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3219 - acc: 0.5350 - top5-acc: 0.9384 - val_loss: 1.2315 - val_acc: 0.5714 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3230 - acc: 0.5338 - top5-acc: 0.9368 - val_loss: 1.2243 - val_acc: 0.5646 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3132 - acc: 0.5372 - top5-acc: 0.9369 - val_loss: 1.2012 - val_acc: 0.5762 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.3161 - acc: 0.5356 - top5-acc: 0.9379 - val_loss: 1.2572 - val_acc: 0.5568 - val_top5-acc: 0.9458 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.3068 - acc: 0.5427 - top5-acc: 0.9378 - val_loss: 1.2184 - val_acc: 0.5692 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3032 - acc: 0.5396 - top5-acc: 0.9390 - val_loss: 1.2134 - val_acc: 0.5746 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3053 - acc: 0.5401 - top5-acc: 0.9393 - val_loss: 1.1935 - val_acc: 0.5844 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.2993 - acc: 0.5431 - top5-acc: 0.9402 - val_loss: 1.2185 - val_acc: 0.5712 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3096 - acc: 0.5395 - top5-acc: 0.9385 - val_loss: 1.2163 - val_acc: 0.5710 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3042 - acc: 0.5391 - top5-acc: 0.9395 - val_loss: 1.2168 - val_acc: 0.5736 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.3101 - acc: 0.5372 - top5-acc: 0.9392 - val_loss: 1.2139 - val_acc: 0.5718 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.2959 - acc: 0.5416 - top5-acc: 0.9407 - val_loss: 1.2065 - val_acc: 0.5760 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.2772 - acc: 0.5481 - top5-acc: 0.9417 - val_loss: 1.1807 - val_acc: 0.5946 - val_top5-acc: 0.9514 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2731 - acc: 0.5521 - top5-acc: 0.9404 - val_loss: 1.1968 - val_acc: 0.5822 - val_top5-acc: 0.9540 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2832 - acc: 0.5478 - top5-acc: 0.9412 - val_loss: 1.1751 - val_acc: 0.5904 - val_top5-acc: 0.9544 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2766 - acc: 0.5498 - top5-acc: 0.9412 - val_loss: 1.1775 - val_acc: 0.5914 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2782 - acc: 0.5500 - top5-acc: 0.9420 - val_loss: 1.1754 - val_acc: 0.5892 - val_top5-acc: 0.9556 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2738 - acc: 0.5492 - top5-acc: 0.9425 - val_loss: 1.1827 - val_acc: 0.5880 - val_top5-acc: 0.9544 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2761 - acc: 0.5509 - top5-acc: 0.9429 - val_loss: 1.1797 - val_acc: 0.5896 - val_top5-acc: 0.9548 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2777 - acc: 0.5523 - top5-acc: 0.9415 - val_loss: 1.1874 - val_acc: 0.5870 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2689 - acc: 0.5542 - top5-acc: 0.9429 - val_loss: 1.1678 - val_acc: 0.5954 - val_top5-acc: 0.9560 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.2677 - acc: 0.5541 - top5-acc: 0.9424 - val_loss: 1.1759 - val_acc: 0.5882 - val_top5-acc: 0.9556 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2649 - acc: 0.5529 - top5-acc: 0.9438 - val_loss: 1.1753 - val_acc: 0.5874 - val_top5-acc: 0.9554 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2663 - acc: 0.5553 - top5-acc: 0.9431 - val_loss: 1.1718 - val_acc: 0.5950 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2637 - acc: 0.5563 - top5-acc: 0.9433 - val_loss: 1.1701 - val_acc: 0.5964 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2711 - acc: 0.5510 - top5-acc: 0.9424 - val_loss: 1.1855 - val_acc: 0.5854 - val_top5-acc: 0.9566 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2661 - acc: 0.5535 - top5-acc: 0.9430 - val_loss: 1.1674 - val_acc: 0.5936 - val_top5-acc: 0.9570 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2641 - acc: 0.5543 - top5-acc: 0.9434 - val_loss: 1.1748 - val_acc: 0.5930 - val_top5-acc: 0.9586 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.2602 - acc: 0.5558 - top5-acc: 0.9438 - val_loss: 1.1698 - val_acc: 0.5938 - val_top5-acc: 0.9588 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.2653 - acc: 0.5566 - top5-acc: 0.9425 - val_loss: 1.1798 - val_acc: 0.5864 - val_top5-acc: 0.9572 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.2618 - acc: 0.5557 - top5-acc: 0.9441 - val_loss: 1.1735 - val_acc: 0.5934 - val_top5-acc: 0.9566 - lr: 6.2500e-04\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2641 - acc: 0.5566 - top5-acc: 0.9436 - val_loss: 1.1708 - val_acc: 0.5946 - val_top5-acc: 0.9572 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2616 - acc: 0.5572 - top5-acc: 0.9435 - val_loss: 1.1751 - val_acc: 0.5928 - val_top5-acc: 0.9588 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2654 - acc: 0.5571 - top5-acc: 0.9426 - val_loss: 1.1749 - val_acc: 0.5944 - val_top5-acc: 0.9564 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2642 - acc: 0.5551 - top5-acc: 0.9426 - val_loss: 1.1795 - val_acc: 0.5892 - val_top5-acc: 0.9560 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2624 - acc: 0.5588 - top5-acc: 0.9428 - val_loss: 1.1773 - val_acc: 0.5956 - val_top5-acc: 0.9570 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2643 - acc: 0.5575 - top5-acc: 0.9446 - val_loss: 1.1763 - val_acc: 0.5948 - val_top5-acc: 0.9588 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.2620 - acc: 0.5568 - top5-acc: 0.9449 - val_loss: 1.1756 - val_acc: 0.5952 - val_top5-acc: 0.9582 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.2677 - acc: 0.5606 - top5-acc: 0.9433 - val_loss: 1.1773 - val_acc: 0.5954 - val_top5-acc: 0.9574 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 1.2055 - acc: 0.5791 - top5-acc: 0.9535\n",
      "Test accuracy: 57.91%\n",
      "Test top 5 accuracy: 95.35%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 22s 190ms/step - loss: 1.7808 - acc: 0.3719 - top5-acc: 0.8511 - val_loss: 1.5058 - val_acc: 0.4676 - val_top5-acc: 0.9156 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.4844 - acc: 0.4758 - top5-acc: 0.9144 - val_loss: 1.3727 - val_acc: 0.5100 - val_top5-acc: 0.9374 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.4225 - acc: 0.4987 - top5-acc: 0.9231 - val_loss: 1.3269 - val_acc: 0.5280 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.3852 - acc: 0.5116 - top5-acc: 0.9291 - val_loss: 1.3004 - val_acc: 0.5380 - val_top5-acc: 0.9426 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.3622 - acc: 0.5195 - top5-acc: 0.9309 - val_loss: 1.2987 - val_acc: 0.5376 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.3437 - acc: 0.5260 - top5-acc: 0.9343 - val_loss: 1.2705 - val_acc: 0.5494 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.3314 - acc: 0.5289 - top5-acc: 0.9360 - val_loss: 1.2519 - val_acc: 0.5584 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.3284 - acc: 0.5318 - top5-acc: 0.9363 - val_loss: 1.2750 - val_acc: 0.5484 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.3215 - acc: 0.5332 - top5-acc: 0.9359 - val_loss: 1.2381 - val_acc: 0.5628 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.3131 - acc: 0.5375 - top5-acc: 0.9364 - val_loss: 1.2336 - val_acc: 0.5714 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.3005 - acc: 0.5383 - top5-acc: 0.9391 - val_loss: 1.2166 - val_acc: 0.5746 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.3020 - acc: 0.5381 - top5-acc: 0.9386 - val_loss: 1.2325 - val_acc: 0.5614 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.2896 - acc: 0.5426 - top5-acc: 0.9419 - val_loss: 1.2368 - val_acc: 0.5574 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2856 - acc: 0.5460 - top5-acc: 0.9416 - val_loss: 1.2175 - val_acc: 0.5676 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 15s 176ms/step - loss: 1.2906 - acc: 0.5442 - top5-acc: 0.9402 - val_loss: 1.1976 - val_acc: 0.5744 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2881 - acc: 0.5488 - top5-acc: 0.9403 - val_loss: 1.1762 - val_acc: 0.5842 - val_top5-acc: 0.9578 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2886 - acc: 0.5470 - top5-acc: 0.9406 - val_loss: 1.2004 - val_acc: 0.5792 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2842 - acc: 0.5485 - top5-acc: 0.9405 - val_loss: 1.1937 - val_acc: 0.5828 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2847 - acc: 0.5443 - top5-acc: 0.9407 - val_loss: 1.1776 - val_acc: 0.5912 - val_top5-acc: 0.9570 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2849 - acc: 0.5460 - top5-acc: 0.9398 - val_loss: 1.1876 - val_acc: 0.5814 - val_top5-acc: 0.9570 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.2880 - acc: 0.5454 - top5-acc: 0.9422 - val_loss: 1.1891 - val_acc: 0.5788 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.2661 - acc: 0.5521 - top5-acc: 0.9432 - val_loss: 1.1643 - val_acc: 0.5972 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2587 - acc: 0.5584 - top5-acc: 0.9448 - val_loss: 1.1645 - val_acc: 0.5952 - val_top5-acc: 0.9584 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.2613 - acc: 0.5545 - top5-acc: 0.9439 - val_loss: 1.1643 - val_acc: 0.5992 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 15s 176ms/step - loss: 1.2606 - acc: 0.5576 - top5-acc: 0.9429 - val_loss: 1.1694 - val_acc: 0.5894 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.2618 - acc: 0.5527 - top5-acc: 0.9435 - val_loss: 1.1816 - val_acc: 0.5844 - val_top5-acc: 0.9590 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 15s 176ms/step - loss: 1.2608 - acc: 0.5578 - top5-acc: 0.9426 - val_loss: 1.1595 - val_acc: 0.5948 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.2561 - acc: 0.5563 - top5-acc: 0.9431 - val_loss: 1.1720 - val_acc: 0.5966 - val_top5-acc: 0.9550 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 15s 176ms/step - loss: 1.2603 - acc: 0.5561 - top5-acc: 0.9437 - val_loss: 1.1685 - val_acc: 0.5916 - val_top5-acc: 0.9580 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.2623 - acc: 0.5546 - top5-acc: 0.9433 - val_loss: 1.1711 - val_acc: 0.5920 - val_top5-acc: 0.9550 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 16s 177ms/step - loss: 1.2662 - acc: 0.5540 - top5-acc: 0.9432 - val_loss: 1.1657 - val_acc: 0.5922 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 16s 177ms/step - loss: 1.2603 - acc: 0.5540 - top5-acc: 0.9444 - val_loss: 1.1665 - val_acc: 0.5966 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 16s 177ms/step - loss: 1.2541 - acc: 0.5566 - top5-acc: 0.9449 - val_loss: 1.1613 - val_acc: 0.5992 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 15s 176ms/step - loss: 1.2478 - acc: 0.5612 - top5-acc: 0.9461 - val_loss: 1.1590 - val_acc: 0.5954 - val_top5-acc: 0.9560 - lr: 0.0012\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 16s 184ms/step - loss: 1.2507 - acc: 0.5593 - top5-acc: 0.9442 - val_loss: 1.1604 - val_acc: 0.5986 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2476 - acc: 0.5600 - top5-acc: 0.9446 - val_loss: 1.1621 - val_acc: 0.5948 - val_top5-acc: 0.9572 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2548 - acc: 0.5578 - top5-acc: 0.9445 - val_loss: 1.1599 - val_acc: 0.5946 - val_top5-acc: 0.9582 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 16s 179ms/step - loss: 1.2521 - acc: 0.5575 - top5-acc: 0.9433 - val_loss: 1.1591 - val_acc: 0.6030 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2490 - acc: 0.5598 - top5-acc: 0.9450 - val_loss: 1.1598 - val_acc: 0.5990 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 16s 180ms/step - loss: 1.2435 - acc: 0.5613 - top5-acc: 0.9462 - val_loss: 1.1559 - val_acc: 0.6032 - val_top5-acc: 0.9580 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 16s 179ms/step - loss: 1.2487 - acc: 0.5609 - top5-acc: 0.9443 - val_loss: 1.1571 - val_acc: 0.6006 - val_top5-acc: 0.9580 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 16s 178ms/step - loss: 1.2458 - acc: 0.5585 - top5-acc: 0.9457 - val_loss: 1.1574 - val_acc: 0.6018 - val_top5-acc: 0.9578 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 16s 178ms/step - loss: 1.2522 - acc: 0.5588 - top5-acc: 0.9454 - val_loss: 1.1572 - val_acc: 0.6036 - val_top5-acc: 0.9582 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 16s 179ms/step - loss: 1.2453 - acc: 0.5604 - top5-acc: 0.9477 - val_loss: 1.1631 - val_acc: 0.5984 - val_top5-acc: 0.9564 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 16s 180ms/step - loss: 1.2484 - acc: 0.5613 - top5-acc: 0.9449 - val_loss: 1.1621 - val_acc: 0.5902 - val_top5-acc: 0.9572 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2417 - acc: 0.5629 - top5-acc: 0.9462 - val_loss: 1.1577 - val_acc: 0.5988 - val_top5-acc: 0.9578 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 16s 178ms/step - loss: 1.2479 - acc: 0.5615 - top5-acc: 0.9456 - val_loss: 1.1569 - val_acc: 0.6030 - val_top5-acc: 0.9576 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 16s 177ms/step - loss: 1.2453 - acc: 0.5596 - top5-acc: 0.9460 - val_loss: 1.1627 - val_acc: 0.6026 - val_top5-acc: 0.9562 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 16s 177ms/step - loss: 1.2452 - acc: 0.5635 - top5-acc: 0.9449 - val_loss: 1.1608 - val_acc: 0.6028 - val_top5-acc: 0.9578 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 16s 177ms/step - loss: 1.2485 - acc: 0.5602 - top5-acc: 0.9451 - val_loss: 1.1617 - val_acc: 0.6000 - val_top5-acc: 0.9572 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 1.1883 - acc: 0.5860 - top5-acc: 0.9544\n",
      "Test accuracy: 58.6%\n",
      "Test top 5 accuracy: 95.44%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 22s 205ms/step - loss: 1.7889 - acc: 0.3745 - top5-acc: 0.8504 - val_loss: 1.4599 - val_acc: 0.4790 - val_top5-acc: 0.9282 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.4755 - acc: 0.4785 - top5-acc: 0.9167 - val_loss: 1.3553 - val_acc: 0.5172 - val_top5-acc: 0.9380 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.4124 - acc: 0.5018 - top5-acc: 0.9261 - val_loss: 1.3012 - val_acc: 0.5416 - val_top5-acc: 0.9414 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.3816 - acc: 0.5112 - top5-acc: 0.9307 - val_loss: 1.2952 - val_acc: 0.5460 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.3506 - acc: 0.5265 - top5-acc: 0.9330 - val_loss: 1.2493 - val_acc: 0.5630 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.3331 - acc: 0.5296 - top5-acc: 0.9355 - val_loss: 1.2390 - val_acc: 0.5648 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.3262 - acc: 0.5296 - top5-acc: 0.9369 - val_loss: 1.2261 - val_acc: 0.5692 - val_top5-acc: 0.9484 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.3132 - acc: 0.5376 - top5-acc: 0.9375 - val_loss: 1.2125 - val_acc: 0.5776 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.3018 - acc: 0.5427 - top5-acc: 0.9403 - val_loss: 1.2180 - val_acc: 0.5658 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 16s 186ms/step - loss: 1.2974 - acc: 0.5436 - top5-acc: 0.9406 - val_loss: 1.2113 - val_acc: 0.5674 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 16s 186ms/step - loss: 1.3032 - acc: 0.5403 - top5-acc: 0.9395 - val_loss: 1.2091 - val_acc: 0.5716 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2892 - acc: 0.5446 - top5-acc: 0.9395 - val_loss: 1.1956 - val_acc: 0.5798 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2891 - acc: 0.5435 - top5-acc: 0.9411 - val_loss: 1.1927 - val_acc: 0.5820 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2875 - acc: 0.5440 - top5-acc: 0.9396 - val_loss: 1.1909 - val_acc: 0.5770 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2843 - acc: 0.5476 - top5-acc: 0.9410 - val_loss: 1.1854 - val_acc: 0.5778 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2816 - acc: 0.5497 - top5-acc: 0.9411 - val_loss: 1.1912 - val_acc: 0.5834 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2814 - acc: 0.5480 - top5-acc: 0.9421 - val_loss: 1.1835 - val_acc: 0.5848 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2757 - acc: 0.5500 - top5-acc: 0.9417 - val_loss: 1.1731 - val_acc: 0.5856 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2778 - acc: 0.5504 - top5-acc: 0.9428 - val_loss: 1.1723 - val_acc: 0.5898 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2697 - acc: 0.5518 - top5-acc: 0.9427 - val_loss: 1.1571 - val_acc: 0.5964 - val_top5-acc: 0.9580 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2749 - acc: 0.5487 - top5-acc: 0.9427 - val_loss: 1.1651 - val_acc: 0.5880 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2739 - acc: 0.5502 - top5-acc: 0.9418 - val_loss: 1.1828 - val_acc: 0.5840 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2634 - acc: 0.5538 - top5-acc: 0.9440 - val_loss: 1.1685 - val_acc: 0.5900 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2718 - acc: 0.5516 - top5-acc: 0.9414 - val_loss: 1.1754 - val_acc: 0.5880 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2671 - acc: 0.5518 - top5-acc: 0.9441 - val_loss: 1.1628 - val_acc: 0.5896 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 17s 194ms/step - loss: 1.2550 - acc: 0.5546 - top5-acc: 0.9448 - val_loss: 1.1474 - val_acc: 0.6008 - val_top5-acc: 0.9556 - lr: 0.0025\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 17s 197ms/step - loss: 1.2462 - acc: 0.5612 - top5-acc: 0.9453 - val_loss: 1.1605 - val_acc: 0.5936 - val_top5-acc: 0.9568 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.2486 - acc: 0.5562 - top5-acc: 0.9451 - val_loss: 1.1437 - val_acc: 0.6000 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.2516 - acc: 0.5589 - top5-acc: 0.9462 - val_loss: 1.1510 - val_acc: 0.5944 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.2553 - acc: 0.5546 - top5-acc: 0.9428 - val_loss: 1.1492 - val_acc: 0.5922 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2483 - acc: 0.5598 - top5-acc: 0.9451 - val_loss: 1.1500 - val_acc: 0.5968 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2493 - acc: 0.5586 - top5-acc: 0.9433 - val_loss: 1.1541 - val_acc: 0.5914 - val_top5-acc: 0.9592 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.2518 - acc: 0.5561 - top5-acc: 0.9454 - val_loss: 1.1458 - val_acc: 0.6020 - val_top5-acc: 0.9592 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.2393 - acc: 0.5622 - top5-acc: 0.9471 - val_loss: 1.1419 - val_acc: 0.6004 - val_top5-acc: 0.9584 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2418 - acc: 0.5619 - top5-acc: 0.9452 - val_loss: 1.1362 - val_acc: 0.6034 - val_top5-acc: 0.9568 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.2381 - acc: 0.5643 - top5-acc: 0.9469 - val_loss: 1.1453 - val_acc: 0.5990 - val_top5-acc: 0.9582 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2374 - acc: 0.5658 - top5-acc: 0.9455 - val_loss: 1.1491 - val_acc: 0.5944 - val_top5-acc: 0.9560 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.2375 - acc: 0.5652 - top5-acc: 0.9462 - val_loss: 1.1402 - val_acc: 0.6024 - val_top5-acc: 0.9582 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.2360 - acc: 0.5638 - top5-acc: 0.9472 - val_loss: 1.1408 - val_acc: 0.6014 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.2375 - acc: 0.5630 - top5-acc: 0.9460 - val_loss: 1.1492 - val_acc: 0.5910 - val_top5-acc: 0.9586 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2326 - acc: 0.5651 - top5-acc: 0.9464 - val_loss: 1.1413 - val_acc: 0.6008 - val_top5-acc: 0.9572 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2372 - acc: 0.5664 - top5-acc: 0.9472 - val_loss: 1.1370 - val_acc: 0.5996 - val_top5-acc: 0.9570 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2340 - acc: 0.5648 - top5-acc: 0.9467 - val_loss: 1.1405 - val_acc: 0.6024 - val_top5-acc: 0.9590 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 16s 180ms/step - loss: 1.2401 - acc: 0.5626 - top5-acc: 0.9465 - val_loss: 1.1447 - val_acc: 0.5994 - val_top5-acc: 0.9568 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 16s 184ms/step - loss: 1.2396 - acc: 0.5632 - top5-acc: 0.9456 - val_loss: 1.1457 - val_acc: 0.6000 - val_top5-acc: 0.9566 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 17s 195ms/step - loss: 1.2332 - acc: 0.5658 - top5-acc: 0.9473 - val_loss: 1.1385 - val_acc: 0.6034 - val_top5-acc: 0.9580 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2360 - acc: 0.5663 - top5-acc: 0.9464 - val_loss: 1.1422 - val_acc: 0.6020 - val_top5-acc: 0.9586 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2358 - acc: 0.5670 - top5-acc: 0.9461 - val_loss: 1.1400 - val_acc: 0.6046 - val_top5-acc: 0.9582 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2390 - acc: 0.5653 - top5-acc: 0.9469 - val_loss: 1.1458 - val_acc: 0.6014 - val_top5-acc: 0.9576 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2327 - acc: 0.5658 - top5-acc: 0.9464 - val_loss: 1.1446 - val_acc: 0.6054 - val_top5-acc: 0.9582 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 1.1706 - acc: 0.5919 - top5-acc: 0.9553\n",
      "Test accuracy: 59.19%\n",
      "Test top 5 accuracy: 95.53%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 23s 213ms/step - loss: 1.7903 - acc: 0.3673 - top5-acc: 0.8466 - val_loss: 1.5165 - val_acc: 0.4654 - val_top5-acc: 0.9122 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.5197 - acc: 0.4647 - top5-acc: 0.9076 - val_loss: 1.4031 - val_acc: 0.5018 - val_top5-acc: 0.9292 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.4464 - acc: 0.4910 - top5-acc: 0.9200 - val_loss: 1.3651 - val_acc: 0.5182 - val_top5-acc: 0.9312 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.4149 - acc: 0.5053 - top5-acc: 0.9249 - val_loss: 1.3257 - val_acc: 0.5294 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.3864 - acc: 0.5116 - top5-acc: 0.9286 - val_loss: 1.3243 - val_acc: 0.5290 - val_top5-acc: 0.9408 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.3612 - acc: 0.5206 - top5-acc: 0.9326 - val_loss: 1.2817 - val_acc: 0.5408 - val_top5-acc: 0.9436 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.3644 - acc: 0.5228 - top5-acc: 0.9301 - val_loss: 1.2678 - val_acc: 0.5498 - val_top5-acc: 0.9484 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.3453 - acc: 0.5260 - top5-acc: 0.9341 - val_loss: 1.2565 - val_acc: 0.5542 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.3378 - acc: 0.5269 - top5-acc: 0.9362 - val_loss: 1.2460 - val_acc: 0.5548 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.3371 - acc: 0.5316 - top5-acc: 0.9342 - val_loss: 1.2421 - val_acc: 0.5594 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.3243 - acc: 0.5333 - top5-acc: 0.9363 - val_loss: 1.2596 - val_acc: 0.5500 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.3319 - acc: 0.5301 - top5-acc: 0.9360 - val_loss: 1.2437 - val_acc: 0.5596 - val_top5-acc: 0.9484 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.3236 - acc: 0.5324 - top5-acc: 0.9373 - val_loss: 1.2122 - val_acc: 0.5704 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.3136 - acc: 0.5356 - top5-acc: 0.9366 - val_loss: 1.2514 - val_acc: 0.5462 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.3093 - acc: 0.5387 - top5-acc: 0.9382 - val_loss: 1.2351 - val_acc: 0.5636 - val_top5-acc: 0.9490 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.3154 - acc: 0.5366 - top5-acc: 0.9385 - val_loss: 1.2019 - val_acc: 0.5728 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.3061 - acc: 0.5391 - top5-acc: 0.9389 - val_loss: 1.2142 - val_acc: 0.5692 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.3154 - acc: 0.5376 - top5-acc: 0.9375 - val_loss: 1.2023 - val_acc: 0.5774 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 17s 189ms/step - loss: 1.3036 - acc: 0.5390 - top5-acc: 0.9394 - val_loss: 1.2022 - val_acc: 0.5734 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2975 - acc: 0.5406 - top5-acc: 0.9404 - val_loss: 1.1998 - val_acc: 0.5774 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.3099 - acc: 0.5395 - top5-acc: 0.9377 - val_loss: 1.1927 - val_acc: 0.5752 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.3028 - acc: 0.5420 - top5-acc: 0.9391 - val_loss: 1.1956 - val_acc: 0.5746 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2951 - acc: 0.5437 - top5-acc: 0.9392 - val_loss: 1.1974 - val_acc: 0.5808 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2945 - acc: 0.5417 - top5-acc: 0.9406 - val_loss: 1.1927 - val_acc: 0.5782 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2950 - acc: 0.5418 - top5-acc: 0.9390 - val_loss: 1.1813 - val_acc: 0.5806 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2926 - acc: 0.5426 - top5-acc: 0.9402 - val_loss: 1.1924 - val_acc: 0.5780 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2844 - acc: 0.5471 - top5-acc: 0.9421 - val_loss: 1.1940 - val_acc: 0.5796 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2908 - acc: 0.5448 - top5-acc: 0.9407 - val_loss: 1.2032 - val_acc: 0.5754 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.2954 - acc: 0.5422 - top5-acc: 0.9394 - val_loss: 1.2116 - val_acc: 0.5730 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2927 - acc: 0.5441 - top5-acc: 0.9399 - val_loss: 1.2035 - val_acc: 0.5676 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.2792 - acc: 0.5487 - top5-acc: 0.9420 - val_loss: 1.1804 - val_acc: 0.5870 - val_top5-acc: 0.9568 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2755 - acc: 0.5504 - top5-acc: 0.9428 - val_loss: 1.1635 - val_acc: 0.5918 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2749 - acc: 0.5485 - top5-acc: 0.9415 - val_loss: 1.1700 - val_acc: 0.5876 - val_top5-acc: 0.9536 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2761 - acc: 0.5485 - top5-acc: 0.9417 - val_loss: 1.1687 - val_acc: 0.5826 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2787 - acc: 0.5503 - top5-acc: 0.9404 - val_loss: 1.1801 - val_acc: 0.5828 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2726 - acc: 0.5529 - top5-acc: 0.9418 - val_loss: 1.1686 - val_acc: 0.5906 - val_top5-acc: 0.9540 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2693 - acc: 0.5538 - top5-acc: 0.9427 - val_loss: 1.1659 - val_acc: 0.5878 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2660 - acc: 0.5556 - top5-acc: 0.9430 - val_loss: 1.1621 - val_acc: 0.5894 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2692 - acc: 0.5532 - top5-acc: 0.9449 - val_loss: 1.1644 - val_acc: 0.5816 - val_top5-acc: 0.9550 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2657 - acc: 0.5544 - top5-acc: 0.9418 - val_loss: 1.1659 - val_acc: 0.5908 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2623 - acc: 0.5558 - top5-acc: 0.9427 - val_loss: 1.1703 - val_acc: 0.5898 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 16s 188ms/step - loss: 1.2641 - acc: 0.5550 - top5-acc: 0.9438 - val_loss: 1.1680 - val_acc: 0.5898 - val_top5-acc: 0.9542 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2655 - acc: 0.5517 - top5-acc: 0.9448 - val_loss: 1.1732 - val_acc: 0.5878 - val_top5-acc: 0.9538 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.2615 - acc: 0.5564 - top5-acc: 0.9435 - val_loss: 1.1656 - val_acc: 0.5878 - val_top5-acc: 0.9552 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 18s 202ms/step - loss: 1.2628 - acc: 0.5566 - top5-acc: 0.9440 - val_loss: 1.1684 - val_acc: 0.5940 - val_top5-acc: 0.9562 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2616 - acc: 0.5565 - top5-acc: 0.9436 - val_loss: 1.1688 - val_acc: 0.5862 - val_top5-acc: 0.9566 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2622 - acc: 0.5564 - top5-acc: 0.9442 - val_loss: 1.1652 - val_acc: 0.5874 - val_top5-acc: 0.9550 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2622 - acc: 0.5554 - top5-acc: 0.9432 - val_loss: 1.1712 - val_acc: 0.5876 - val_top5-acc: 0.9566 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2630 - acc: 0.5559 - top5-acc: 0.9422 - val_loss: 1.1668 - val_acc: 0.5844 - val_top5-acc: 0.9554 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2620 - acc: 0.5565 - top5-acc: 0.9435 - val_loss: 1.1658 - val_acc: 0.5866 - val_top5-acc: 0.9546 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 27s 85ms/step - loss: 1.1872 - acc: 0.5821 - top5-acc: 0.9571\n",
      "Test accuracy: 58.21%\n",
      "Test top 5 accuracy: 95.71%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 23s 213ms/step - loss: 1.7239 - acc: 0.3932 - top5-acc: 0.8644 - val_loss: 1.4491 - val_acc: 0.4986 - val_top5-acc: 0.9250 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.4751 - acc: 0.4798 - top5-acc: 0.9153 - val_loss: 1.3630 - val_acc: 0.5262 - val_top5-acc: 0.9312 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.4094 - acc: 0.5058 - top5-acc: 0.9272 - val_loss: 1.3324 - val_acc: 0.5338 - val_top5-acc: 0.9318 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.3786 - acc: 0.5135 - top5-acc: 0.9293 - val_loss: 1.2805 - val_acc: 0.5598 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.3636 - acc: 0.5200 - top5-acc: 0.9311 - val_loss: 1.2747 - val_acc: 0.5458 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.3358 - acc: 0.5312 - top5-acc: 0.9354 - val_loss: 1.2509 - val_acc: 0.5624 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.3331 - acc: 0.5304 - top5-acc: 0.9344 - val_loss: 1.2380 - val_acc: 0.5616 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 17s 195ms/step - loss: 1.3232 - acc: 0.5322 - top5-acc: 0.9365 - val_loss: 1.2491 - val_acc: 0.5558 - val_top5-acc: 0.9448 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 17s 195ms/step - loss: 1.3179 - acc: 0.5350 - top5-acc: 0.9373 - val_loss: 1.2272 - val_acc: 0.5718 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.2995 - acc: 0.5433 - top5-acc: 0.9391 - val_loss: 1.2208 - val_acc: 0.5600 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 17s 196ms/step - loss: 1.2963 - acc: 0.5452 - top5-acc: 0.9379 - val_loss: 1.2257 - val_acc: 0.5648 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 18s 201ms/step - loss: 1.3054 - acc: 0.5382 - top5-acc: 0.9390 - val_loss: 1.2048 - val_acc: 0.5732 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 18s 203ms/step - loss: 1.2912 - acc: 0.5430 - top5-acc: 0.9412 - val_loss: 1.2002 - val_acc: 0.5726 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 18s 203ms/step - loss: 1.2923 - acc: 0.5432 - top5-acc: 0.9390 - val_loss: 1.1976 - val_acc: 0.5776 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2844 - acc: 0.5448 - top5-acc: 0.9408 - val_loss: 1.1845 - val_acc: 0.5808 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2796 - acc: 0.5493 - top5-acc: 0.9430 - val_loss: 1.1922 - val_acc: 0.5712 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2900 - acc: 0.5450 - top5-acc: 0.9388 - val_loss: 1.2061 - val_acc: 0.5766 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2814 - acc: 0.5473 - top5-acc: 0.9418 - val_loss: 1.1911 - val_acc: 0.5668 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2867 - acc: 0.5466 - top5-acc: 0.9408 - val_loss: 1.2115 - val_acc: 0.5750 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2882 - acc: 0.5444 - top5-acc: 0.9410 - val_loss: 1.1772 - val_acc: 0.5860 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 18s 209ms/step - loss: 1.2768 - acc: 0.5497 - top5-acc: 0.9424 - val_loss: 1.1717 - val_acc: 0.5838 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2736 - acc: 0.5490 - top5-acc: 0.9431 - val_loss: 1.1851 - val_acc: 0.5762 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2797 - acc: 0.5475 - top5-acc: 0.9417 - val_loss: 1.1835 - val_acc: 0.5836 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2719 - acc: 0.5503 - top5-acc: 0.9420 - val_loss: 1.1781 - val_acc: 0.5832 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 18s 209ms/step - loss: 1.2701 - acc: 0.5491 - top5-acc: 0.9436 - val_loss: 1.1782 - val_acc: 0.5828 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2718 - acc: 0.5500 - top5-acc: 0.9422 - val_loss: 1.1642 - val_acc: 0.5836 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2735 - acc: 0.5482 - top5-acc: 0.9428 - val_loss: 1.1696 - val_acc: 0.5834 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2679 - acc: 0.5529 - top5-acc: 0.9436 - val_loss: 1.1613 - val_acc: 0.5830 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 18s 203ms/step - loss: 1.2655 - acc: 0.5554 - top5-acc: 0.9439 - val_loss: 1.1569 - val_acc: 0.5868 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 18s 206ms/step - loss: 1.2650 - acc: 0.5570 - top5-acc: 0.9419 - val_loss: 1.1753 - val_acc: 0.5800 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 19s 220ms/step - loss: 1.2678 - acc: 0.5513 - top5-acc: 0.9423 - val_loss: 1.1718 - val_acc: 0.5870 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.2673 - acc: 0.5544 - top5-acc: 0.9436 - val_loss: 1.1465 - val_acc: 0.5936 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2681 - acc: 0.5498 - top5-acc: 0.9437 - val_loss: 1.1784 - val_acc: 0.5826 - val_top5-acc: 0.9488 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 18s 201ms/step - loss: 1.2751 - acc: 0.5528 - top5-acc: 0.9423 - val_loss: 1.1616 - val_acc: 0.5868 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.2705 - acc: 0.5513 - top5-acc: 0.9418 - val_loss: 1.2013 - val_acc: 0.5746 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.2638 - acc: 0.5538 - top5-acc: 0.9418 - val_loss: 1.1594 - val_acc: 0.5866 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2775 - acc: 0.5512 - top5-acc: 0.9418 - val_loss: 1.2015 - val_acc: 0.5806 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 18s 203ms/step - loss: 1.2526 - acc: 0.5596 - top5-acc: 0.9453 - val_loss: 1.1659 - val_acc: 0.5896 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 18s 206ms/step - loss: 1.2514 - acc: 0.5589 - top5-acc: 0.9454 - val_loss: 1.1433 - val_acc: 0.5972 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.2432 - acc: 0.5632 - top5-acc: 0.9464 - val_loss: 1.1547 - val_acc: 0.5860 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2475 - acc: 0.5595 - top5-acc: 0.9460 - val_loss: 1.1492 - val_acc: 0.5968 - val_top5-acc: 0.9540 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.2439 - acc: 0.5610 - top5-acc: 0.9453 - val_loss: 1.1551 - val_acc: 0.5880 - val_top5-acc: 0.9568 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.2497 - acc: 0.5605 - top5-acc: 0.9434 - val_loss: 1.1450 - val_acc: 0.5934 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2487 - acc: 0.5585 - top5-acc: 0.9446 - val_loss: 1.1583 - val_acc: 0.5980 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 18s 209ms/step - loss: 1.2376 - acc: 0.5628 - top5-acc: 0.9464 - val_loss: 1.1452 - val_acc: 0.5968 - val_top5-acc: 0.9560 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.2375 - acc: 0.5630 - top5-acc: 0.9482 - val_loss: 1.1373 - val_acc: 0.6000 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2418 - acc: 0.5657 - top5-acc: 0.9454 - val_loss: 1.1416 - val_acc: 0.5978 - val_top5-acc: 0.9578 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 18s 210ms/step - loss: 1.2405 - acc: 0.5628 - top5-acc: 0.9456 - val_loss: 1.1440 - val_acc: 0.5926 - val_top5-acc: 0.9600 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.2410 - acc: 0.5642 - top5-acc: 0.9459 - val_loss: 1.1432 - val_acc: 0.5944 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.2418 - acc: 0.5630 - top5-acc: 0.9464 - val_loss: 1.1438 - val_acc: 0.5982 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "313/313 [==============================] - 29s 94ms/step - loss: 1.1637 - acc: 0.5906 - top5-acc: 0.9563\n",
      "Test accuracy: 59.06%\n",
      "Test top 5 accuracy: 95.63%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 26s 242ms/step - loss: 1.7665 - acc: 0.3846 - top5-acc: 0.8513 - val_loss: 1.4619 - val_acc: 0.4956 - val_top5-acc: 0.9230 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.4877 - acc: 0.4812 - top5-acc: 0.9121 - val_loss: 1.3686 - val_acc: 0.5206 - val_top5-acc: 0.9320 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.4154 - acc: 0.5052 - top5-acc: 0.9236 - val_loss: 1.3340 - val_acc: 0.5388 - val_top5-acc: 0.9396 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.3744 - acc: 0.5178 - top5-acc: 0.9286 - val_loss: 1.2953 - val_acc: 0.5456 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.3559 - acc: 0.5247 - top5-acc: 0.9322 - val_loss: 1.2731 - val_acc: 0.5530 - val_top5-acc: 0.9458 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.3435 - acc: 0.5262 - top5-acc: 0.9344 - val_loss: 1.2603 - val_acc: 0.5530 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.3208 - acc: 0.5356 - top5-acc: 0.9355 - val_loss: 1.2350 - val_acc: 0.5662 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 19s 213ms/step - loss: 1.3144 - acc: 0.5385 - top5-acc: 0.9363 - val_loss: 1.2442 - val_acc: 0.5644 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.3134 - acc: 0.5364 - top5-acc: 0.9370 - val_loss: 1.2222 - val_acc: 0.5706 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 18s 210ms/step - loss: 1.3065 - acc: 0.5400 - top5-acc: 0.9388 - val_loss: 1.2151 - val_acc: 0.5620 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2927 - acc: 0.5486 - top5-acc: 0.9391 - val_loss: 1.2371 - val_acc: 0.5644 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2987 - acc: 0.5432 - top5-acc: 0.9396 - val_loss: 1.2182 - val_acc: 0.5674 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.2831 - acc: 0.5470 - top5-acc: 0.9400 - val_loss: 1.2070 - val_acc: 0.5762 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 19s 213ms/step - loss: 1.2858 - acc: 0.5436 - top5-acc: 0.9427 - val_loss: 1.2145 - val_acc: 0.5706 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 18s 206ms/step - loss: 1.2733 - acc: 0.5512 - top5-acc: 0.9422 - val_loss: 1.2087 - val_acc: 0.5726 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 19s 214ms/step - loss: 1.2828 - acc: 0.5448 - top5-acc: 0.9407 - val_loss: 1.1987 - val_acc: 0.5798 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2853 - acc: 0.5453 - top5-acc: 0.9408 - val_loss: 1.1886 - val_acc: 0.5840 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2748 - acc: 0.5520 - top5-acc: 0.9412 - val_loss: 1.1924 - val_acc: 0.5824 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2807 - acc: 0.5502 - top5-acc: 0.9411 - val_loss: 1.1780 - val_acc: 0.5818 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2729 - acc: 0.5509 - top5-acc: 0.9430 - val_loss: 1.2036 - val_acc: 0.5740 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 18s 210ms/step - loss: 1.2770 - acc: 0.5485 - top5-acc: 0.9410 - val_loss: 1.1828 - val_acc: 0.5820 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 18s 210ms/step - loss: 1.2677 - acc: 0.5532 - top5-acc: 0.9429 - val_loss: 1.1944 - val_acc: 0.5718 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 18s 210ms/step - loss: 1.2644 - acc: 0.5544 - top5-acc: 0.9436 - val_loss: 1.1759 - val_acc: 0.5908 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2729 - acc: 0.5526 - top5-acc: 0.9418 - val_loss: 1.1773 - val_acc: 0.5850 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 19s 218ms/step - loss: 1.2680 - acc: 0.5531 - top5-acc: 0.9426 - val_loss: 1.1858 - val_acc: 0.5886 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.2624 - acc: 0.5515 - top5-acc: 0.9444 - val_loss: 1.1791 - val_acc: 0.5830 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2730 - acc: 0.5528 - top5-acc: 0.9412 - val_loss: 1.1678 - val_acc: 0.5858 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2701 - acc: 0.5508 - top5-acc: 0.9427 - val_loss: 1.1624 - val_acc: 0.5942 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.2669 - acc: 0.5526 - top5-acc: 0.9419 - val_loss: 1.1851 - val_acc: 0.5842 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 19s 222ms/step - loss: 1.2658 - acc: 0.5519 - top5-acc: 0.9428 - val_loss: 1.1772 - val_acc: 0.5830 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 223ms/step - loss: 1.2696 - acc: 0.5525 - top5-acc: 0.9431 - val_loss: 1.1634 - val_acc: 0.5912 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 19s 215ms/step - loss: 1.2689 - acc: 0.5481 - top5-acc: 0.9425 - val_loss: 1.1849 - val_acc: 0.5842 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 19s 214ms/step - loss: 1.2685 - acc: 0.5519 - top5-acc: 0.9426 - val_loss: 1.1740 - val_acc: 0.5872 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.2449 - acc: 0.5604 - top5-acc: 0.9450 - val_loss: 1.1622 - val_acc: 0.5870 - val_top5-acc: 0.9550 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2491 - acc: 0.5608 - top5-acc: 0.9436 - val_loss: 1.1580 - val_acc: 0.5948 - val_top5-acc: 0.9548 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2439 - acc: 0.5608 - top5-acc: 0.9459 - val_loss: 1.1610 - val_acc: 0.5894 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 18s 203ms/step - loss: 1.2527 - acc: 0.5574 - top5-acc: 0.9441 - val_loss: 1.1745 - val_acc: 0.5782 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2479 - acc: 0.5605 - top5-acc: 0.9438 - val_loss: 1.1611 - val_acc: 0.5914 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2510 - acc: 0.5583 - top5-acc: 0.9446 - val_loss: 1.1678 - val_acc: 0.5852 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2581 - acc: 0.5557 - top5-acc: 0.9436 - val_loss: 1.1562 - val_acc: 0.5976 - val_top5-acc: 0.9570 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2468 - acc: 0.5596 - top5-acc: 0.9440 - val_loss: 1.1690 - val_acc: 0.5872 - val_top5-acc: 0.9532 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2444 - acc: 0.5598 - top5-acc: 0.9448 - val_loss: 1.1539 - val_acc: 0.5964 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2410 - acc: 0.5617 - top5-acc: 0.9468 - val_loss: 1.1613 - val_acc: 0.5870 - val_top5-acc: 0.9556 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2488 - acc: 0.5566 - top5-acc: 0.9449 - val_loss: 1.1615 - val_acc: 0.5924 - val_top5-acc: 0.9532 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.2484 - acc: 0.5603 - top5-acc: 0.9441 - val_loss: 1.1504 - val_acc: 0.5928 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 18s 206ms/step - loss: 1.2477 - acc: 0.5598 - top5-acc: 0.9456 - val_loss: 1.1421 - val_acc: 0.5962 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2459 - acc: 0.5605 - top5-acc: 0.9455 - val_loss: 1.1507 - val_acc: 0.5904 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2408 - acc: 0.5639 - top5-acc: 0.9460 - val_loss: 1.1626 - val_acc: 0.5906 - val_top5-acc: 0.9540 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 18s 205ms/step - loss: 1.2494 - acc: 0.5602 - top5-acc: 0.9451 - val_loss: 1.1624 - val_acc: 0.5866 - val_top5-acc: 0.9540 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 18s 206ms/step - loss: 1.2503 - acc: 0.5584 - top5-acc: 0.9438 - val_loss: 1.1647 - val_acc: 0.5894 - val_top5-acc: 0.9532 - lr: 0.0025\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 1.1765 - acc: 0.5836 - top5-acc: 0.9519\n",
      "Test accuracy: 58.36%\n",
      "Test top 5 accuracy: 95.19%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 25s 231ms/step - loss: 1.7464 - acc: 0.3841 - top5-acc: 0.8548 - val_loss: 1.4401 - val_acc: 0.5012 - val_top5-acc: 0.9236 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.4711 - acc: 0.4865 - top5-acc: 0.9168 - val_loss: 1.3482 - val_acc: 0.5272 - val_top5-acc: 0.9344 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.4012 - acc: 0.5084 - top5-acc: 0.9258 - val_loss: 1.3167 - val_acc: 0.5378 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.3661 - acc: 0.5206 - top5-acc: 0.9314 - val_loss: 1.2715 - val_acc: 0.5516 - val_top5-acc: 0.9404 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.3407 - acc: 0.5283 - top5-acc: 0.9332 - val_loss: 1.2588 - val_acc: 0.5548 - val_top5-acc: 0.9422 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3264 - acc: 0.5352 - top5-acc: 0.9355 - val_loss: 1.2557 - val_acc: 0.5576 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.3115 - acc: 0.5383 - top5-acc: 0.9373 - val_loss: 1.2392 - val_acc: 0.5612 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.3075 - acc: 0.5396 - top5-acc: 0.9361 - val_loss: 1.2131 - val_acc: 0.5714 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.2956 - acc: 0.5437 - top5-acc: 0.9389 - val_loss: 1.2179 - val_acc: 0.5752 - val_top5-acc: 0.9488 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 225ms/step - loss: 1.2953 - acc: 0.5453 - top5-acc: 0.9396 - val_loss: 1.2071 - val_acc: 0.5732 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.2924 - acc: 0.5453 - top5-acc: 0.9398 - val_loss: 1.2099 - val_acc: 0.5752 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 223ms/step - loss: 1.2896 - acc: 0.5443 - top5-acc: 0.9393 - val_loss: 1.2117 - val_acc: 0.5760 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2806 - acc: 0.5490 - top5-acc: 0.9413 - val_loss: 1.2287 - val_acc: 0.5600 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2813 - acc: 0.5455 - top5-acc: 0.9408 - val_loss: 1.2006 - val_acc: 0.5766 - val_top5-acc: 0.9458 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2779 - acc: 0.5511 - top5-acc: 0.9408 - val_loss: 1.1958 - val_acc: 0.5796 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2678 - acc: 0.5522 - top5-acc: 0.9425 - val_loss: 1.1827 - val_acc: 0.5808 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 19s 220ms/step - loss: 1.2733 - acc: 0.5527 - top5-acc: 0.9424 - val_loss: 1.1943 - val_acc: 0.5762 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.2715 - acc: 0.5506 - top5-acc: 0.9412 - val_loss: 1.1715 - val_acc: 0.5874 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2614 - acc: 0.5558 - top5-acc: 0.9429 - val_loss: 1.1851 - val_acc: 0.5808 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.2681 - acc: 0.5512 - top5-acc: 0.9422 - val_loss: 1.1719 - val_acc: 0.5884 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.2635 - acc: 0.5551 - top5-acc: 0.9441 - val_loss: 1.1788 - val_acc: 0.5854 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2647 - acc: 0.5522 - top5-acc: 0.9438 - val_loss: 1.1609 - val_acc: 0.5866 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.2686 - acc: 0.5512 - top5-acc: 0.9422 - val_loss: 1.1622 - val_acc: 0.5852 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2668 - acc: 0.5520 - top5-acc: 0.9436 - val_loss: 1.1655 - val_acc: 0.5892 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2610 - acc: 0.5532 - top5-acc: 0.9437 - val_loss: 1.1621 - val_acc: 0.5858 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2674 - acc: 0.5528 - top5-acc: 0.9432 - val_loss: 1.1650 - val_acc: 0.5904 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 19s 218ms/step - loss: 1.2626 - acc: 0.5558 - top5-acc: 0.9426 - val_loss: 1.1977 - val_acc: 0.5804 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 19s 220ms/step - loss: 1.2492 - acc: 0.5575 - top5-acc: 0.9434 - val_loss: 1.1439 - val_acc: 0.5988 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 19s 216ms/step - loss: 1.2459 - acc: 0.5609 - top5-acc: 0.9440 - val_loss: 1.1485 - val_acc: 0.5978 - val_top5-acc: 0.9530 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 19s 216ms/step - loss: 1.2390 - acc: 0.5635 - top5-acc: 0.9454 - val_loss: 1.1599 - val_acc: 0.5896 - val_top5-acc: 0.9538 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 19s 222ms/step - loss: 1.2465 - acc: 0.5597 - top5-acc: 0.9439 - val_loss: 1.1682 - val_acc: 0.5910 - val_top5-acc: 0.9518 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 19s 222ms/step - loss: 1.2460 - acc: 0.5583 - top5-acc: 0.9439 - val_loss: 1.1413 - val_acc: 0.5940 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 19s 221ms/step - loss: 1.2437 - acc: 0.5644 - top5-acc: 0.9448 - val_loss: 1.1567 - val_acc: 0.5878 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2465 - acc: 0.5595 - top5-acc: 0.9441 - val_loss: 1.1661 - val_acc: 0.5916 - val_top5-acc: 0.9542 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 19s 213ms/step - loss: 1.2404 - acc: 0.5632 - top5-acc: 0.9444 - val_loss: 1.1478 - val_acc: 0.5952 - val_top5-acc: 0.9534 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 19s 216ms/step - loss: 1.2456 - acc: 0.5598 - top5-acc: 0.9450 - val_loss: 1.1439 - val_acc: 0.5954 - val_top5-acc: 0.9542 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 19s 216ms/step - loss: 1.2471 - acc: 0.5604 - top5-acc: 0.9444 - val_loss: 1.1718 - val_acc: 0.5862 - val_top5-acc: 0.9494 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.2332 - acc: 0.5655 - top5-acc: 0.9456 - val_loss: 1.1411 - val_acc: 0.5990 - val_top5-acc: 0.9564 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 19s 216ms/step - loss: 1.2362 - acc: 0.5671 - top5-acc: 0.9452 - val_loss: 1.1433 - val_acc: 0.5974 - val_top5-acc: 0.9560 - lr: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "88/88 [==============================] - 19s 215ms/step - loss: 1.2371 - acc: 0.5613 - top5-acc: 0.9469 - val_loss: 1.1477 - val_acc: 0.5958 - val_top5-acc: 0.9542 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 19s 214ms/step - loss: 1.2368 - acc: 0.5652 - top5-acc: 0.9452 - val_loss: 1.1405 - val_acc: 0.5956 - val_top5-acc: 0.9564 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 19s 218ms/step - loss: 1.2366 - acc: 0.5652 - top5-acc: 0.9458 - val_loss: 1.1481 - val_acc: 0.5974 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2379 - acc: 0.5632 - top5-acc: 0.9444 - val_loss: 1.1423 - val_acc: 0.5940 - val_top5-acc: 0.9560 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 19s 215ms/step - loss: 1.2382 - acc: 0.5653 - top5-acc: 0.9435 - val_loss: 1.1488 - val_acc: 0.5990 - val_top5-acc: 0.9546 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.2377 - acc: 0.5629 - top5-acc: 0.9450 - val_loss: 1.1492 - val_acc: 0.5960 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 19s 214ms/step - loss: 1.2396 - acc: 0.5622 - top5-acc: 0.9457 - val_loss: 1.1573 - val_acc: 0.5914 - val_top5-acc: 0.9526 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2343 - acc: 0.5671 - top5-acc: 0.9456 - val_loss: 1.1451 - val_acc: 0.5988 - val_top5-acc: 0.9548 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2343 - acc: 0.5667 - top5-acc: 0.9465 - val_loss: 1.1434 - val_acc: 0.5964 - val_top5-acc: 0.9552 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2321 - acc: 0.5654 - top5-acc: 0.9469 - val_loss: 1.1427 - val_acc: 0.5998 - val_top5-acc: 0.9546 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 222ms/step - loss: 1.2376 - acc: 0.5621 - top5-acc: 0.9457 - val_loss: 1.1423 - val_acc: 0.6020 - val_top5-acc: 0.9560 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 29s 93ms/step - loss: 1.1597 - acc: 0.5942 - top5-acc: 0.9555\n",
      "Test accuracy: 59.42%\n",
      "Test top 5 accuracy: 95.55%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 26s 246ms/step - loss: 1.7237 - acc: 0.3911 - top5-acc: 0.8618 - val_loss: 1.4526 - val_acc: 0.4896 - val_top5-acc: 0.9248 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.4714 - acc: 0.4845 - top5-acc: 0.9153 - val_loss: 1.3560 - val_acc: 0.5272 - val_top5-acc: 0.9368 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 225ms/step - loss: 1.4035 - acc: 0.5070 - top5-acc: 0.9257 - val_loss: 1.3079 - val_acc: 0.5420 - val_top5-acc: 0.9412 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.3702 - acc: 0.5182 - top5-acc: 0.9304 - val_loss: 1.2871 - val_acc: 0.5440 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.3458 - acc: 0.5272 - top5-acc: 0.9330 - val_loss: 1.2553 - val_acc: 0.5536 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3307 - acc: 0.5310 - top5-acc: 0.9340 - val_loss: 1.2345 - val_acc: 0.5690 - val_top5-acc: 0.9480 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3215 - acc: 0.5342 - top5-acc: 0.9360 - val_loss: 1.2435 - val_acc: 0.5578 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.3166 - acc: 0.5366 - top5-acc: 0.9354 - val_loss: 1.2314 - val_acc: 0.5620 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.3013 - acc: 0.5423 - top5-acc: 0.9388 - val_loss: 1.2310 - val_acc: 0.5584 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.3001 - acc: 0.5419 - top5-acc: 0.9392 - val_loss: 1.2207 - val_acc: 0.5728 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2948 - acc: 0.5454 - top5-acc: 0.9395 - val_loss: 1.2121 - val_acc: 0.5706 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2917 - acc: 0.5463 - top5-acc: 0.9398 - val_loss: 1.1976 - val_acc: 0.5766 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.2898 - acc: 0.5459 - top5-acc: 0.9396 - val_loss: 1.1888 - val_acc: 0.5780 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2877 - acc: 0.5452 - top5-acc: 0.9400 - val_loss: 1.1808 - val_acc: 0.5806 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2871 - acc: 0.5441 - top5-acc: 0.9419 - val_loss: 1.1870 - val_acc: 0.5714 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 19s 221ms/step - loss: 1.2828 - acc: 0.5482 - top5-acc: 0.9406 - val_loss: 1.2227 - val_acc: 0.5628 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 19s 221ms/step - loss: 1.2741 - acc: 0.5505 - top5-acc: 0.9414 - val_loss: 1.1872 - val_acc: 0.5806 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.2820 - acc: 0.5457 - top5-acc: 0.9411 - val_loss: 1.1891 - val_acc: 0.5826 - val_top5-acc: 0.9472 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 19s 220ms/step - loss: 1.2768 - acc: 0.5494 - top5-acc: 0.9414 - val_loss: 1.1982 - val_acc: 0.5770 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 19s 220ms/step - loss: 1.2642 - acc: 0.5554 - top5-acc: 0.9421 - val_loss: 1.1630 - val_acc: 0.5862 - val_top5-acc: 0.9532 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2603 - acc: 0.5563 - top5-acc: 0.9416 - val_loss: 1.1650 - val_acc: 0.5874 - val_top5-acc: 0.9512 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 19s 220ms/step - loss: 1.2617 - acc: 0.5526 - top5-acc: 0.9423 - val_loss: 1.1593 - val_acc: 0.5888 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2597 - acc: 0.5550 - top5-acc: 0.9436 - val_loss: 1.1687 - val_acc: 0.5828 - val_top5-acc: 0.9514 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2571 - acc: 0.5580 - top5-acc: 0.9422 - val_loss: 1.1641 - val_acc: 0.5802 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2573 - acc: 0.5560 - top5-acc: 0.9446 - val_loss: 1.1817 - val_acc: 0.5854 - val_top5-acc: 0.9508 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2606 - acc: 0.5537 - top5-acc: 0.9431 - val_loss: 1.1640 - val_acc: 0.5882 - val_top5-acc: 0.9542 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2584 - acc: 0.5573 - top5-acc: 0.9445 - val_loss: 1.1711 - val_acc: 0.5842 - val_top5-acc: 0.9504 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2501 - acc: 0.5604 - top5-acc: 0.9430 - val_loss: 1.1545 - val_acc: 0.5914 - val_top5-acc: 0.9530 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2487 - acc: 0.5610 - top5-acc: 0.9440 - val_loss: 1.1607 - val_acc: 0.5858 - val_top5-acc: 0.9530 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2501 - acc: 0.5610 - top5-acc: 0.9452 - val_loss: 1.1595 - val_acc: 0.5900 - val_top5-acc: 0.9518 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2507 - acc: 0.5616 - top5-acc: 0.9430 - val_loss: 1.1575 - val_acc: 0.5858 - val_top5-acc: 0.9544 - lr: 0.0012\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 225ms/step - loss: 1.2542 - acc: 0.5577 - top5-acc: 0.9429 - val_loss: 1.1612 - val_acc: 0.5868 - val_top5-acc: 0.9546 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2506 - acc: 0.5607 - top5-acc: 0.9423 - val_loss: 1.1559 - val_acc: 0.5902 - val_top5-acc: 0.9512 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 223ms/step - loss: 1.2506 - acc: 0.5626 - top5-acc: 0.9425 - val_loss: 1.1578 - val_acc: 0.5860 - val_top5-acc: 0.9544 - lr: 6.2500e-04\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 225ms/step - loss: 1.2486 - acc: 0.5621 - top5-acc: 0.9437 - val_loss: 1.1555 - val_acc: 0.5894 - val_top5-acc: 0.9540 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2514 - acc: 0.5593 - top5-acc: 0.9446 - val_loss: 1.1562 - val_acc: 0.5864 - val_top5-acc: 0.9526 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2515 - acc: 0.5622 - top5-acc: 0.9452 - val_loss: 1.1595 - val_acc: 0.5920 - val_top5-acc: 0.9538 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2494 - acc: 0.5593 - top5-acc: 0.9443 - val_loss: 1.1570 - val_acc: 0.5886 - val_top5-acc: 0.9524 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2450 - acc: 0.5642 - top5-acc: 0.9452 - val_loss: 1.1568 - val_acc: 0.5922 - val_top5-acc: 0.9516 - lr: 3.1250e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2448 - acc: 0.5624 - top5-acc: 0.9450 - val_loss: 1.1592 - val_acc: 0.5884 - val_top5-acc: 0.9544 - lr: 3.1250e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2460 - acc: 0.5630 - top5-acc: 0.9452 - val_loss: 1.1595 - val_acc: 0.5896 - val_top5-acc: 0.9526 - lr: 3.1250e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2422 - acc: 0.5673 - top5-acc: 0.9456 - val_loss: 1.1565 - val_acc: 0.5940 - val_top5-acc: 0.9526 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2453 - acc: 0.5638 - top5-acc: 0.9455 - val_loss: 1.1578 - val_acc: 0.5924 - val_top5-acc: 0.9534 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2458 - acc: 0.5627 - top5-acc: 0.9440 - val_loss: 1.1609 - val_acc: 0.5904 - val_top5-acc: 0.9540 - lr: 1.5625e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2485 - acc: 0.5614 - top5-acc: 0.9455 - val_loss: 1.1597 - val_acc: 0.5898 - val_top5-acc: 0.9552 - lr: 1.5625e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2496 - acc: 0.5611 - top5-acc: 0.9446 - val_loss: 1.1610 - val_acc: 0.5890 - val_top5-acc: 0.9542 - lr: 1.5625e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2506 - acc: 0.5663 - top5-acc: 0.9451 - val_loss: 1.1616 - val_acc: 0.5902 - val_top5-acc: 0.9532 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2498 - acc: 0.5650 - top5-acc: 0.9448 - val_loss: 1.1655 - val_acc: 0.5890 - val_top5-acc: 0.9524 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2504 - acc: 0.5643 - top5-acc: 0.9450 - val_loss: 1.1643 - val_acc: 0.5896 - val_top5-acc: 0.9542 - lr: 7.8125e-05\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2558 - acc: 0.5618 - top5-acc: 0.9436 - val_loss: 1.1654 - val_acc: 0.5896 - val_top5-acc: 0.9530 - lr: 7.8125e-05\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.1865 - acc: 0.5864 - top5-acc: 0.9529\n",
      "Test accuracy: 58.64%\n",
      "Test top 5 accuracy: 95.29%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 36s 262ms/step - loss: 1.7863 - acc: 0.3730 - top5-acc: 0.8461 - val_loss: 1.4810 - val_acc: 0.4792 - val_top5-acc: 0.9230 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.4897 - acc: 0.4798 - top5-acc: 0.9116 - val_loss: 1.3793 - val_acc: 0.5148 - val_top5-acc: 0.9296 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.4184 - acc: 0.5011 - top5-acc: 0.9239 - val_loss: 1.3220 - val_acc: 0.5282 - val_top5-acc: 0.9392 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.3780 - acc: 0.5150 - top5-acc: 0.9296 - val_loss: 1.2832 - val_acc: 0.5454 - val_top5-acc: 0.9446 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.3565 - acc: 0.5238 - top5-acc: 0.9301 - val_loss: 1.2645 - val_acc: 0.5436 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.3442 - acc: 0.5252 - top5-acc: 0.9328 - val_loss: 1.2576 - val_acc: 0.5572 - val_top5-acc: 0.9450 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.3312 - acc: 0.5316 - top5-acc: 0.9343 - val_loss: 1.2349 - val_acc: 0.5662 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 22s 245ms/step - loss: 1.3235 - acc: 0.5359 - top5-acc: 0.9353 - val_loss: 1.2678 - val_acc: 0.5480 - val_top5-acc: 0.9414 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.3097 - acc: 0.5372 - top5-acc: 0.9376 - val_loss: 1.2163 - val_acc: 0.5674 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.3033 - acc: 0.5421 - top5-acc: 0.9385 - val_loss: 1.2088 - val_acc: 0.5726 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.3040 - acc: 0.5387 - top5-acc: 0.9385 - val_loss: 1.2121 - val_acc: 0.5592 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2932 - acc: 0.5430 - top5-acc: 0.9392 - val_loss: 1.1969 - val_acc: 0.5816 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2890 - acc: 0.5480 - top5-acc: 0.9388 - val_loss: 1.2179 - val_acc: 0.5638 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2917 - acc: 0.5451 - top5-acc: 0.9400 - val_loss: 1.2089 - val_acc: 0.5688 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2861 - acc: 0.5469 - top5-acc: 0.9394 - val_loss: 1.1855 - val_acc: 0.5826 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2840 - acc: 0.5451 - top5-acc: 0.9400 - val_loss: 1.2010 - val_acc: 0.5744 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2765 - acc: 0.5490 - top5-acc: 0.9392 - val_loss: 1.2032 - val_acc: 0.5676 - val_top5-acc: 0.9484 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2829 - acc: 0.5492 - top5-acc: 0.9410 - val_loss: 1.1901 - val_acc: 0.5748 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2751 - acc: 0.5508 - top5-acc: 0.9413 - val_loss: 1.1753 - val_acc: 0.5810 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2785 - acc: 0.5487 - top5-acc: 0.9398 - val_loss: 1.1741 - val_acc: 0.5898 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2818 - acc: 0.5480 - top5-acc: 0.9396 - val_loss: 1.1677 - val_acc: 0.5826 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2693 - acc: 0.5524 - top5-acc: 0.9421 - val_loss: 1.1948 - val_acc: 0.5832 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2720 - acc: 0.5512 - top5-acc: 0.9421 - val_loss: 1.1883 - val_acc: 0.5850 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2733 - acc: 0.5514 - top5-acc: 0.9417 - val_loss: 1.1850 - val_acc: 0.5810 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2781 - acc: 0.5484 - top5-acc: 0.9428 - val_loss: 1.1593 - val_acc: 0.5904 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2745 - acc: 0.5490 - top5-acc: 0.9422 - val_loss: 1.1688 - val_acc: 0.5778 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2663 - acc: 0.5528 - top5-acc: 0.9422 - val_loss: 1.1681 - val_acc: 0.5830 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2657 - acc: 0.5531 - top5-acc: 0.9425 - val_loss: 1.1769 - val_acc: 0.5804 - val_top5-acc: 0.9578 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2661 - acc: 0.5551 - top5-acc: 0.9416 - val_loss: 1.1694 - val_acc: 0.5862 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2672 - acc: 0.5536 - top5-acc: 0.9416 - val_loss: 1.1989 - val_acc: 0.5766 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2565 - acc: 0.5589 - top5-acc: 0.9439 - val_loss: 1.1577 - val_acc: 0.5956 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2532 - acc: 0.5591 - top5-acc: 0.9440 - val_loss: 1.1538 - val_acc: 0.5906 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2568 - acc: 0.5558 - top5-acc: 0.9435 - val_loss: 1.1518 - val_acc: 0.5930 - val_top5-acc: 0.9548 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2514 - acc: 0.5594 - top5-acc: 0.9433 - val_loss: 1.1508 - val_acc: 0.5922 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2557 - acc: 0.5576 - top5-acc: 0.9436 - val_loss: 1.1574 - val_acc: 0.5948 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2487 - acc: 0.5584 - top5-acc: 0.9452 - val_loss: 1.1552 - val_acc: 0.5924 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2543 - acc: 0.5566 - top5-acc: 0.9443 - val_loss: 1.1480 - val_acc: 0.5906 - val_top5-acc: 0.9584 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2525 - acc: 0.5580 - top5-acc: 0.9435 - val_loss: 1.1502 - val_acc: 0.5962 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2528 - acc: 0.5598 - top5-acc: 0.9436 - val_loss: 1.1526 - val_acc: 0.5950 - val_top5-acc: 0.9580 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2470 - acc: 0.5609 - top5-acc: 0.9444 - val_loss: 1.1541 - val_acc: 0.5884 - val_top5-acc: 0.9534 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2567 - acc: 0.5566 - top5-acc: 0.9436 - val_loss: 1.1506 - val_acc: 0.5926 - val_top5-acc: 0.9568 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2549 - acc: 0.5591 - top5-acc: 0.9440 - val_loss: 1.1574 - val_acc: 0.5876 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2450 - acc: 0.5627 - top5-acc: 0.9446 - val_loss: 1.1448 - val_acc: 0.5990 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2433 - acc: 0.5622 - top5-acc: 0.9444 - val_loss: 1.1492 - val_acc: 0.5958 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2448 - acc: 0.5617 - top5-acc: 0.9440 - val_loss: 1.1471 - val_acc: 0.5928 - val_top5-acc: 0.9578 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2456 - acc: 0.5614 - top5-acc: 0.9445 - val_loss: 1.1525 - val_acc: 0.5934 - val_top5-acc: 0.9568 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2430 - acc: 0.5624 - top5-acc: 0.9458 - val_loss: 1.1521 - val_acc: 0.5962 - val_top5-acc: 0.9590 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2403 - acc: 0.5636 - top5-acc: 0.9451 - val_loss: 1.1492 - val_acc: 0.5940 - val_top5-acc: 0.9564 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2418 - acc: 0.5641 - top5-acc: 0.9443 - val_loss: 1.1413 - val_acc: 0.5968 - val_top5-acc: 0.9586 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2434 - acc: 0.5635 - top5-acc: 0.9442 - val_loss: 1.1430 - val_acc: 0.5956 - val_top5-acc: 0.9586 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 33s 104ms/step - loss: 1.1750 - acc: 0.5880 - top5-acc: 0.9565\n",
      "Test accuracy: 58.8%\n",
      "Test top 5 accuracy: 95.65%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 27s 255ms/step - loss: 1.7889 - acc: 0.3759 - top5-acc: 0.8429 - val_loss: 1.4851 - val_acc: 0.4764 - val_top5-acc: 0.9176 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.4905 - acc: 0.4755 - top5-acc: 0.9118 - val_loss: 1.3724 - val_acc: 0.5178 - val_top5-acc: 0.9296 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.4156 - acc: 0.5023 - top5-acc: 0.9219 - val_loss: 1.3174 - val_acc: 0.5250 - val_top5-acc: 0.9394 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.3744 - acc: 0.5164 - top5-acc: 0.9286 - val_loss: 1.3001 - val_acc: 0.5408 - val_top5-acc: 0.9436 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.3522 - acc: 0.5241 - top5-acc: 0.9318 - val_loss: 1.2764 - val_acc: 0.5454 - val_top5-acc: 0.9394 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.3397 - acc: 0.5278 - top5-acc: 0.9309 - val_loss: 1.2442 - val_acc: 0.5596 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.3295 - acc: 0.5315 - top5-acc: 0.9336 - val_loss: 1.2369 - val_acc: 0.5564 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.3148 - acc: 0.5367 - top5-acc: 0.9364 - val_loss: 1.2303 - val_acc: 0.5624 - val_top5-acc: 0.9458 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.3090 - acc: 0.5376 - top5-acc: 0.9372 - val_loss: 1.2114 - val_acc: 0.5738 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.3003 - acc: 0.5431 - top5-acc: 0.9384 - val_loss: 1.1944 - val_acc: 0.5782 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2955 - acc: 0.5452 - top5-acc: 0.9371 - val_loss: 1.2223 - val_acc: 0.5640 - val_top5-acc: 0.9472 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2907 - acc: 0.5463 - top5-acc: 0.9394 - val_loss: 1.1925 - val_acc: 0.5834 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2900 - acc: 0.5461 - top5-acc: 0.9406 - val_loss: 1.1773 - val_acc: 0.5850 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2782 - acc: 0.5494 - top5-acc: 0.9412 - val_loss: 1.1796 - val_acc: 0.5824 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2796 - acc: 0.5484 - top5-acc: 0.9395 - val_loss: 1.1927 - val_acc: 0.5738 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2743 - acc: 0.5495 - top5-acc: 0.9414 - val_loss: 1.1922 - val_acc: 0.5746 - val_top5-acc: 0.9498 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2727 - acc: 0.5513 - top5-acc: 0.9416 - val_loss: 1.2059 - val_acc: 0.5676 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.2844 - acc: 0.5450 - top5-acc: 0.9399 - val_loss: 1.1657 - val_acc: 0.5900 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2737 - acc: 0.5494 - top5-acc: 0.9417 - val_loss: 1.1651 - val_acc: 0.5846 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2697 - acc: 0.5530 - top5-acc: 0.9423 - val_loss: 1.1701 - val_acc: 0.5830 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2744 - acc: 0.5485 - top5-acc: 0.9426 - val_loss: 1.1785 - val_acc: 0.5818 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2692 - acc: 0.5513 - top5-acc: 0.9413 - val_loss: 1.1537 - val_acc: 0.5918 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2652 - acc: 0.5526 - top5-acc: 0.9414 - val_loss: 1.1722 - val_acc: 0.5876 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2674 - acc: 0.5510 - top5-acc: 0.9434 - val_loss: 1.1569 - val_acc: 0.5832 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2636 - acc: 0.5548 - top5-acc: 0.9442 - val_loss: 1.1539 - val_acc: 0.5916 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2651 - acc: 0.5543 - top5-acc: 0.9427 - val_loss: 1.1672 - val_acc: 0.5818 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2687 - acc: 0.5517 - top5-acc: 0.9424 - val_loss: 1.1653 - val_acc: 0.5824 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2484 - acc: 0.5571 - top5-acc: 0.9445 - val_loss: 1.1454 - val_acc: 0.6002 - val_top5-acc: 0.9536 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.2459 - acc: 0.5617 - top5-acc: 0.9446 - val_loss: 1.1393 - val_acc: 0.5988 - val_top5-acc: 0.9570 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2477 - acc: 0.5608 - top5-acc: 0.9448 - val_loss: 1.1443 - val_acc: 0.5974 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2496 - acc: 0.5589 - top5-acc: 0.9423 - val_loss: 1.1405 - val_acc: 0.5922 - val_top5-acc: 0.9546 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2431 - acc: 0.5612 - top5-acc: 0.9460 - val_loss: 1.1418 - val_acc: 0.6024 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2457 - acc: 0.5613 - top5-acc: 0.9445 - val_loss: 1.1512 - val_acc: 0.5964 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2485 - acc: 0.5607 - top5-acc: 0.9443 - val_loss: 1.1371 - val_acc: 0.5966 - val_top5-acc: 0.9546 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2393 - acc: 0.5642 - top5-acc: 0.9455 - val_loss: 1.1445 - val_acc: 0.5906 - val_top5-acc: 0.9546 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2459 - acc: 0.5604 - top5-acc: 0.9441 - val_loss: 1.1436 - val_acc: 0.5964 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2450 - acc: 0.5606 - top5-acc: 0.9445 - val_loss: 1.1488 - val_acc: 0.5996 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2470 - acc: 0.5612 - top5-acc: 0.9452 - val_loss: 1.1683 - val_acc: 0.5870 - val_top5-acc: 0.9538 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2503 - acc: 0.5589 - top5-acc: 0.9446 - val_loss: 1.1405 - val_acc: 0.6018 - val_top5-acc: 0.9556 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2391 - acc: 0.5635 - top5-acc: 0.9458 - val_loss: 1.1350 - val_acc: 0.6014 - val_top5-acc: 0.9564 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2411 - acc: 0.5648 - top5-acc: 0.9464 - val_loss: 1.1376 - val_acc: 0.5994 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2419 - acc: 0.5637 - top5-acc: 0.9455 - val_loss: 1.1416 - val_acc: 0.5984 - val_top5-acc: 0.9582 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2406 - acc: 0.5632 - top5-acc: 0.9461 - val_loss: 1.1456 - val_acc: 0.5980 - val_top5-acc: 0.9546 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2391 - acc: 0.5637 - top5-acc: 0.9451 - val_loss: 1.1391 - val_acc: 0.5972 - val_top5-acc: 0.9564 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2404 - acc: 0.5648 - top5-acc: 0.9454 - val_loss: 1.1418 - val_acc: 0.5998 - val_top5-acc: 0.9568 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2365 - acc: 0.5643 - top5-acc: 0.9455 - val_loss: 1.1375 - val_acc: 0.5990 - val_top5-acc: 0.9568 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2348 - acc: 0.5682 - top5-acc: 0.9454 - val_loss: 1.1372 - val_acc: 0.6012 - val_top5-acc: 0.9566 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2355 - acc: 0.5656 - top5-acc: 0.9460 - val_loss: 1.1412 - val_acc: 0.6014 - val_top5-acc: 0.9556 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2402 - acc: 0.5622 - top5-acc: 0.9441 - val_loss: 1.1428 - val_acc: 0.6040 - val_top5-acc: 0.9560 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2351 - acc: 0.5653 - top5-acc: 0.9456 - val_loss: 1.1395 - val_acc: 0.6018 - val_top5-acc: 0.9588 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 34s 109ms/step - loss: 1.1676 - acc: 0.5884 - top5-acc: 0.9551\n",
      "Test accuracy: 58.84%\n",
      "Test top 5 accuracy: 95.51%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 28s 264ms/step - loss: 1.7841 - acc: 0.3666 - top5-acc: 0.8414 - val_loss: 1.5163 - val_acc: 0.4684 - val_top5-acc: 0.9158 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.5109 - acc: 0.4704 - top5-acc: 0.9095 - val_loss: 1.3891 - val_acc: 0.5142 - val_top5-acc: 0.9294 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.4296 - acc: 0.4969 - top5-acc: 0.9210 - val_loss: 1.3630 - val_acc: 0.5178 - val_top5-acc: 0.9328 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3942 - acc: 0.5119 - top5-acc: 0.9256 - val_loss: 1.3139 - val_acc: 0.5416 - val_top5-acc: 0.9318 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3673 - acc: 0.5202 - top5-acc: 0.9287 - val_loss: 1.2768 - val_acc: 0.5524 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3550 - acc: 0.5246 - top5-acc: 0.9315 - val_loss: 1.2497 - val_acc: 0.5664 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3413 - acc: 0.5290 - top5-acc: 0.9322 - val_loss: 1.2501 - val_acc: 0.5558 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3274 - acc: 0.5350 - top5-acc: 0.9324 - val_loss: 1.2627 - val_acc: 0.5616 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3151 - acc: 0.5389 - top5-acc: 0.9352 - val_loss: 1.2317 - val_acc: 0.5698 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3221 - acc: 0.5335 - top5-acc: 0.9357 - val_loss: 1.2512 - val_acc: 0.5642 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.3114 - acc: 0.5380 - top5-acc: 0.9385 - val_loss: 1.2337 - val_acc: 0.5624 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.3112 - acc: 0.5397 - top5-acc: 0.9368 - val_loss: 1.2077 - val_acc: 0.5826 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2976 - acc: 0.5424 - top5-acc: 0.9384 - val_loss: 1.2132 - val_acc: 0.5764 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3051 - acc: 0.5420 - top5-acc: 0.9364 - val_loss: 1.2615 - val_acc: 0.5484 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2968 - acc: 0.5444 - top5-acc: 0.9383 - val_loss: 1.1912 - val_acc: 0.5800 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2931 - acc: 0.5456 - top5-acc: 0.9384 - val_loss: 1.2157 - val_acc: 0.5714 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2959 - acc: 0.5422 - top5-acc: 0.9385 - val_loss: 1.1879 - val_acc: 0.5810 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2909 - acc: 0.5439 - top5-acc: 0.9384 - val_loss: 1.2092 - val_acc: 0.5812 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2951 - acc: 0.5448 - top5-acc: 0.9390 - val_loss: 1.2195 - val_acc: 0.5694 - val_top5-acc: 0.9446 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2949 - acc: 0.5409 - top5-acc: 0.9379 - val_loss: 1.1959 - val_acc: 0.5822 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2851 - acc: 0.5466 - top5-acc: 0.9396 - val_loss: 1.2049 - val_acc: 0.5742 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2839 - acc: 0.5465 - top5-acc: 0.9403 - val_loss: 1.1781 - val_acc: 0.5822 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2875 - acc: 0.5491 - top5-acc: 0.9374 - val_loss: 1.1735 - val_acc: 0.5928 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2799 - acc: 0.5486 - top5-acc: 0.9404 - val_loss: 1.1969 - val_acc: 0.5810 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2832 - acc: 0.5483 - top5-acc: 0.9398 - val_loss: 1.1885 - val_acc: 0.5820 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2826 - acc: 0.5484 - top5-acc: 0.9388 - val_loss: 1.1889 - val_acc: 0.5874 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2830 - acc: 0.5501 - top5-acc: 0.9400 - val_loss: 1.1921 - val_acc: 0.5740 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2853 - acc: 0.5468 - top5-acc: 0.9414 - val_loss: 1.1678 - val_acc: 0.5936 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2847 - acc: 0.5471 - top5-acc: 0.9398 - val_loss: 1.1982 - val_acc: 0.5820 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2798 - acc: 0.5480 - top5-acc: 0.9408 - val_loss: 1.1662 - val_acc: 0.5870 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2836 - acc: 0.5488 - top5-acc: 0.9402 - val_loss: 1.1618 - val_acc: 0.5986 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 22s 245ms/step - loss: 1.2798 - acc: 0.5486 - top5-acc: 0.9403 - val_loss: 1.1530 - val_acc: 0.5970 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 22s 245ms/step - loss: 1.2785 - acc: 0.5488 - top5-acc: 0.9408 - val_loss: 1.1972 - val_acc: 0.5738 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2742 - acc: 0.5505 - top5-acc: 0.9405 - val_loss: 1.1830 - val_acc: 0.5914 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 22s 248ms/step - loss: 1.2766 - acc: 0.5503 - top5-acc: 0.9414 - val_loss: 1.1735 - val_acc: 0.5906 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 22s 246ms/step - loss: 1.2778 - acc: 0.5490 - top5-acc: 0.9400 - val_loss: 1.1661 - val_acc: 0.5950 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2653 - acc: 0.5563 - top5-acc: 0.9417 - val_loss: 1.1988 - val_acc: 0.5682 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2568 - acc: 0.5564 - top5-acc: 0.9435 - val_loss: 1.1693 - val_acc: 0.5904 - val_top5-acc: 0.9536 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2699 - acc: 0.5518 - top5-acc: 0.9407 - val_loss: 1.1571 - val_acc: 0.5976 - val_top5-acc: 0.9524 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2592 - acc: 0.5557 - top5-acc: 0.9447 - val_loss: 1.1537 - val_acc: 0.5948 - val_top5-acc: 0.9546 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2613 - acc: 0.5554 - top5-acc: 0.9435 - val_loss: 1.1563 - val_acc: 0.5976 - val_top5-acc: 0.9518 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2660 - acc: 0.5529 - top5-acc: 0.9421 - val_loss: 1.1591 - val_acc: 0.5950 - val_top5-acc: 0.9532 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2517 - acc: 0.5594 - top5-acc: 0.9432 - val_loss: 1.1421 - val_acc: 0.6032 - val_top5-acc: 0.9554 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2536 - acc: 0.5573 - top5-acc: 0.9427 - val_loss: 1.1489 - val_acc: 0.6026 - val_top5-acc: 0.9534 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2492 - acc: 0.5630 - top5-acc: 0.9429 - val_loss: 1.1437 - val_acc: 0.6022 - val_top5-acc: 0.9548 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2522 - acc: 0.5591 - top5-acc: 0.9437 - val_loss: 1.1476 - val_acc: 0.6008 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2528 - acc: 0.5584 - top5-acc: 0.9430 - val_loss: 1.1500 - val_acc: 0.6000 - val_top5-acc: 0.9536 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2545 - acc: 0.5589 - top5-acc: 0.9434 - val_loss: 1.1441 - val_acc: 0.6042 - val_top5-acc: 0.9544 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2470 - acc: 0.5621 - top5-acc: 0.9451 - val_loss: 1.1467 - val_acc: 0.6012 - val_top5-acc: 0.9566 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2464 - acc: 0.5619 - top5-acc: 0.9443 - val_loss: 1.1496 - val_acc: 0.6004 - val_top5-acc: 0.9558 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.1694 - acc: 0.5869 - top5-acc: 0.9507\n",
      "Test accuracy: 58.69%\n",
      "Test top 5 accuracy: 95.07%\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 29s 275ms/step - loss: 1.6950 - acc: 0.4007 - top5-acc: 0.8629 - val_loss: 1.4048 - val_acc: 0.5046 - val_top5-acc: 0.9262 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.4149 - acc: 0.5056 - top5-acc: 0.9238 - val_loss: 1.3035 - val_acc: 0.5470 - val_top5-acc: 0.9360 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.3510 - acc: 0.5249 - top5-acc: 0.9310 - val_loss: 1.2586 - val_acc: 0.5588 - val_top5-acc: 0.9404 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.3125 - acc: 0.5406 - top5-acc: 0.9359 - val_loss: 1.2210 - val_acc: 0.5830 - val_top5-acc: 0.9470 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2961 - acc: 0.5452 - top5-acc: 0.9376 - val_loss: 1.2014 - val_acc: 0.5878 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2745 - acc: 0.5535 - top5-acc: 0.9409 - val_loss: 1.1816 - val_acc: 0.5912 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2617 - acc: 0.5569 - top5-acc: 0.9431 - val_loss: 1.1814 - val_acc: 0.5894 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2560 - acc: 0.5575 - top5-acc: 0.9416 - val_loss: 1.1683 - val_acc: 0.5976 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2515 - acc: 0.5620 - top5-acc: 0.9438 - val_loss: 1.1740 - val_acc: 0.5966 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2552 - acc: 0.5601 - top5-acc: 0.9420 - val_loss: 1.1506 - val_acc: 0.5996 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2445 - acc: 0.5618 - top5-acc: 0.9419 - val_loss: 1.1544 - val_acc: 0.5932 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2362 - acc: 0.5657 - top5-acc: 0.9438 - val_loss: 1.1516 - val_acc: 0.5940 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2411 - acc: 0.5643 - top5-acc: 0.9430 - val_loss: 1.1746 - val_acc: 0.5898 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2316 - acc: 0.5648 - top5-acc: 0.9451 - val_loss: 1.1589 - val_acc: 0.5894 - val_top5-acc: 0.9490 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2319 - acc: 0.5652 - top5-acc: 0.9436 - val_loss: 1.1249 - val_acc: 0.6128 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2297 - acc: 0.5683 - top5-acc: 0.9450 - val_loss: 1.1465 - val_acc: 0.6054 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2195 - acc: 0.5729 - top5-acc: 0.9453 - val_loss: 1.1299 - val_acc: 0.6106 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2239 - acc: 0.5677 - top5-acc: 0.9452 - val_loss: 1.1289 - val_acc: 0.6080 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2160 - acc: 0.5702 - top5-acc: 0.9464 - val_loss: 1.1344 - val_acc: 0.6042 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2188 - acc: 0.5703 - top5-acc: 0.9476 - val_loss: 1.1266 - val_acc: 0.6056 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2024 - acc: 0.5788 - top5-acc: 0.9465 - val_loss: 1.1162 - val_acc: 0.6150 - val_top5-acc: 0.9518 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2011 - acc: 0.5758 - top5-acc: 0.9484 - val_loss: 1.1085 - val_acc: 0.6158 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2059 - acc: 0.5743 - top5-acc: 0.9468 - val_loss: 1.1001 - val_acc: 0.6212 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2109 - acc: 0.5754 - top5-acc: 0.9475 - val_loss: 1.1170 - val_acc: 0.6076 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2076 - acc: 0.5747 - top5-acc: 0.9456 - val_loss: 1.1066 - val_acc: 0.6192 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2005 - acc: 0.5777 - top5-acc: 0.9476 - val_loss: 1.1093 - val_acc: 0.6160 - val_top5-acc: 0.9592 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2044 - acc: 0.5776 - top5-acc: 0.9468 - val_loss: 1.1023 - val_acc: 0.6166 - val_top5-acc: 0.9568 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2075 - acc: 0.5742 - top5-acc: 0.9474 - val_loss: 1.1141 - val_acc: 0.6144 - val_top5-acc: 0.9576 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1999 - acc: 0.5799 - top5-acc: 0.9470 - val_loss: 1.1047 - val_acc: 0.6164 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1953 - acc: 0.5797 - top5-acc: 0.9495 - val_loss: 1.1167 - val_acc: 0.6104 - val_top5-acc: 0.9562 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1968 - acc: 0.5818 - top5-acc: 0.9484 - val_loss: 1.1121 - val_acc: 0.6170 - val_top5-acc: 0.9556 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1981 - acc: 0.5794 - top5-acc: 0.9480 - val_loss: 1.1121 - val_acc: 0.6178 - val_top5-acc: 0.9544 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.1963 - acc: 0.5804 - top5-acc: 0.9479 - val_loss: 1.0995 - val_acc: 0.6208 - val_top5-acc: 0.9560 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1985 - acc: 0.5809 - top5-acc: 0.9481 - val_loss: 1.1023 - val_acc: 0.6204 - val_top5-acc: 0.9564 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1980 - acc: 0.5812 - top5-acc: 0.9488 - val_loss: 1.1059 - val_acc: 0.6192 - val_top5-acc: 0.9588 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.1990 - acc: 0.5823 - top5-acc: 0.9478 - val_loss: 1.0988 - val_acc: 0.6206 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.1984 - acc: 0.5766 - top5-acc: 0.9500 - val_loss: 1.1129 - val_acc: 0.6150 - val_top5-acc: 0.9586 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2015 - acc: 0.5772 - top5-acc: 0.9475 - val_loss: 1.1046 - val_acc: 0.6206 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.1937 - acc: 0.5817 - top5-acc: 0.9495 - val_loss: 1.1069 - val_acc: 0.6206 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1965 - acc: 0.5821 - top5-acc: 0.9491 - val_loss: 1.0986 - val_acc: 0.6224 - val_top5-acc: 0.9556 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.1937 - acc: 0.5815 - top5-acc: 0.9474 - val_loss: 1.1084 - val_acc: 0.6208 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2025 - acc: 0.5784 - top5-acc: 0.9471 - val_loss: 1.1228 - val_acc: 0.6096 - val_top5-acc: 0.9554 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1978 - acc: 0.5776 - top5-acc: 0.9492 - val_loss: 1.1110 - val_acc: 0.6110 - val_top5-acc: 0.9572 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1950 - acc: 0.5809 - top5-acc: 0.9491 - val_loss: 1.1085 - val_acc: 0.6208 - val_top5-acc: 0.9560 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2010 - acc: 0.5763 - top5-acc: 0.9482 - val_loss: 1.1100 - val_acc: 0.6204 - val_top5-acc: 0.9562 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.1949 - acc: 0.5812 - top5-acc: 0.9480 - val_loss: 1.1043 - val_acc: 0.6212 - val_top5-acc: 0.9560 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.1915 - acc: 0.5832 - top5-acc: 0.9488 - val_loss: 1.1067 - val_acc: 0.6220 - val_top5-acc: 0.9562 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.1956 - acc: 0.5790 - top5-acc: 0.9482 - val_loss: 1.1032 - val_acc: 0.6192 - val_top5-acc: 0.9580 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.1946 - acc: 0.5802 - top5-acc: 0.9488 - val_loss: 1.1123 - val_acc: 0.6196 - val_top5-acc: 0.9552 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.1964 - acc: 0.5812 - top5-acc: 0.9476 - val_loss: 1.1084 - val_acc: 0.6210 - val_top5-acc: 0.9558 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 36s 116ms/step - loss: 1.1168 - acc: 0.6052 - top5-acc: 0.9581\n",
      "Test accuracy: 60.52%\n",
      "Test top 5 accuracy: 95.81%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 282ms/step - loss: 1.4816 - acc: 0.4844 - top5-acc: 0.8952 - val_loss: 1.1571 - val_acc: 0.5874 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.1457 - acc: 0.5975 - top5-acc: 0.9539 - val_loss: 1.0981 - val_acc: 0.6114 - val_top5-acc: 0.9584 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.0992 - acc: 0.6151 - top5-acc: 0.9580 - val_loss: 1.0309 - val_acc: 0.6380 - val_top5-acc: 0.9644 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.0711 - acc: 0.6229 - top5-acc: 0.9612 - val_loss: 1.0314 - val_acc: 0.6352 - val_top5-acc: 0.9646 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 26s 299ms/step - loss: 1.0664 - acc: 0.6251 - top5-acc: 0.9600 - val_loss: 1.0117 - val_acc: 0.6444 - val_top5-acc: 0.9638 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 1.0594 - acc: 0.6271 - top5-acc: 0.9608 - val_loss: 1.0145 - val_acc: 0.6406 - val_top5-acc: 0.9658 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 24s 274ms/step - loss: 1.0492 - acc: 0.6320 - top5-acc: 0.9601 - val_loss: 1.0090 - val_acc: 0.6460 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 25s 280ms/step - loss: 1.0426 - acc: 0.6326 - top5-acc: 0.9634 - val_loss: 1.0026 - val_acc: 0.6424 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 1.0387 - acc: 0.6354 - top5-acc: 0.9633 - val_loss: 0.9910 - val_acc: 0.6514 - val_top5-acc: 0.9690 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.0381 - acc: 0.6363 - top5-acc: 0.9621 - val_loss: 0.9957 - val_acc: 0.6546 - val_top5-acc: 0.9680 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 1.0373 - acc: 0.6337 - top5-acc: 0.9649 - val_loss: 0.9893 - val_acc: 0.6566 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 24s 273ms/step - loss: 1.0327 - acc: 0.6382 - top5-acc: 0.9636 - val_loss: 1.0136 - val_acc: 0.6386 - val_top5-acc: 0.9664 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 1.0272 - acc: 0.6406 - top5-acc: 0.9639 - val_loss: 0.9984 - val_acc: 0.6510 - val_top5-acc: 0.9664 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.0283 - acc: 0.6365 - top5-acc: 0.9646 - val_loss: 0.9949 - val_acc: 0.6522 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 1.0344 - acc: 0.6336 - top5-acc: 0.9647 - val_loss: 0.9917 - val_acc: 0.6464 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 1.0322 - acc: 0.6359 - top5-acc: 0.9644 - val_loss: 0.9908 - val_acc: 0.6550 - val_top5-acc: 0.9696 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 1.0164 - acc: 0.6409 - top5-acc: 0.9649 - val_loss: 0.9691 - val_acc: 0.6586 - val_top5-acc: 0.9712 - lr: 0.0025\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.0154 - acc: 0.6427 - top5-acc: 0.9657 - val_loss: 0.9721 - val_acc: 0.6624 - val_top5-acc: 0.9698 - lr: 0.0025\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 1.0122 - acc: 0.6443 - top5-acc: 0.9656 - val_loss: 0.9793 - val_acc: 0.6578 - val_top5-acc: 0.9690 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 25s 289ms/step - loss: 1.0077 - acc: 0.6469 - top5-acc: 0.9648 - val_loss: 0.9751 - val_acc: 0.6592 - val_top5-acc: 0.9694 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 25s 279ms/step - loss: 1.0086 - acc: 0.6459 - top5-acc: 0.9658 - val_loss: 0.9679 - val_acc: 0.6626 - val_top5-acc: 0.9718 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 24s 276ms/step - loss: 1.0038 - acc: 0.6460 - top5-acc: 0.9655 - val_loss: 0.9763 - val_acc: 0.6572 - val_top5-acc: 0.9714 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 25s 281ms/step - loss: 1.0115 - acc: 0.6449 - top5-acc: 0.9640 - val_loss: 0.9649 - val_acc: 0.6576 - val_top5-acc: 0.9734 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.0106 - acc: 0.6456 - top5-acc: 0.9652 - val_loss: 0.9645 - val_acc: 0.6576 - val_top5-acc: 0.9734 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.0092 - acc: 0.6442 - top5-acc: 0.9656 - val_loss: 0.9619 - val_acc: 0.6630 - val_top5-acc: 0.9714 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.0103 - acc: 0.6431 - top5-acc: 0.9651 - val_loss: 0.9661 - val_acc: 0.6596 - val_top5-acc: 0.9706 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 25s 283ms/step - loss: 1.0204 - acc: 0.6355 - top5-acc: 0.9648 - val_loss: 0.9747 - val_acc: 0.6592 - val_top5-acc: 0.9704 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 26s 295ms/step - loss: 1.0099 - acc: 0.6433 - top5-acc: 0.9650 - val_loss: 0.9817 - val_acc: 0.6516 - val_top5-acc: 0.9698 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 25s 279ms/step - loss: 1.0044 - acc: 0.6476 - top5-acc: 0.9658 - val_loss: 0.9705 - val_acc: 0.6602 - val_top5-acc: 0.9718 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 25s 286ms/step - loss: 0.9973 - acc: 0.6493 - top5-acc: 0.9672 - val_loss: 0.9732 - val_acc: 0.6572 - val_top5-acc: 0.9728 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.9995 - acc: 0.6502 - top5-acc: 0.9658 - val_loss: 0.9502 - val_acc: 0.6670 - val_top5-acc: 0.9730 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.0026 - acc: 0.6482 - top5-acc: 0.9657 - val_loss: 0.9560 - val_acc: 0.6690 - val_top5-acc: 0.9692 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.0019 - acc: 0.6475 - top5-acc: 0.9662 - val_loss: 0.9551 - val_acc: 0.6638 - val_top5-acc: 0.9700 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.9988 - acc: 0.6482 - top5-acc: 0.9666 - val_loss: 0.9586 - val_acc: 0.6620 - val_top5-acc: 0.9714 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.9999 - acc: 0.6482 - top5-acc: 0.9661 - val_loss: 0.9564 - val_acc: 0.6634 - val_top5-acc: 0.9728 - lr: 0.0012\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 24s 269ms/step - loss: 0.9972 - acc: 0.6472 - top5-acc: 0.9680 - val_loss: 0.9545 - val_acc: 0.6648 - val_top5-acc: 0.9726 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.9982 - acc: 0.6490 - top5-acc: 0.9662 - val_loss: 0.9573 - val_acc: 0.6626 - val_top5-acc: 0.9714 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 23s 263ms/step - loss: 0.9940 - acc: 0.6496 - top5-acc: 0.9665 - val_loss: 0.9556 - val_acc: 0.6610 - val_top5-acc: 0.9720 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.9953 - acc: 0.6497 - top5-acc: 0.9670 - val_loss: 0.9608 - val_acc: 0.6618 - val_top5-acc: 0.9728 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.9986 - acc: 0.6471 - top5-acc: 0.9666 - val_loss: 0.9552 - val_acc: 0.6656 - val_top5-acc: 0.9730 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 23s 265ms/step - loss: 1.0007 - acc: 0.6495 - top5-acc: 0.9672 - val_loss: 0.9504 - val_acc: 0.6684 - val_top5-acc: 0.9720 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 0.9936 - acc: 0.6514 - top5-acc: 0.9677 - val_loss: 0.9542 - val_acc: 0.6656 - val_top5-acc: 0.9720 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.9931 - acc: 0.6520 - top5-acc: 0.9661 - val_loss: 0.9581 - val_acc: 0.6622 - val_top5-acc: 0.9722 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 23s 264ms/step - loss: 0.9982 - acc: 0.6486 - top5-acc: 0.9664 - val_loss: 0.9567 - val_acc: 0.6634 - val_top5-acc: 0.9712 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.9974 - acc: 0.6490 - top5-acc: 0.9661 - val_loss: 0.9590 - val_acc: 0.6634 - val_top5-acc: 0.9718 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.9947 - acc: 0.6499 - top5-acc: 0.9678 - val_loss: 0.9556 - val_acc: 0.6664 - val_top5-acc: 0.9726 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 23s 262ms/step - loss: 1.0004 - acc: 0.6476 - top5-acc: 0.9676 - val_loss: 0.9557 - val_acc: 0.6620 - val_top5-acc: 0.9726 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 23s 264ms/step - loss: 0.9975 - acc: 0.6486 - top5-acc: 0.9668 - val_loss: 0.9560 - val_acc: 0.6612 - val_top5-acc: 0.9726 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 0.9954 - acc: 0.6516 - top5-acc: 0.9661 - val_loss: 0.9555 - val_acc: 0.6618 - val_top5-acc: 0.9720 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.0004 - acc: 0.6486 - top5-acc: 0.9667 - val_loss: 0.9569 - val_acc: 0.6642 - val_top5-acc: 0.9714 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 34s 110ms/step - loss: 0.9750 - acc: 0.6604 - top5-acc: 0.9676\n",
      "Test accuracy: 66.04%\n",
      "Test top 5 accuracy: 96.76%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 33s 303ms/step - loss: 1.0002 - acc: 0.6666 - top5-acc: 0.9656 - val_loss: 0.8791 - val_acc: 0.7092 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 25s 282ms/step - loss: 0.8105 - acc: 0.7167 - top5-acc: 0.9802 - val_loss: 0.8587 - val_acc: 0.7086 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.7886 - acc: 0.7206 - top5-acc: 0.9809 - val_loss: 0.8401 - val_acc: 0.7152 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 0.7782 - acc: 0.7247 - top5-acc: 0.9826 - val_loss: 0.8493 - val_acc: 0.7102 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.7782 - acc: 0.7256 - top5-acc: 0.9818 - val_loss: 0.8523 - val_acc: 0.7106 - val_top5-acc: 0.9796 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.7829 - acc: 0.7239 - top5-acc: 0.9816 - val_loss: 0.8304 - val_acc: 0.7206 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.7711 - acc: 0.7306 - top5-acc: 0.9820 - val_loss: 0.8448 - val_acc: 0.7160 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.7789 - acc: 0.7261 - top5-acc: 0.9828 - val_loss: 0.8501 - val_acc: 0.7154 - val_top5-acc: 0.9796 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.7868 - acc: 0.7238 - top5-acc: 0.9823 - val_loss: 0.8470 - val_acc: 0.7148 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.7810 - acc: 0.7258 - top5-acc: 0.9823 - val_loss: 0.8509 - val_acc: 0.7114 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 24s 273ms/step - loss: 0.7800 - acc: 0.7255 - top5-acc: 0.9822 - val_loss: 0.8499 - val_acc: 0.7110 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.7614 - acc: 0.7308 - top5-acc: 0.9833 - val_loss: 0.8316 - val_acc: 0.7158 - val_top5-acc: 0.9808 - lr: 0.0025\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.7542 - acc: 0.7326 - top5-acc: 0.9836 - val_loss: 0.8282 - val_acc: 0.7186 - val_top5-acc: 0.9814 - lr: 0.0025\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.7568 - acc: 0.7346 - top5-acc: 0.9828 - val_loss: 0.8231 - val_acc: 0.7154 - val_top5-acc: 0.9806 - lr: 0.0025\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.7587 - acc: 0.7329 - top5-acc: 0.9827 - val_loss: 0.8318 - val_acc: 0.7202 - val_top5-acc: 0.9806 - lr: 0.0025\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.7588 - acc: 0.7328 - top5-acc: 0.9832 - val_loss: 0.8262 - val_acc: 0.7228 - val_top5-acc: 0.9806 - lr: 0.0025\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.7556 - acc: 0.7299 - top5-acc: 0.9836 - val_loss: 0.8281 - val_acc: 0.7176 - val_top5-acc: 0.9820 - lr: 0.0025\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.7607 - acc: 0.7329 - top5-acc: 0.9819 - val_loss: 0.8182 - val_acc: 0.7200 - val_top5-acc: 0.9810 - lr: 0.0025\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 23s 265ms/step - loss: 0.7588 - acc: 0.7339 - top5-acc: 0.9836 - val_loss: 0.8192 - val_acc: 0.7224 - val_top5-acc: 0.9804 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 23s 264ms/step - loss: 0.7580 - acc: 0.7306 - top5-acc: 0.9820 - val_loss: 0.8214 - val_acc: 0.7188 - val_top5-acc: 0.9796 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 23s 265ms/step - loss: 0.7588 - acc: 0.7330 - top5-acc: 0.9828 - val_loss: 0.8266 - val_acc: 0.7174 - val_top5-acc: 0.9808 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.7592 - acc: 0.7342 - top5-acc: 0.9826 - val_loss: 0.8187 - val_acc: 0.7224 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 25s 283ms/step - loss: 0.7583 - acc: 0.7316 - top5-acc: 0.9834 - val_loss: 0.8316 - val_acc: 0.7200 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.7483 - acc: 0.7348 - top5-acc: 0.9834 - val_loss: 0.8179 - val_acc: 0.7228 - val_top5-acc: 0.9812 - lr: 0.0012\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.7442 - acc: 0.7374 - top5-acc: 0.9839 - val_loss: 0.8140 - val_acc: 0.7232 - val_top5-acc: 0.9824 - lr: 0.0012\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 0.7499 - acc: 0.7345 - top5-acc: 0.9833 - val_loss: 0.8085 - val_acc: 0.7262 - val_top5-acc: 0.9808 - lr: 0.0012\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 26s 294ms/step - loss: 0.7458 - acc: 0.7380 - top5-acc: 0.9830 - val_loss: 0.8129 - val_acc: 0.7206 - val_top5-acc: 0.9818 - lr: 0.0012\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 25s 284ms/step - loss: 0.7504 - acc: 0.7361 - top5-acc: 0.9831 - val_loss: 0.8161 - val_acc: 0.7232 - val_top5-acc: 0.9830 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.7469 - acc: 0.7363 - top5-acc: 0.9829 - val_loss: 0.8185 - val_acc: 0.7236 - val_top5-acc: 0.9810 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.7472 - acc: 0.7362 - top5-acc: 0.9838 - val_loss: 0.8202 - val_acc: 0.7224 - val_top5-acc: 0.9818 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 26s 301ms/step - loss: 0.7498 - acc: 0.7353 - top5-acc: 0.9837 - val_loss: 0.8116 - val_acc: 0.7248 - val_top5-acc: 0.9818 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 26s 298ms/step - loss: 0.7416 - acc: 0.7395 - top5-acc: 0.9845 - val_loss: 0.8125 - val_acc: 0.7244 - val_top5-acc: 0.9804 - lr: 6.2500e-04\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 0.7448 - acc: 0.7381 - top5-acc: 0.9826 - val_loss: 0.8070 - val_acc: 0.7236 - val_top5-acc: 0.9820 - lr: 6.2500e-04\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.7410 - acc: 0.7388 - top5-acc: 0.9832 - val_loss: 0.8064 - val_acc: 0.7234 - val_top5-acc: 0.9810 - lr: 6.2500e-04\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.7456 - acc: 0.7368 - top5-acc: 0.9837 - val_loss: 0.8076 - val_acc: 0.7258 - val_top5-acc: 0.9812 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 26s 294ms/step - loss: 0.7425 - acc: 0.7378 - top5-acc: 0.9837 - val_loss: 0.8066 - val_acc: 0.7248 - val_top5-acc: 0.9828 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 27s 302ms/step - loss: 0.7421 - acc: 0.7373 - top5-acc: 0.9836 - val_loss: 0.8130 - val_acc: 0.7238 - val_top5-acc: 0.9814 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 26s 297ms/step - loss: 0.7380 - acc: 0.7410 - top5-acc: 0.9839 - val_loss: 0.8087 - val_acc: 0.7232 - val_top5-acc: 0.9818 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 26s 298ms/step - loss: 0.7402 - acc: 0.7395 - top5-acc: 0.9835 - val_loss: 0.8125 - val_acc: 0.7232 - val_top5-acc: 0.9822 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 26s 299ms/step - loss: 0.7408 - acc: 0.7386 - top5-acc: 0.9832 - val_loss: 0.8101 - val_acc: 0.7238 - val_top5-acc: 0.9814 - lr: 3.1250e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 26s 301ms/step - loss: 0.7372 - acc: 0.7418 - top5-acc: 0.9845 - val_loss: 0.8083 - val_acc: 0.7242 - val_top5-acc: 0.9808 - lr: 3.1250e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 27s 304ms/step - loss: 0.7359 - acc: 0.7402 - top5-acc: 0.9837 - val_loss: 0.8068 - val_acc: 0.7254 - val_top5-acc: 0.9818 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 27s 304ms/step - loss: 0.7337 - acc: 0.7416 - top5-acc: 0.9842 - val_loss: 0.8040 - val_acc: 0.7256 - val_top5-acc: 0.9818 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 27s 302ms/step - loss: 0.7380 - acc: 0.7389 - top5-acc: 0.9847 - val_loss: 0.8051 - val_acc: 0.7250 - val_top5-acc: 0.9816 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 25s 285ms/step - loss: 0.7344 - acc: 0.7415 - top5-acc: 0.9840 - val_loss: 0.8074 - val_acc: 0.7246 - val_top5-acc: 0.9816 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 24s 273ms/step - loss: 0.7371 - acc: 0.7396 - top5-acc: 0.9840 - val_loss: 0.8052 - val_acc: 0.7264 - val_top5-acc: 0.9824 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.7354 - acc: 0.7412 - top5-acc: 0.9849 - val_loss: 0.8078 - val_acc: 0.7248 - val_top5-acc: 0.9820 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.7359 - acc: 0.7414 - top5-acc: 0.9834 - val_loss: 0.8064 - val_acc: 0.7274 - val_top5-acc: 0.9822 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.7336 - acc: 0.7403 - top5-acc: 0.9840 - val_loss: 0.8034 - val_acc: 0.7264 - val_top5-acc: 0.9818 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 0.7369 - acc: 0.7373 - top5-acc: 0.9836 - val_loss: 0.8059 - val_acc: 0.7248 - val_top5-acc: 0.9820 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 39s 125ms/step - loss: 0.8441 - acc: 0.7069 - top5-acc: 0.9793\n",
      "Test accuracy: 70.69%\n",
      "Test top 5 accuracy: 97.93%\n"
     ]
    }
   ],
   "source": [
    "tested_acc_evolution = evol_accuracy(all_models,listnumblocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 1: VERIFICATIONS (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change paths in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'Results_Article/1A/1/mlpmixer_32ly_384Dc_2022-02-25'\n",
    "path = 'Results_Article/1A/mlpmixer_32ly_384Dc'\n",
    "#Call the file\n",
    "#tested_history=np.load( path + '/history_2022-02-25.npy',allow_pickle='TRUE').item()\n",
    "tested_history=np.load( path + '/history.npy',allow_pickle='TRUE').item()\n",
    "with open(path + '/accuracy.pkl','rb') as file:\n",
    "    tested_accuracy = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "curves(tested_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Results_Article/1B'\n",
    "total_acc = list()\n",
    "num_blocks = 32\n",
    "for i in range(0,num_blocks):\n",
    "    with open(path + '/accuracy_Blocks_'+ str(num_blocks) + '_L' + str(i+1) +'.pkl','rb') as file:\n",
    "        partial_acc = pickle.load(file)\n",
    "        total_acc.append(partial_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Change this parameters\n",
    "accuracy_evolution_tested = total_acc # the index represents the position of num_blocks\n",
    "name = '_' + str(num_blocks) +'L' # num_block[1] = 12 (layers)\n",
    "######################################################\n",
    "block = len(accuracy_evolution_tested)\n",
    "x = list(range(1,block+1))\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.plot(x,accuracy_evolution_tested,marker='o')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.locator_params(axis='x', nbins=block)\n",
    "plt.title('Evolution of Accuracy')\n",
    "plt.tight_layout()\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.savefig('Results_Article/1B/EvolutionAcc_' + name +'.png') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_image_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
