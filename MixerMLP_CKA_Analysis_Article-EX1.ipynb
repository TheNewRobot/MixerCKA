{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSzgroG_Suq"
   },
   "source": [
    "# Study of Image classification with modern MLP Mixer model and CKA\n",
    "\n",
    "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
    "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U087DdDw_Suy"
   },
   "source": [
    "# Setup for the MLP-Mixer Architecture\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AnTyoluw_Suz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import pickle\n",
    "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
    "from CKA_Google import *\n",
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Understand Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DAOrIHu_Su2"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1iMiVS7o_Su3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224 = 50176\n",
      "Patch size: 32 X 32 = 1024 \n",
      "Patches per image: 49\n",
      "Elements per patch (3 channels): 3072\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 512 \n",
    "num_epochs = 50\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Selected Architecture: B/32\n",
    "\n",
    "image_size = 224  # We'll resize input images to this size. Square\n",
    "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
    "embedding_dim = [384]  # Fixed Embedding Dimension\n",
    "num_blocks = [8,12,24,32]\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "#now = datetime.datetime.now()\n",
    "#date = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "now = datetime.datetime.today()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-02-25'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.datetime.today()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n",
    "str(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuu2OAS_Su0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Dataset for training \n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "#plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mpnEZH_Su5"
   },
   "source": [
    "## Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ra-bXojQ_Su6"
   },
   "outputs": [],
   "source": [
    "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data. \n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches. \n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks. ## (sequential_82)\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation) \n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpQFU8H__Su8"
   },
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9E1zD2BY_Su8"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        #Negative Log Likelihood = Categorical Cross Entropy\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping regularization callback. \n",
    "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history, accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxeE0cmM_Su9"
   },
   "source": [
    "## Use data augmentation\n",
    "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zh6hFPWX_Su-"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G77cUgiu_Su-"
   },
   "source": [
    "## Implement patch extraction as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYKNktXe_Su_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        #Extract the shape dimension in the position 0 = columns\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Without overlapping, stride horizontally and vertically\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
    "            rates=[1, 1, 1, 1],\n",
    "            #Patches contained in the images are considered, no zero padding\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        #shape[-1], number of colummns, as well as shape[0]\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Patches, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'patch_size' : self.patch_size ,\n",
    "            'num_patches' : self.num_patches\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yehcSS_Su_"
   },
   "source": [
    "## The MLP-Mixer model\n",
    "\n",
    "The MLP-Mixer is an architecture based exclusively on\n",
    "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "1. One applied independently to image patches, which mixes the per-location features.\n",
    "2. The other applied across patches (along channels), which mixes spatial information.\n",
    "\n",
    "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
    "instead of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwg4e2n_SvA"
   },
   "source": [
    "### Implement the MLP-Mixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dT6wVEki_SvA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp2 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super(MLPMixerLayer, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'num_patches' : num_patches,\n",
    "            'embedding_dim' : embedding_dim,\n",
    "            'dropout_rate' : dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUiufq92_SvA"
   },
   "source": [
    "## Build, train, and evaluate the MLP-Mixer model\n",
    "\n",
    "Note that training the model with the current settings on a V100 GPUs\n",
    "takes around 8 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report: Learning Curve\n",
    "def curves(history):\n",
    "    ymax1 = min(history[\"loss\"])\n",
    "    xmax1 = history[\"loss\"].index(ymax1)\n",
    "    ymax2 = min(history[\"val_loss\"])\n",
    "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
    "    plt.title(\"Cross Entropy Loss\")\n",
    "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
    "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Graph accuracy\n",
    "    ymax3 = max(history[\"acc\"])\n",
    "    xmax3 = history[\"acc\"].index(ymax3)\n",
    "    ymax4 = max(history[\"val_acc\"])\n",
    "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
    "    ymax5 = max(history[\"top5-acc\"])\n",
    "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
    "    ymax6 = max(history[\"val_top5-acc\"])\n",
    "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Classification accuracy')\n",
    "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Classification top5-acc')\n",
    "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.suptitle(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain activations + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers + Patches + One dense layer\n",
    "def Preprocessing(num_example):\n",
    "    augmented = data_augmentation(x_train[num_example])\n",
    "    b = Patches(patch_size, num_patches)(augmented)\n",
    "    a = layers.Dense(units=embedding_dim)(b)\n",
    "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
    "def Batch_Preprocessing(batch_size):\n",
    "    #Vector with the number of Sample of the Xtrain\n",
    "    a  = list(range(0,x_train.shape[0]))\n",
    "    b = random.sample(a,batch_size)\n",
    "    batch_regularization = list()\n",
    "    for i in range(0,batch_size):\n",
    "        inter_result = Preprocessing(b[i])\n",
    "        batch_regularization.append(inter_result)\n",
    "    return batch_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_out(result,layer_number,example):\n",
    "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "    ax1.imshow(x_train[example])\n",
    "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
    "    ax2.imshow(result[layer_number])\n",
    "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
    "    #The input is fixed to the beginning of the mlp blocks\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[0]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of layer's activation\n",
    "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
    "def Heatmap(result,type,sigma):\n",
    "    dim = len(result)\n",
    "    k = (dim - 1)\n",
    "    heatmap_CKA = np.zeros((dim,dim))\n",
    "    for i in range(0,dim):\n",
    "        tr = (dim - 1)\n",
    "        for j in range(0,dim):\n",
    "            if type == 'kernel':\n",
    "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
    "            elif type == 'linear':\n",
    "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
    "            else:\n",
    "                print('There is no such category, try again')\n",
    "                break\n",
    "\n",
    "            tr -= 1\n",
    "        k -= 1\n",
    "    #print('CKA' + type + 'calculated')\n",
    "    return heatmap_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of heatmaps (obsolet)\n",
    "def Prom_Mixer_Heatmaps(batch_result,type):\n",
    "    mat_heatmaps = list()\n",
    "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
    "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
    "        mat_heatmaps.append(heatmap_raw)\n",
    "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
    "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
    "    return prom_mixer_heatmap,mat_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 4\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i'%(i+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(6,6))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Understand Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A: Different Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create different mlpmixers according to an array of widths or depths\n",
    "def mlpmixer_iterations(num_patches,experiment,embedding_dim,num_blocks):\n",
    "    it_widths = len(embedding_dim)\n",
    "    it_blocks = len(num_blocks)\n",
    "    for j in range(it_widths):\n",
    "        for i in range(it_blocks):\n",
    "            mlpmixer_blocks = keras.Sequential(\n",
    "            [MLPMixerLayer(num_patches, embedding_dim[j], dropout_rate) for _ in range(num_blocks[i])] # creates the number of block without a \n",
    "            )\n",
    "            mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim[j]) # Returns the model\n",
    "            history,accuracy, top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "            #Saving Results\n",
    "            pwd = 'Results_Article/'+ str(experiment) +'/mlpmixer_'+ str(num_blocks[i]) + 'ly_' + str(embedding_dim[j]) + 'Dc_' + str(date)\n",
    "            mlpmixer_classifier.save(pwd)\n",
    "            np.save( pwd + '/history_' + str(date) +'.npy',history.history)\n",
    "            with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "                pickle.dump(accuracy,file)\n",
    "            with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "                pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 22s 174ms/step - loss: 4.5446 - acc: 0.2343 - top5-acc: 0.7322 - val_loss: 1.7836 - val_acc: 0.3754 - val_top5-acc: 0.8638 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.6200 - acc: 0.4134 - top5-acc: 0.8955 - val_loss: 1.5375 - val_acc: 0.4696 - val_top5-acc: 0.9132 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.4915 - acc: 0.4621 - top5-acc: 0.9159 - val_loss: 1.3941 - val_acc: 0.5020 - val_top5-acc: 0.9340 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.4013 - acc: 0.4958 - top5-acc: 0.9279 - val_loss: 1.2713 - val_acc: 0.5390 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.3262 - acc: 0.5251 - top5-acc: 0.9372 - val_loss: 1.2302 - val_acc: 0.5584 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 1.2824 - acc: 0.5405 - top5-acc: 0.9426 - val_loss: 1.2156 - val_acc: 0.5746 - val_top5-acc: 0.9490 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 15s 170ms/step - loss: 1.2562 - acc: 0.5486 - top5-acc: 0.9453 - val_loss: 1.1434 - val_acc: 0.5898 - val_top5-acc: 0.9588 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2119 - acc: 0.5658 - top5-acc: 0.9500 - val_loss: 1.1061 - val_acc: 0.6082 - val_top5-acc: 0.9634 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 15s 169ms/step - loss: 1.1840 - acc: 0.5761 - top5-acc: 0.9522 - val_loss: 1.0866 - val_acc: 0.6130 - val_top5-acc: 0.9668 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 1.1492 - acc: 0.5916 - top5-acc: 0.9560 - val_loss: 1.0643 - val_acc: 0.6220 - val_top5-acc: 0.9660 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.1287 - acc: 0.6023 - top5-acc: 0.9578 - val_loss: 1.0295 - val_acc: 0.6384 - val_top5-acc: 0.9666 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.1152 - acc: 0.6037 - top5-acc: 0.9580 - val_loss: 1.0228 - val_acc: 0.6458 - val_top5-acc: 0.9702 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.0933 - acc: 0.6101 - top5-acc: 0.9607 - val_loss: 1.0099 - val_acc: 0.6468 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.0614 - acc: 0.6238 - top5-acc: 0.9624 - val_loss: 0.9955 - val_acc: 0.6548 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.0553 - acc: 0.6277 - top5-acc: 0.9632 - val_loss: 1.0108 - val_acc: 0.6458 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.0319 - acc: 0.6340 - top5-acc: 0.9669 - val_loss: 0.9636 - val_acc: 0.6642 - val_top5-acc: 0.9742 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.0314 - acc: 0.6342 - top5-acc: 0.9648 - val_loss: 0.9282 - val_acc: 0.6728 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.9959 - acc: 0.6502 - top5-acc: 0.9664 - val_loss: 0.9078 - val_acc: 0.6786 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 0.9900 - acc: 0.6526 - top5-acc: 0.9680 - val_loss: 0.9116 - val_acc: 0.6832 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.9748 - acc: 0.6562 - top5-acc: 0.9697 - val_loss: 0.9506 - val_acc: 0.6736 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.9674 - acc: 0.6592 - top5-acc: 0.9708 - val_loss: 0.8963 - val_acc: 0.6818 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 0.9592 - acc: 0.6596 - top5-acc: 0.9708 - val_loss: 0.9270 - val_acc: 0.6762 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 0.9481 - acc: 0.6636 - top5-acc: 0.9724 - val_loss: 0.9315 - val_acc: 0.6822 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.9290 - acc: 0.6713 - top5-acc: 0.9728 - val_loss: 0.9071 - val_acc: 0.6872 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.9093 - acc: 0.6792 - top5-acc: 0.9744 - val_loss: 0.8650 - val_acc: 0.7038 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.9047 - acc: 0.6816 - top5-acc: 0.9742 - val_loss: 0.8928 - val_acc: 0.6944 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.9127 - acc: 0.6773 - top5-acc: 0.9737 - val_loss: 0.8697 - val_acc: 0.6974 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.8999 - acc: 0.6808 - top5-acc: 0.9767 - val_loss: 0.8936 - val_acc: 0.6888 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.9064 - acc: 0.6826 - top5-acc: 0.9749 - val_loss: 0.8560 - val_acc: 0.7052 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.8986 - acc: 0.6842 - top5-acc: 0.9755 - val_loss: 0.8382 - val_acc: 0.7134 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 0.8795 - acc: 0.6904 - top5-acc: 0.9747 - val_loss: 0.8399 - val_acc: 0.7082 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 0.8811 - acc: 0.6903 - top5-acc: 0.9763 - val_loss: 0.8835 - val_acc: 0.6990 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 0.8728 - acc: 0.6940 - top5-acc: 0.9772 - val_loss: 0.8552 - val_acc: 0.7088 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 0.8548 - acc: 0.6970 - top5-acc: 0.9789 - val_loss: 0.8204 - val_acc: 0.7190 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.8363 - acc: 0.7062 - top5-acc: 0.9782 - val_loss: 0.8161 - val_acc: 0.7198 - val_top5-acc: 0.9828 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.8430 - acc: 0.7014 - top5-acc: 0.9788 - val_loss: 0.8175 - val_acc: 0.7138 - val_top5-acc: 0.9828 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.8330 - acc: 0.7071 - top5-acc: 0.9793 - val_loss: 0.7990 - val_acc: 0.7238 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.8392 - acc: 0.7030 - top5-acc: 0.9786 - val_loss: 0.7936 - val_acc: 0.7330 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 0.8366 - acc: 0.7062 - top5-acc: 0.9784 - val_loss: 0.8254 - val_acc: 0.7176 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 15s 165ms/step - loss: 0.8347 - acc: 0.7055 - top5-acc: 0.9798 - val_loss: 0.8130 - val_acc: 0.7206 - val_top5-acc: 0.9834 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 0.8244 - acc: 0.7099 - top5-acc: 0.9791 - val_loss: 0.8516 - val_acc: 0.7164 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.8049 - acc: 0.7153 - top5-acc: 0.9808 - val_loss: 0.8506 - val_acc: 0.7264 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 0.7996 - acc: 0.7188 - top5-acc: 0.9804 - val_loss: 0.8722 - val_acc: 0.7132 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 0.7226 - acc: 0.7448 - top5-acc: 0.9852 - val_loss: 0.7585 - val_acc: 0.7392 - val_top5-acc: 0.9856 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 0.6969 - acc: 0.7561 - top5-acc: 0.9852 - val_loss: 0.7419 - val_acc: 0.7490 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 15s 170ms/step - loss: 0.6792 - acc: 0.7599 - top5-acc: 0.9878 - val_loss: 0.7377 - val_acc: 0.7472 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 0.6784 - acc: 0.7596 - top5-acc: 0.9872 - val_loss: 0.7512 - val_acc: 0.7474 - val_top5-acc: 0.9854 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 0.6735 - acc: 0.7634 - top5-acc: 0.9873 - val_loss: 0.7363 - val_acc: 0.7520 - val_top5-acc: 0.9850 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 0.6761 - acc: 0.7614 - top5-acc: 0.9868 - val_loss: 0.7635 - val_acc: 0.7454 - val_top5-acc: 0.9852 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.6845 - acc: 0.7615 - top5-acc: 0.9864 - val_loss: 0.7457 - val_acc: 0.7484 - val_top5-acc: 0.9852 - lr: 0.0025\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.7867 - acc: 0.7404 - top5-acc: 0.9828\n",
      "Test accuracy: 74.04%\n",
      "Test top 5 accuracy: 98.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn, layer_normalization_1_layer_call_and_return_conditional_losses, layer_normalization_2_layer_call_fn while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_8ly_384Dc_2022-02-25_13-55\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_8ly_384Dc_2022-02-25_13-55\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 245ms/step - loss: 3.8224 - acc: 0.2632 - top5-acc: 0.7548 - val_loss: 1.5950 - val_acc: 0.4200 - val_top5-acc: 0.8932 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.5689 - acc: 0.4318 - top5-acc: 0.9009 - val_loss: 1.3964 - val_acc: 0.4852 - val_top5-acc: 0.9286 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.4328 - acc: 0.4846 - top5-acc: 0.9240 - val_loss: 1.3727 - val_acc: 0.5036 - val_top5-acc: 0.9368 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.3495 - acc: 0.5159 - top5-acc: 0.9352 - val_loss: 1.2174 - val_acc: 0.5644 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2671 - acc: 0.5455 - top5-acc: 0.9450 - val_loss: 1.1756 - val_acc: 0.5774 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2351 - acc: 0.5601 - top5-acc: 0.9488 - val_loss: 1.1350 - val_acc: 0.5870 - val_top5-acc: 0.9598 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1946 - acc: 0.5748 - top5-acc: 0.9523 - val_loss: 1.1395 - val_acc: 0.5900 - val_top5-acc: 0.9636 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1611 - acc: 0.5842 - top5-acc: 0.9561 - val_loss: 1.1043 - val_acc: 0.6068 - val_top5-acc: 0.9650 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1282 - acc: 0.5967 - top5-acc: 0.9596 - val_loss: 1.1020 - val_acc: 0.6106 - val_top5-acc: 0.9632 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.1172 - acc: 0.5995 - top5-acc: 0.9599 - val_loss: 1.0573 - val_acc: 0.6258 - val_top5-acc: 0.9690 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0923 - acc: 0.6115 - top5-acc: 0.9602 - val_loss: 1.0144 - val_acc: 0.6442 - val_top5-acc: 0.9680 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0574 - acc: 0.6234 - top5-acc: 0.9646 - val_loss: 0.9843 - val_acc: 0.6466 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.0514 - acc: 0.6254 - top5-acc: 0.9642 - val_loss: 1.0033 - val_acc: 0.6468 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.0346 - acc: 0.6336 - top5-acc: 0.9669 - val_loss: 1.0243 - val_acc: 0.6474 - val_top5-acc: 0.9672 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0122 - acc: 0.6418 - top5-acc: 0.9670 - val_loss: 0.9712 - val_acc: 0.6634 - val_top5-acc: 0.9696 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0009 - acc: 0.6461 - top5-acc: 0.9673 - val_loss: 0.9622 - val_acc: 0.6636 - val_top5-acc: 0.9732 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9712 - acc: 0.6549 - top5-acc: 0.9696 - val_loss: 0.9642 - val_acc: 0.6644 - val_top5-acc: 0.9734 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9483 - acc: 0.6638 - top5-acc: 0.9718 - val_loss: 0.9305 - val_acc: 0.6790 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9470 - acc: 0.6664 - top5-acc: 0.9714 - val_loss: 0.9289 - val_acc: 0.6736 - val_top5-acc: 0.9758 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9293 - acc: 0.6725 - top5-acc: 0.9734 - val_loss: 0.9146 - val_acc: 0.6836 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9199 - acc: 0.6765 - top5-acc: 0.9734 - val_loss: 0.8870 - val_acc: 0.6880 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9158 - acc: 0.6742 - top5-acc: 0.9736 - val_loss: 0.8768 - val_acc: 0.6936 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8986 - acc: 0.6838 - top5-acc: 0.9765 - val_loss: 0.9679 - val_acc: 0.6714 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8850 - acc: 0.6859 - top5-acc: 0.9758 - val_loss: 0.8687 - val_acc: 0.6958 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8812 - acc: 0.6886 - top5-acc: 0.9767 - val_loss: 0.8590 - val_acc: 0.7014 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8714 - acc: 0.6913 - top5-acc: 0.9762 - val_loss: 0.8926 - val_acc: 0.6894 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8669 - acc: 0.6979 - top5-acc: 0.9773 - val_loss: 0.8642 - val_acc: 0.7074 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8632 - acc: 0.6960 - top5-acc: 0.9778 - val_loss: 0.9370 - val_acc: 0.6842 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8687 - acc: 0.6947 - top5-acc: 0.9775 - val_loss: 0.8760 - val_acc: 0.6942 - val_top5-acc: 0.9826 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8398 - acc: 0.7036 - top5-acc: 0.9794 - val_loss: 0.8504 - val_acc: 0.7124 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8130 - acc: 0.7136 - top5-acc: 0.9799 - val_loss: 0.8294 - val_acc: 0.7118 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8273 - acc: 0.7078 - top5-acc: 0.9788 - val_loss: 0.8347 - val_acc: 0.7210 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8172 - acc: 0.7123 - top5-acc: 0.9798 - val_loss: 0.8542 - val_acc: 0.7098 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8116 - acc: 0.7156 - top5-acc: 0.9806 - val_loss: 0.9028 - val_acc: 0.7036 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.7914 - acc: 0.7196 - top5-acc: 0.9816 - val_loss: 0.8432 - val_acc: 0.7058 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8076 - acc: 0.7156 - top5-acc: 0.9813 - val_loss: 0.8236 - val_acc: 0.7250 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.7849 - acc: 0.7224 - top5-acc: 0.9819 - val_loss: 0.8461 - val_acc: 0.7188 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.7729 - acc: 0.7272 - top5-acc: 0.9832 - val_loss: 0.8342 - val_acc: 0.7248 - val_top5-acc: 0.9836 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 8.7678 - acc: 0.4286 - top5-acc: 0.8470 - val_loss: 1.1751 - val_acc: 0.5950 - val_top5-acc: 0.9578 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.0850 - acc: 0.6149 - top5-acc: 0.9607 - val_loss: 0.9423 - val_acc: 0.6780 - val_top5-acc: 0.9730 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.9359 - acc: 0.6677 - top5-acc: 0.9727 - val_loss: 0.8536 - val_acc: 0.7028 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8528 - acc: 0.6982 - top5-acc: 0.9786 - val_loss: 0.8377 - val_acc: 0.7132 - val_top5-acc: 0.9788 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.8174 - acc: 0.7120 - top5-acc: 0.9791 - val_loss: 0.7989 - val_acc: 0.7272 - val_top5-acc: 0.9792 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.7868 - acc: 0.7210 - top5-acc: 0.9814 - val_loss: 0.7940 - val_acc: 0.7270 - val_top5-acc: 0.9800 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.7635 - acc: 0.7286 - top5-acc: 0.9826 - val_loss: 0.7639 - val_acc: 0.7350 - val_top5-acc: 0.9840 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.7388 - acc: 0.7397 - top5-acc: 0.9826 - val_loss: 0.7602 - val_acc: 0.7434 - val_top5-acc: 0.9846 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.7210 - acc: 0.7452 - top5-acc: 0.9848 - val_loss: 0.7597 - val_acc: 0.7400 - val_top5-acc: 0.9848 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.6987 - acc: 0.7562 - top5-acc: 0.9846 - val_loss: 0.7596 - val_acc: 0.7464 - val_top5-acc: 0.9852 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.6897 - acc: 0.7566 - top5-acc: 0.9858 - val_loss: 0.7518 - val_acc: 0.7544 - val_top5-acc: 0.9866 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 0.6731 - acc: 0.7604 - top5-acc: 0.9871 - val_loss: 0.7394 - val_acc: 0.7470 - val_top5-acc: 0.9852 - lr: 0.0025\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.7590 - acc: 0.7459 - top5-acc: 0.9832\n",
      "Test accuracy: 74.59%\n",
      "Test top 5 accuracy: 98.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_8_layer_call_fn, layer_normalization_8_layer_call_and_return_conditional_losses, layer_normalization_9_layer_call_fn, layer_normalization_9_layer_call_and_return_conditional_losses, layer_normalization_10_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_12ly_384Dc_2022-02-25_13-55\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_12ly_384Dc_2022-02-25_13-55\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 57s 462ms/step - loss: 3.5592 - acc: 0.2721 - top5-acc: 0.7581 - val_loss: 1.5523 - val_acc: 0.4408 - val_top5-acc: 0.9074 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 38s 436ms/step - loss: 1.5207 - acc: 0.4530 - top5-acc: 0.9097 - val_loss: 1.3787 - val_acc: 0.5068 - val_top5-acc: 0.9294 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 1.3784 - acc: 0.5028 - top5-acc: 0.9318 - val_loss: 1.3047 - val_acc: 0.5316 - val_top5-acc: 0.9426 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 1.3176 - acc: 0.5282 - top5-acc: 0.9386 - val_loss: 1.2428 - val_acc: 0.5602 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 1.2449 - acc: 0.5551 - top5-acc: 0.9457 - val_loss: 1.1765 - val_acc: 0.5790 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.2168 - acc: 0.5656 - top5-acc: 0.9502 - val_loss: 1.1143 - val_acc: 0.5984 - val_top5-acc: 0.9590 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.1638 - acc: 0.5857 - top5-acc: 0.9532 - val_loss: 1.0960 - val_acc: 0.6132 - val_top5-acc: 0.9630 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 1.1209 - acc: 0.6015 - top5-acc: 0.9578 - val_loss: 1.0668 - val_acc: 0.6160 - val_top5-acc: 0.9622 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 1.0866 - acc: 0.6138 - top5-acc: 0.9626 - val_loss: 1.0319 - val_acc: 0.6392 - val_top5-acc: 0.9652 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 38s 436ms/step - loss: 1.0665 - acc: 0.6218 - top5-acc: 0.9638 - val_loss: 1.0113 - val_acc: 0.6432 - val_top5-acc: 0.9712 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 38s 436ms/step - loss: 1.0574 - acc: 0.6248 - top5-acc: 0.9633 - val_loss: 1.0034 - val_acc: 0.6464 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 1.0280 - acc: 0.6347 - top5-acc: 0.9647 - val_loss: 0.9677 - val_acc: 0.6578 - val_top5-acc: 0.9722 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 1.0012 - acc: 0.6449 - top5-acc: 0.9685 - val_loss: 0.9657 - val_acc: 0.6586 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 0.9842 - acc: 0.6512 - top5-acc: 0.9700 - val_loss: 0.9398 - val_acc: 0.6698 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 0.9764 - acc: 0.6570 - top5-acc: 0.9697 - val_loss: 0.9326 - val_acc: 0.6774 - val_top5-acc: 0.9734 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 0.9430 - acc: 0.6676 - top5-acc: 0.9724 - val_loss: 0.9623 - val_acc: 0.6626 - val_top5-acc: 0.9744 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 39s 438ms/step - loss: 0.9242 - acc: 0.6755 - top5-acc: 0.9740 - val_loss: 0.9054 - val_acc: 0.6756 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 38s 438ms/step - loss: 0.8951 - acc: 0.6852 - top5-acc: 0.9764 - val_loss: 0.8799 - val_acc: 0.6958 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 13.2241 - acc: 0.5778 - top5-acc: 0.8968 - val_loss: 50.1574 - val_acc: 0.1312 - val_top5-acc: 0.5182 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 5.5453 - acc: 0.3199 - top5-acc: 0.8037 - val_loss: 1.4428 - val_acc: 0.4826 - val_top5-acc: 0.9242 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 38s 436ms/step - loss: 1.4177 - acc: 0.4941 - top5-acc: 0.9232 - val_loss: 1.2638 - val_acc: 0.5546 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 38s 436ms/step - loss: 1.2724 - acc: 0.5452 - top5-acc: 0.9414 - val_loss: 1.1445 - val_acc: 0.5910 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 1.1764 - acc: 0.5794 - top5-acc: 0.9542 - val_loss: 1.1040 - val_acc: 0.6068 - val_top5-acc: 0.9640 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 38s 436ms/step - loss: 1.1090 - acc: 0.6062 - top5-acc: 0.9601 - val_loss: 1.0278 - val_acc: 0.6400 - val_top5-acc: 0.9712 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 38s 436ms/step - loss: 1.0685 - acc: 0.6210 - top5-acc: 0.9630 - val_loss: 0.9916 - val_acc: 0.6456 - val_top5-acc: 0.9702 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 38s 437ms/step - loss: 1.0373 - acc: 0.6304 - top5-acc: 0.9648 - val_loss: 0.9753 - val_acc: 0.6556 - val_top5-acc: 0.9726 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 38s 435ms/step - loss: 1.0163 - acc: 0.6391 - top5-acc: 0.9678 - val_loss: 0.9461 - val_acc: 0.6694 - val_top5-acc: 0.9720 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 38s 436ms/step - loss: 0.9852 - acc: 0.6516 - top5-acc: 0.9687 - val_loss: 0.9585 - val_acc: 0.6608 - val_top5-acc: 0.9724 - lr: 0.0025\n",
      "313/313 [==============================] - 30s 95ms/step - loss: 0.9242 - acc: 0.6850 - top5-acc: 0.9742\n",
      "Test accuracy: 68.5%\n",
      "Test top 5 accuracy: 97.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_20_layer_call_fn, layer_normalization_20_layer_call_and_return_conditional_losses, layer_normalization_21_layer_call_fn, layer_normalization_21_layer_call_and_return_conditional_losses, layer_normalization_22_layer_call_fn while saving (showing 5 of 48). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_24ly_384Dc_2022-02-25_13-55\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_24ly_384Dc_2022-02-25_13-55\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 77s 622ms/step - loss: 4.9228 - acc: 0.2344 - top5-acc: 0.7239 - val_loss: 1.6128 - val_acc: 0.4122 - val_top5-acc: 0.8912 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 1.6078 - acc: 0.4168 - top5-acc: 0.8970 - val_loss: 1.3992 - val_acc: 0.4938 - val_top5-acc: 0.9302 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.4302 - acc: 0.4872 - top5-acc: 0.9233 - val_loss: 1.2825 - val_acc: 0.5446 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.3356 - acc: 0.5216 - top5-acc: 0.9360 - val_loss: 1.1851 - val_acc: 0.5758 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 1.2743 - acc: 0.5443 - top5-acc: 0.9436 - val_loss: 1.1888 - val_acc: 0.5766 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.2230 - acc: 0.5626 - top5-acc: 0.9489 - val_loss: 1.1161 - val_acc: 0.5992 - val_top5-acc: 0.9620 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 1.1755 - acc: 0.5832 - top5-acc: 0.9528 - val_loss: 1.1363 - val_acc: 0.5948 - val_top5-acc: 0.9586 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 1.1410 - acc: 0.5951 - top5-acc: 0.9566 - val_loss: 1.0929 - val_acc: 0.6160 - val_top5-acc: 0.9626 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.1028 - acc: 0.6104 - top5-acc: 0.9600 - val_loss: 1.0933 - val_acc: 0.6172 - val_top5-acc: 0.9676 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.0654 - acc: 0.6219 - top5-acc: 0.9630 - val_loss: 1.0253 - val_acc: 0.6328 - val_top5-acc: 0.9702 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 1.0588 - acc: 0.6277 - top5-acc: 0.9627 - val_loss: 0.9847 - val_acc: 0.6492 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 1.0256 - acc: 0.6377 - top5-acc: 0.9660 - val_loss: 0.9957 - val_acc: 0.6470 - val_top5-acc: 0.9696 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.9973 - acc: 0.6469 - top5-acc: 0.9674 - val_loss: 0.9430 - val_acc: 0.6590 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.9746 - acc: 0.6573 - top5-acc: 0.9688 - val_loss: 0.9544 - val_acc: 0.6606 - val_top5-acc: 0.9708 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.9487 - acc: 0.6635 - top5-acc: 0.9717 - val_loss: 0.9301 - val_acc: 0.6738 - val_top5-acc: 0.9758 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.9417 - acc: 0.6670 - top5-acc: 0.9725 - val_loss: 0.8907 - val_acc: 0.6862 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.9140 - acc: 0.6776 - top5-acc: 0.9737 - val_loss: 0.8645 - val_acc: 0.7004 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.8941 - acc: 0.6862 - top5-acc: 0.9756 - val_loss: 0.8435 - val_acc: 0.6954 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.8644 - acc: 0.6928 - top5-acc: 0.9780 - val_loss: 0.8690 - val_acc: 0.6962 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.8631 - acc: 0.6948 - top5-acc: 0.9782 - val_loss: 0.8764 - val_acc: 0.6962 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.8465 - acc: 0.6989 - top5-acc: 0.9780 - val_loss: 0.8463 - val_acc: 0.7040 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.8289 - acc: 0.7087 - top5-acc: 0.9794 - val_loss: 0.8569 - val_acc: 0.7000 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.8092 - acc: 0.7147 - top5-acc: 0.9804 - val_loss: 0.8562 - val_acc: 0.7114 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.7201 - acc: 0.7444 - top5-acc: 0.9853 - val_loss: 0.7570 - val_acc: 0.7386 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.6827 - acc: 0.7594 - top5-acc: 0.9862 - val_loss: 0.7556 - val_acc: 0.7360 - val_top5-acc: 0.9856 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.6687 - acc: 0.7641 - top5-acc: 0.9870 - val_loss: 0.7742 - val_acc: 0.7360 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.6598 - acc: 0.7658 - top5-acc: 0.9881 - val_loss: 0.7362 - val_acc: 0.7480 - val_top5-acc: 0.9850 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.6496 - acc: 0.7705 - top5-acc: 0.9882 - val_loss: 0.7531 - val_acc: 0.7472 - val_top5-acc: 0.9822 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.6457 - acc: 0.7726 - top5-acc: 0.9883 - val_loss: 0.7601 - val_acc: 0.7440 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.6352 - acc: 0.7745 - top5-acc: 0.9888 - val_loss: 0.7743 - val_acc: 0.7344 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 51s 580ms/step - loss: 0.6245 - acc: 0.7796 - top5-acc: 0.9895 - val_loss: 0.7671 - val_acc: 0.7486 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 51s 580ms/step - loss: 0.6203 - acc: 0.7803 - top5-acc: 0.9894 - val_loss: 0.7502 - val_acc: 0.7480 - val_top5-acc: 0.9848 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 51s 581ms/step - loss: 0.5462 - acc: 0.8063 - top5-acc: 0.9920 - val_loss: 0.7079 - val_acc: 0.7584 - val_top5-acc: 0.9870 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 51s 581ms/step - loss: 0.5244 - acc: 0.8142 - top5-acc: 0.9922 - val_loss: 0.7015 - val_acc: 0.7686 - val_top5-acc: 0.9870 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 51s 580ms/step - loss: 0.5162 - acc: 0.8170 - top5-acc: 0.9929 - val_loss: 0.7106 - val_acc: 0.7640 - val_top5-acc: 0.9850 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.5044 - acc: 0.8210 - top5-acc: 0.9934 - val_loss: 0.7096 - val_acc: 0.7612 - val_top5-acc: 0.9860 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.4935 - acc: 0.8228 - top5-acc: 0.9942 - val_loss: 0.7340 - val_acc: 0.7558 - val_top5-acc: 0.9858 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.4915 - acc: 0.8252 - top5-acc: 0.9943 - val_loss: 0.7191 - val_acc: 0.7618 - val_top5-acc: 0.9860 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.4808 - acc: 0.8273 - top5-acc: 0.9946 - val_loss: 0.7205 - val_acc: 0.7664 - val_top5-acc: 0.9856 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.4320 - acc: 0.8464 - top5-acc: 0.9953 - val_loss: 0.7005 - val_acc: 0.7746 - val_top5-acc: 0.9880 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.4144 - acc: 0.8521 - top5-acc: 0.9962 - val_loss: 0.7026 - val_acc: 0.7750 - val_top5-acc: 0.9854 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.4046 - acc: 0.8572 - top5-acc: 0.9959 - val_loss: 0.7310 - val_acc: 0.7692 - val_top5-acc: 0.9848 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.4032 - acc: 0.8553 - top5-acc: 0.9966 - val_loss: 0.7206 - val_acc: 0.7718 - val_top5-acc: 0.9852 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.3875 - acc: 0.8637 - top5-acc: 0.9964 - val_loss: 0.7344 - val_acc: 0.7702 - val_top5-acc: 0.9838 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 51s 577ms/step - loss: 0.3909 - acc: 0.8600 - top5-acc: 0.9968 - val_loss: 0.7413 - val_acc: 0.7618 - val_top5-acc: 0.9858 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.3588 - acc: 0.8703 - top5-acc: 0.9972 - val_loss: 0.7344 - val_acc: 0.7776 - val_top5-acc: 0.9850 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.3446 - acc: 0.8778 - top5-acc: 0.9974 - val_loss: 0.7320 - val_acc: 0.7730 - val_top5-acc: 0.9860 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.3441 - acc: 0.8764 - top5-acc: 0.9974 - val_loss: 0.7372 - val_acc: 0.7708 - val_top5-acc: 0.9852 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 51s 578ms/step - loss: 0.3400 - acc: 0.8796 - top5-acc: 0.9975 - val_loss: 0.7535 - val_acc: 0.7724 - val_top5-acc: 0.9858 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 51s 579ms/step - loss: 0.3333 - acc: 0.8817 - top5-acc: 0.9975 - val_loss: 0.7438 - val_acc: 0.7712 - val_top5-acc: 0.9868 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 40s 129ms/step - loss: 0.7463 - acc: 0.7660 - top5-acc: 0.9840\n",
      "Test accuracy: 76.6%\n",
      "Test top 5 accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_44_layer_call_fn, layer_normalization_44_layer_call_and_return_conditional_losses, layer_normalization_45_layer_call_fn, layer_normalization_45_layer_call_and_return_conditional_losses, layer_normalization_46_layer_call_fn while saving (showing 5 of 64). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_32ly_384Dc_2022-02-25_13-55\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/1A/mlpmixer_32ly_384Dc_2022-02-25_13-55\\assets\n"
     ]
    }
   ],
   "source": [
    "mlpmixer_iterations(num_patches,'1A', embedding_dim,num_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the only parameter that have to be initialized since, all the parameters are shared with Experiment 1A\n",
    "embedding_d = embedding_dim[0]  # Fixed Embedding Dimension from experiment 1A\n",
    "path_1B = 'Results_Article/1B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evol_accuracy(all_models,num_blocks):\n",
    "    total_plots=list()\n",
    "    for f in range(len(all_models)) :\n",
    "        testing_model = all_models[f]\n",
    "        partial_plots = list()\n",
    "        for j in range(num_blocks[f]):\n",
    "            #Define the Mixer Block that are going to participate (Cumulative Approach)\n",
    "            inter_input = testing_model.layers[4].layers[0].input\n",
    "            inter_output = testing_model.layers[4].layers[j].output\n",
    "            partial_models=tf.keras.models.Model(inputs=inter_input,outputs=inter_output, name = 'Mixer_Blocks')\n",
    "            #Create the structure of the model\n",
    "            inputs = layers.Input(shape=input_shape)\n",
    "            augmented = data_augmentation(inputs)\n",
    "            patches = Patches(patch_size, num_patches)(augmented)\n",
    "            x = testing_model.layers[3](patches)\n",
    "            intermediate_output  =  partial_models(x)\n",
    "            representation = layers.GlobalAveragePooling1D()(intermediate_output)\n",
    "            output =  layers.Dense(units=num_classes, activation='softmax')(representation) # Linear regression that is going to be trained\n",
    "            final_modelx =   keras.Model(inputs=inputs, outputs=output)\n",
    "            #Set the condition to not trainable\n",
    "            final_modelx.layers[3].trainable = False\n",
    "            final_modelx.layers[4].trainable = False\n",
    "            __,accuracy,__= run_experiment(final_modelx)\n",
    "            with open(path_1B + '/accuracy_Blocks_'+ str(num_blocks[f]) + '_L' + str(j+1)+ '.pkl','wb') as file:\n",
    "                        pickle.dump(accuracy,file)\n",
    "            partial_plots.append(accuracy)\n",
    "        total_plots.append(partial_plots)\n",
    "    return total_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the path in this cell (loading results from the experiment 1A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 12, 24, 32]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = list()\n",
    "for layer in num_blocks:\n",
    "    #Call the folder\n",
    "    pwd1 = 'Results_Article/1A/mlpmixer_'+ str(layer) + 'ly_' + str(embedding_d) + 'Dc_' + str(date) \n",
    "    layers_models = tf.keras.models.load_model(pwd1, compile=False)\n",
    "    all_models.append(layers_models)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 4s 34ms/step - loss: 2.9419 - acc: 0.2841 - top5-acc: 0.8091 - val_loss: 1.9108 - val_acc: 0.3454 - val_top5-acc: 0.8624 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.9402 - acc: 0.3399 - top5-acc: 0.8556 - val_loss: 1.8338 - val_acc: 0.3612 - val_top5-acc: 0.8728 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 4s 44ms/step - loss: 1.9416 - acc: 0.3448 - top5-acc: 0.8569 - val_loss: 1.8371 - val_acc: 0.3570 - val_top5-acc: 0.8792 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 3s 29ms/step - loss: 1.8768 - acc: 0.3588 - top5-acc: 0.8637 - val_loss: 1.8596 - val_acc: 0.3490 - val_top5-acc: 0.8890 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.8985 - acc: 0.3574 - top5-acc: 0.8661 - val_loss: 1.7749 - val_acc: 0.3740 - val_top5-acc: 0.8762 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.8479 - acc: 0.3671 - top5-acc: 0.8690 - val_loss: 1.7241 - val_acc: 0.4008 - val_top5-acc: 0.8876 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8325 - acc: 0.3707 - top5-acc: 0.8713 - val_loss: 1.7079 - val_acc: 0.3880 - val_top5-acc: 0.8858 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8187 - acc: 0.3762 - top5-acc: 0.8730 - val_loss: 1.7099 - val_acc: 0.3874 - val_top5-acc: 0.8834 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 1.8286 - acc: 0.3752 - top5-acc: 0.8704 - val_loss: 1.7015 - val_acc: 0.3848 - val_top5-acc: 0.8954 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 1.8106 - acc: 0.3731 - top5-acc: 0.8726 - val_loss: 1.6838 - val_acc: 0.4020 - val_top5-acc: 0.8886 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7927 - acc: 0.3782 - top5-acc: 0.8770 - val_loss: 1.7552 - val_acc: 0.3820 - val_top5-acc: 0.8922 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.7890 - acc: 0.3813 - top5-acc: 0.8761 - val_loss: 1.7036 - val_acc: 0.4006 - val_top5-acc: 0.8932 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.7900 - acc: 0.3821 - top5-acc: 0.8763 - val_loss: 1.7967 - val_acc: 0.3798 - val_top5-acc: 0.8772 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7830 - acc: 0.3872 - top5-acc: 0.8769 - val_loss: 1.8240 - val_acc: 0.3812 - val_top5-acc: 0.8740 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7882 - acc: 0.3861 - top5-acc: 0.8774 - val_loss: 1.7478 - val_acc: 0.3810 - val_top5-acc: 0.8832 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.6755 - acc: 0.4044 - top5-acc: 0.8876 - val_loss: 1.6100 - val_acc: 0.4194 - val_top5-acc: 0.9002 - lr: 0.0025\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6524 - acc: 0.4125 - top5-acc: 0.8930 - val_loss: 1.6529 - val_acc: 0.4014 - val_top5-acc: 0.9054 - lr: 0.0025\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6602 - acc: 0.4075 - top5-acc: 0.8903 - val_loss: 1.6196 - val_acc: 0.4150 - val_top5-acc: 0.8998 - lr: 0.0025\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.6575 - acc: 0.4092 - top5-acc: 0.8910 - val_loss: 1.6340 - val_acc: 0.4170 - val_top5-acc: 0.8982 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.6578 - acc: 0.4075 - top5-acc: 0.8909 - val_loss: 1.6044 - val_acc: 0.4280 - val_top5-acc: 0.9018 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 1.6615 - acc: 0.4101 - top5-acc: 0.8896 - val_loss: 1.5916 - val_acc: 0.4192 - val_top5-acc: 0.9024 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 1.6591 - acc: 0.4092 - top5-acc: 0.8913 - val_loss: 1.6212 - val_acc: 0.4168 - val_top5-acc: 0.9010 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 3s 40ms/step - loss: 1.6594 - acc: 0.4093 - top5-acc: 0.8928 - val_loss: 1.6083 - val_acc: 0.4290 - val_top5-acc: 0.8966 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 1.6499 - acc: 0.4131 - top5-acc: 0.8940 - val_loss: 1.5928 - val_acc: 0.4266 - val_top5-acc: 0.9020 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6593 - acc: 0.4089 - top5-acc: 0.8906 - val_loss: 1.5947 - val_acc: 0.4210 - val_top5-acc: 0.8996 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6645 - acc: 0.4066 - top5-acc: 0.8910 - val_loss: 1.6347 - val_acc: 0.4106 - val_top5-acc: 0.8940 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6107 - acc: 0.4247 - top5-acc: 0.8963 - val_loss: 1.5768 - val_acc: 0.4242 - val_top5-acc: 0.9046 - lr: 0.0012\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6061 - acc: 0.4242 - top5-acc: 0.8978 - val_loss: 1.5589 - val_acc: 0.4400 - val_top5-acc: 0.9022 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6046 - acc: 0.4251 - top5-acc: 0.8976 - val_loss: 1.5538 - val_acc: 0.4376 - val_top5-acc: 0.9086 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6051 - acc: 0.4269 - top5-acc: 0.8963 - val_loss: 1.5832 - val_acc: 0.4258 - val_top5-acc: 0.9058 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 3s 29ms/step - loss: 1.6056 - acc: 0.4253 - top5-acc: 0.8971 - val_loss: 1.5795 - val_acc: 0.4290 - val_top5-acc: 0.9056 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.6152 - acc: 0.4213 - top5-acc: 0.8943 - val_loss: 1.5795 - val_acc: 0.4316 - val_top5-acc: 0.9024 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "29/88 [========>.....................] - ETA: 21s - loss: 1.6111 - acc: 0.4254 - top5-acc: 0.9001"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_46392/284665497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtested_acc_evolution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevol_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_models\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_46392/26460535.py\u001b[0m in \u001b[0;36mevol_accuracy\u001b[1;34m(all_models, num_blocks)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mfinal_modelx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mfinal_modelx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0m__\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m__\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_modelx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_1B\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/accuracy_Blocks_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_L'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_46392/1087711688.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Fit the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     history = model.fit(\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1389\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1390\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \"\"\"\n\u001b[0;32m    437\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m       raise ValueError(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m       \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 914\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 914\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m       \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m     \u001b[1;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \"\"\"\n\u001b[0;32m   1222\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tested_acc_evolution = evol_accuracy(all_models,num_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 1: VERIFICATIONS (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change paths in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Results_Article/1A/mlpmixer_'+ str(layer) + 'ly_' + str(embedding_d) + 'Dc_' + str(date) \n",
    "#Call the folder\n",
    "tested_model = tf.keras.models.load_model(path)\n",
    "#Call the file\n",
    "tested_history=np.load( path + '/history_' + str(date) + '.npy',allow_pickle='TRUE').item()\n",
    "with open(path + '/accuracy.pkl','rb') as file:\n",
    "    tested_accuracy = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves(tested_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manually the values of your loaded model (change this parameters according to your previous selection)\n",
    "embedding_dim = 386\n",
    "num_blocks = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run separtely once to avoid randomness\n",
    "num_example = 39\n",
    "example_prepro = Preprocessing(num_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a MixerBlock and visualize the activation\n",
    "n_MixerLayer = 2\n",
    "\n",
    "bt_result = Mixer_Activations(tested_model,example_prepro)\n",
    "visualize_out(bt_result,n_MixerLayer,num_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heatmap of activations of the random sample\n",
    "sigma = 1\n",
    "type = 'kernel'\n",
    "######################################################\n",
    "bt_heatmap_CKA_ran = Heatmap(bt_result,type,sigma)\n",
    "visualize_Heatmap(bt_heatmap_CKA_ran,type,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run separtely once to avoid randomness \n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Average of layer's activation\n",
    "sigma = 1\n",
    "type = 'kernel'\n",
    "######################################################\n",
    "A1_ave_mixer_activations = Prom_Mixer_Activations_Blocks(tested_model,batch_prepro)\n",
    "A1_global_heatmap = Heatmap(A1_ave_mixer_activations,type,sigma)\n",
    "visualize_Heatmap(A1_global_heatmap,type,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this parameters\n",
    "accuracy_evolution_tested = tested_acc_evolution[0] # the index represents the position of num_blocks\n",
    "name = '_8L' # num_block[0] = 8 (layers)\n",
    "######################################################\n",
    "block = len(accuracy_evolution_tested)\n",
    "x = list(range(1,block+1))\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.plot(x,accuracy_evolution_tested,marker='o')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.locator_params(axis='x', nbins=block)\n",
    "plt.title('Evolution of Accuracy')\n",
    "plt.tight_layout()\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.savefig('Results_Article/1B/EvolutionAcc_' + name +'.png') \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_image_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
