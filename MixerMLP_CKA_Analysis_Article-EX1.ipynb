{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSzgroG_Suq"
   },
   "source": [
    "# Study of Image classification with modern MLP Mixer model and CKA\n",
    "\n",
    "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
    "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U087DdDw_Suy"
   },
   "source": [
    "# Setup for the MLP-Mixer Architecture\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AnTyoluw_Suz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import pickle\n",
    "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
    "from CKA_Google import *\n",
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Understand Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DAOrIHu_Su2"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1iMiVS7o_Su3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224 = 50176\n",
      "Patch size: 32 X 32 = 1024 \n",
      "Patches per image: 49\n",
      "Elements per patch (3 channels): 3072\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 512 \n",
    "num_epochs = 50\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Selected Architecture: B/32\n",
    "\n",
    "image_size = 224  # We'll resize input images to this size. Square\n",
    "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
    "embedding_dim = [384]  # Fixed Embedding Dimension\n",
    "num_blocks = [8,12,24,32]\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d_%H-%M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuu2OAS_Su0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Dataset for training \n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "#plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mpnEZH_Su5"
   },
   "source": [
    "## Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ra-bXojQ_Su6"
   },
   "outputs": [],
   "source": [
    "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data. \n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches. \n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks. ## (sequential_82)\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation) \n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpQFU8H__Su8"
   },
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9E1zD2BY_Su8"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        #Negative Log Likelihood = Categorical Cross Entropy\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping regularization callback. \n",
    "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
    "    #early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    #    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    #)\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history, accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxeE0cmM_Su9"
   },
   "source": [
    "## Use data augmentation\n",
    "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zh6hFPWX_Su-"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G77cUgiu_Su-"
   },
   "source": [
    "## Implement patch extraction as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYKNktXe_Su_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        #Extract the shape dimension in the position 0 = columns\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Without overlapping, stride horizontally and vertically\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
    "            rates=[1, 1, 1, 1],\n",
    "            #Patches contained in the images are considered, no zero padding\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        #shape[-1], number of colummns, as well as shape[0]\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Patches, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'patch_size' : self.patch_size ,\n",
    "            'num_patches' : self.num_patches\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yehcSS_Su_"
   },
   "source": [
    "## The MLP-Mixer model\n",
    "\n",
    "The MLP-Mixer is an architecture based exclusively on\n",
    "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "1. One applied independently to image patches, which mixes the per-location features.\n",
    "2. The other applied across patches (along channels), which mixes spatial information.\n",
    "\n",
    "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
    "instead of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwg4e2n_SvA"
   },
   "source": [
    "### Implement the MLP-Mixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dT6wVEki_SvA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp2 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super(MLPMixerLayer, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'num_patches' : num_patches,\n",
    "            'embedding_dim' : embedding_dim,\n",
    "            'dropout_rate' : dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUiufq92_SvA"
   },
   "source": [
    "## Build, train, and evaluate the MLP-Mixer model\n",
    "\n",
    "Note that training the model with the current settings on a V100 GPUs\n",
    "takes around 8 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report: Learning Curve\n",
    "def curves(history):\n",
    "    ymax1 = min(history[\"loss\"])\n",
    "    xmax1 = history[\"loss\"].index(ymax1)\n",
    "    ymax2 = min(history[\"val_loss\"])\n",
    "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
    "    plt.title(\"Cross Entropy Loss\")\n",
    "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
    "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Graph accuracy\n",
    "    ymax3 = max(history[\"acc\"])\n",
    "    xmax3 = history[\"acc\"].index(ymax3)\n",
    "    ymax4 = max(history[\"val_acc\"])\n",
    "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
    "    ymax5 = max(history[\"top5-acc\"])\n",
    "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
    "    ymax6 = max(history[\"val_top5-acc\"])\n",
    "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Classification accuracy')\n",
    "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Classification top5-acc')\n",
    "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.suptitle(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain activations + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers + Patches + One dense layer\n",
    "def Preprocessing(num_example):\n",
    "    augmented = data_augmentation(x_train[num_example])\n",
    "    b = Patches(patch_size, num_patches)(augmented)\n",
    "    a = layers.Dense(units=embedding_dim)(b)\n",
    "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
    "def Batch_Preprocessing(batch_size):\n",
    "    #Vector with the number of Sample of the Xtrain\n",
    "    a  = list(range(0,x_train.shape[0]))\n",
    "    b = random.sample(a,batch_size)\n",
    "    batch_regularization = list()\n",
    "    for i in range(0,batch_size):\n",
    "        inter_result = Preprocessing(b[i])\n",
    "        batch_regularization.append(inter_result)\n",
    "    return batch_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_out(result,layer_number,example):\n",
    "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "    ax1.imshow(x_train[example])\n",
    "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
    "    ax2.imshow(result[layer_number])\n",
    "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
    "    #The input is fixed to the beginning of the mlp blocks\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[0]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of layer's activation\n",
    "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
    "def Heatmap(result,type,sigma):\n",
    "    dim = len(result)\n",
    "    k = (dim - 1)\n",
    "    heatmap_CKA = np.zeros((dim,dim))\n",
    "    for i in range(0,dim):\n",
    "        tr = (dim - 1)\n",
    "        for j in range(0,dim):\n",
    "            if type == 'kernel':\n",
    "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
    "            elif type == 'linear':\n",
    "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
    "            else:\n",
    "                print('There is no such category, try again')\n",
    "                break\n",
    "\n",
    "            tr -= 1\n",
    "        k -= 1\n",
    "    #print('CKA' + type + 'calculated')\n",
    "    return heatmap_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of heatmaps (obsolet)\n",
    "def Prom_Mixer_Heatmaps(batch_result,type):\n",
    "    mat_heatmaps = list()\n",
    "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
    "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
    "        mat_heatmaps.append(heatmap_raw)\n",
    "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
    "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
    "    return prom_mixer_heatmap,mat_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 4\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i'%(i+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(3,3))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Understand Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A: Different Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create different mlpmixers according to an array of widths or depths\n",
    "def mlpmixer_iterations(num_patches,experiment,embedding_dim,num_blocks):\n",
    "    it_widths = len(embedding_dim)\n",
    "    it_blocks = len(num_blocks)\n",
    "    for j in range(it_widths):\n",
    "        for i in range(it_blocks):\n",
    "            mlpmixer_blocks = keras.Sequential(\n",
    "            [MLPMixerLayer(num_patches, embedding_dim[j], dropout_rate) for _ in range(num_blocks[i])] # creates the number of block without a \n",
    "            )\n",
    "            mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim[j]) # Returns the model\n",
    "            history,accuracy, top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "            #Saving Results\n",
    "            pwd = 'Results_Article/'+ str(experiment) +'/mlpmixer_'+ str(num_blocks[i]) + 'ly_' + str(embedding_dim[j]) + 'Dc'\n",
    "            mlpmixer_classifier.save(pwd)\n",
    "            np.save( pwd + '/history.npy',history.history)\n",
    "            with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "                pickle.dump(accuracy,file)\n",
    "            with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "                pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpmixer_iterations(num_patches,'1A', embedding_dim,num_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Average of layer's activation\n",
    "sigma = 1\n",
    "type = 'kernel'\n",
    "embedding_dim = 384\n",
    "blocks_total = num_blocks\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run separtely once to avoid randomness \n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000016F25074B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000016F25074B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000016E33DE7C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000016E33DE7C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    }
   ],
   "source": [
    "for item in blocks_total:\n",
    "    path = 'Results_Article/1A/mlpmixer_'+ str(item) +'ly_384Dc'\n",
    "    #Call the folder\n",
    "    tested_model = tf.keras.models.load_model(path)\n",
    "    num_blocks = item\n",
    "    A1_ave_mixer_activations = Prom_Mixer_Activations_Blocks(tested_model,batch_prepro)\n",
    "    A1_global_heatmap = Heatmap(A1_ave_mixer_activations,type,sigma)\n",
    "    with open(path + '/heatmap_'+ type + '_Sg'+ str(sigma) +'_'+ str(item)+'ly_384Dc.pkl','wb') as file:\n",
    "                pickle.dump(A1_global_heatmap,file)\n",
    "    with open(path + '/activations_'+ str(item)+'ly_384Dc.pkl','wb') as file:\n",
    "                pickle.dump(A1_ave_mixer_activations,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the only parameter that have to be initialized since, all the parameters are shared with Experiment 1A\n",
    "embedding_d = embedding_dim  # Fixed Embedding Dimension from experiment 1A\n",
    "path_1B = 'Results_Article/1B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evol_accuracy(all_models,num_blocks):\n",
    "    total_plots=list()\n",
    "    for f in range(len(all_models)) :\n",
    "        testing_model = all_models[f]\n",
    "        partial_plots = list()\n",
    "        for j in range(num_blocks[f]):\n",
    "            #Define the Mixer Block that are going to participate (Cumulative Approach)\n",
    "            inter_input = testing_model.layers[4].layers[0].input\n",
    "            inter_output = testing_model.layers[4].layers[j].output\n",
    "            partial_models=tf.keras.models.Model(inputs=inter_input,outputs=inter_output, name = 'Mixer_Blocks')\n",
    "            #Create the structure of the model\n",
    "            inputs = layers.Input(shape=input_shape)\n",
    "            augmented = data_augmentation(inputs)\n",
    "            patches = Patches(patch_size, num_patches)(augmented)\n",
    "            x = testing_model.layers[3](patches)\n",
    "            intermediate_output  =  partial_models(x)\n",
    "            representation = layers.GlobalAveragePooling1D()(intermediate_output)\n",
    "            output =  layers.Dense(units=num_classes, activation='softmax')(representation) # Linear regression that is going to be trained\n",
    "            final_modelx =   keras.Model(inputs=inputs, outputs=output)\n",
    "            #Set the condition to not trainable\n",
    "            final_modelx.layers[3].trainable = False\n",
    "            final_modelx.layers[4].trainable = False\n",
    "            __,accuracy,__= run_experiment(final_modelx)\n",
    "            with open(path_1B + '/accuracy_Blocks_'+ str(num_blocks[f]) + '_L' + str(j+1)+ '.pkl','wb') as file:\n",
    "                        pickle.dump(accuracy,file)\n",
    "            partial_plots.append(accuracy)\n",
    "        total_plots.append(partial_plots)\n",
    "    return total_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the path in this cell (loading results from the experiment 1A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = [8, 12, 24, 32]\n",
    "listnumblocks = num_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = list()\n",
    "for layer in listnumblocks:\n",
    "    #Call the folder\n",
    "    pwd1 = 'Results_Article/1A/mlpmixer_'+ str(layer) + 'ly_' + str(embedding_d) + 'Dc' \n",
    "    layers_models = tf.keras.models.load_model(pwd1, compile=False)\n",
    "    all_models.append(layers_models)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 4s 28ms/step - loss: 2.2886 - acc: 0.2588 - top5-acc: 0.7668 - val_loss: 1.9129 - val_acc: 0.3240 - val_top5-acc: 0.8260 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.9206 - acc: 0.3135 - top5-acc: 0.8257 - val_loss: 1.8709 - val_acc: 0.3250 - val_top5-acc: 0.8376 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.8769 - acc: 0.3305 - top5-acc: 0.8348 - val_loss: 1.8275 - val_acc: 0.3410 - val_top5-acc: 0.8550 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8550 - acc: 0.3365 - top5-acc: 0.8454 - val_loss: 1.7966 - val_acc: 0.3504 - val_top5-acc: 0.8610 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8292 - acc: 0.3457 - top5-acc: 0.8502 - val_loss: 1.7929 - val_acc: 0.3482 - val_top5-acc: 0.8594 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8219 - acc: 0.3447 - top5-acc: 0.8518 - val_loss: 1.7457 - val_acc: 0.3644 - val_top5-acc: 0.8634 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8106 - acc: 0.3514 - top5-acc: 0.8535 - val_loss: 1.7666 - val_acc: 0.3736 - val_top5-acc: 0.8674 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8120 - acc: 0.3546 - top5-acc: 0.8544 - val_loss: 1.7856 - val_acc: 0.3654 - val_top5-acc: 0.8650 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.7976 - acc: 0.3580 - top5-acc: 0.8599 - val_loss: 1.7456 - val_acc: 0.3722 - val_top5-acc: 0.8746 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.7882 - acc: 0.3593 - top5-acc: 0.8619 - val_loss: 1.8092 - val_acc: 0.3610 - val_top5-acc: 0.8646 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8000 - acc: 0.3583 - top5-acc: 0.8616 - val_loss: 1.7230 - val_acc: 0.3750 - val_top5-acc: 0.8754 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7923 - acc: 0.3635 - top5-acc: 0.8621 - val_loss: 1.7838 - val_acc: 0.3640 - val_top5-acc: 0.8756 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.7698 - acc: 0.3676 - top5-acc: 0.8636 - val_loss: 1.7047 - val_acc: 0.3890 - val_top5-acc: 0.8780 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7796 - acc: 0.3643 - top5-acc: 0.8644 - val_loss: 1.7332 - val_acc: 0.3706 - val_top5-acc: 0.8814 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7686 - acc: 0.3625 - top5-acc: 0.8678 - val_loss: 1.7723 - val_acc: 0.3644 - val_top5-acc: 0.8754 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7587 - acc: 0.3685 - top5-acc: 0.8688 - val_loss: 1.7354 - val_acc: 0.3662 - val_top5-acc: 0.8704 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7656 - acc: 0.3684 - top5-acc: 0.8676 - val_loss: 1.7386 - val_acc: 0.3762 - val_top5-acc: 0.8740 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7619 - acc: 0.3694 - top5-acc: 0.8692 - val_loss: 1.7279 - val_acc: 0.3786 - val_top5-acc: 0.8784 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7107 - acc: 0.3862 - top5-acc: 0.8756 - val_loss: 1.6693 - val_acc: 0.4028 - val_top5-acc: 0.8810 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7015 - acc: 0.3871 - top5-acc: 0.8786 - val_loss: 1.7119 - val_acc: 0.3836 - val_top5-acc: 0.8780 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7037 - acc: 0.3870 - top5-acc: 0.8754 - val_loss: 1.6636 - val_acc: 0.4044 - val_top5-acc: 0.8852 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7024 - acc: 0.3862 - top5-acc: 0.8762 - val_loss: 1.6457 - val_acc: 0.4124 - val_top5-acc: 0.8882 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7120 - acc: 0.3827 - top5-acc: 0.8758 - val_loss: 1.6646 - val_acc: 0.3996 - val_top5-acc: 0.8776 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7049 - acc: 0.3893 - top5-acc: 0.8756 - val_loss: 1.6862 - val_acc: 0.3914 - val_top5-acc: 0.8776 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6987 - acc: 0.3880 - top5-acc: 0.8773 - val_loss: 1.6673 - val_acc: 0.3966 - val_top5-acc: 0.8826 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7032 - acc: 0.3862 - top5-acc: 0.8763 - val_loss: 1.6803 - val_acc: 0.3924 - val_top5-acc: 0.8794 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7055 - acc: 0.3877 - top5-acc: 0.8768 - val_loss: 1.6544 - val_acc: 0.3996 - val_top5-acc: 0.8900 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6762 - acc: 0.3974 - top5-acc: 0.8807 - val_loss: 1.6457 - val_acc: 0.4084 - val_top5-acc: 0.8878 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6745 - acc: 0.4016 - top5-acc: 0.8800 - val_loss: 1.6367 - val_acc: 0.4150 - val_top5-acc: 0.8882 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6752 - acc: 0.3966 - top5-acc: 0.8812 - val_loss: 1.6286 - val_acc: 0.4198 - val_top5-acc: 0.8890 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6751 - acc: 0.3995 - top5-acc: 0.8829 - val_loss: 1.6435 - val_acc: 0.4066 - val_top5-acc: 0.8874 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6752 - acc: 0.4000 - top5-acc: 0.8804 - val_loss: 1.6333 - val_acc: 0.4100 - val_top5-acc: 0.8878 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6768 - acc: 0.3986 - top5-acc: 0.8798 - val_loss: 1.6448 - val_acc: 0.4092 - val_top5-acc: 0.8924 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6759 - acc: 0.3973 - top5-acc: 0.8829 - val_loss: 1.6629 - val_acc: 0.4032 - val_top5-acc: 0.8878 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6787 - acc: 0.3991 - top5-acc: 0.8807 - val_loss: 1.6428 - val_acc: 0.4056 - val_top5-acc: 0.8870 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6672 - acc: 0.4035 - top5-acc: 0.8819 - val_loss: 1.6328 - val_acc: 0.4160 - val_top5-acc: 0.8876 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6653 - acc: 0.4033 - top5-acc: 0.8816 - val_loss: 1.6325 - val_acc: 0.4172 - val_top5-acc: 0.8912 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6655 - acc: 0.4042 - top5-acc: 0.8829 - val_loss: 1.6228 - val_acc: 0.4202 - val_top5-acc: 0.8884 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6681 - acc: 0.3995 - top5-acc: 0.8829 - val_loss: 1.6360 - val_acc: 0.4148 - val_top5-acc: 0.8866 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6649 - acc: 0.4023 - top5-acc: 0.8836 - val_loss: 1.6292 - val_acc: 0.4148 - val_top5-acc: 0.8892 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6661 - acc: 0.4002 - top5-acc: 0.8843 - val_loss: 1.6279 - val_acc: 0.4158 - val_top5-acc: 0.8918 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6686 - acc: 0.4037 - top5-acc: 0.8816 - val_loss: 1.6421 - val_acc: 0.4120 - val_top5-acc: 0.8872 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6672 - acc: 0.4020 - top5-acc: 0.8843 - val_loss: 1.6299 - val_acc: 0.4226 - val_top5-acc: 0.8902 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6561 - acc: 0.4093 - top5-acc: 0.8858 - val_loss: 1.6242 - val_acc: 0.4218 - val_top5-acc: 0.8880 - lr: 3.1250e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6605 - acc: 0.4067 - top5-acc: 0.8855 - val_loss: 1.6249 - val_acc: 0.4230 - val_top5-acc: 0.8900 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6591 - acc: 0.4090 - top5-acc: 0.8852 - val_loss: 1.6284 - val_acc: 0.4174 - val_top5-acc: 0.8876 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6617 - acc: 0.4095 - top5-acc: 0.8836 - val_loss: 1.6288 - val_acc: 0.4210 - val_top5-acc: 0.8890 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6622 - acc: 0.4079 - top5-acc: 0.8853 - val_loss: 1.6253 - val_acc: 0.4158 - val_top5-acc: 0.8922 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6576 - acc: 0.4097 - top5-acc: 0.8840 - val_loss: 1.6253 - val_acc: 0.4260 - val_top5-acc: 0.8886 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6590 - acc: 0.4103 - top5-acc: 0.8845 - val_loss: 1.6269 - val_acc: 0.4198 - val_top5-acc: 0.8886 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.6386 - acc: 0.4201 - top5-acc: 0.8932\n",
      "Test accuracy: 42.01%\n",
      "Test top 5 accuracy: 89.32%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 4s 39ms/step - loss: 2.0923 - acc: 0.3009 - top5-acc: 0.8180 - val_loss: 1.7767 - val_acc: 0.3488 - val_top5-acc: 0.8672 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.7736 - acc: 0.3591 - top5-acc: 0.8700 - val_loss: 1.7071 - val_acc: 0.3760 - val_top5-acc: 0.8816 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.7312 - acc: 0.3740 - top5-acc: 0.8763 - val_loss: 1.7268 - val_acc: 0.3780 - val_top5-acc: 0.8852 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.7392 - acc: 0.3740 - top5-acc: 0.8750 - val_loss: 1.6730 - val_acc: 0.3854 - val_top5-acc: 0.8882 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.7331 - acc: 0.3775 - top5-acc: 0.8808 - val_loss: 1.6512 - val_acc: 0.4000 - val_top5-acc: 0.8908 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 1.7043 - acc: 0.3841 - top5-acc: 0.8840 - val_loss: 1.6281 - val_acc: 0.4032 - val_top5-acc: 0.8956 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.7021 - acc: 0.3883 - top5-acc: 0.8840 - val_loss: 1.7000 - val_acc: 0.3870 - val_top5-acc: 0.8874 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.6929 - acc: 0.3907 - top5-acc: 0.8856 - val_loss: 1.6808 - val_acc: 0.3934 - val_top5-acc: 0.8916 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.7045 - acc: 0.3885 - top5-acc: 0.8852 - val_loss: 1.6172 - val_acc: 0.4090 - val_top5-acc: 0.8956 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 1.6819 - acc: 0.3941 - top5-acc: 0.8878 - val_loss: 1.6425 - val_acc: 0.4000 - val_top5-acc: 0.8994 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 1.6874 - acc: 0.3951 - top5-acc: 0.8868 - val_loss: 1.5839 - val_acc: 0.4198 - val_top5-acc: 0.9038 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 3s 40ms/step - loss: 1.6822 - acc: 0.3982 - top5-acc: 0.8873 - val_loss: 1.6066 - val_acc: 0.4212 - val_top5-acc: 0.9024 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 1.6780 - acc: 0.3975 - top5-acc: 0.8888 - val_loss: 1.5979 - val_acc: 0.4236 - val_top5-acc: 0.9014 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 1.6721 - acc: 0.3977 - top5-acc: 0.8899 - val_loss: 1.6308 - val_acc: 0.4090 - val_top5-acc: 0.8976 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 1.6580 - acc: 0.4046 - top5-acc: 0.8918 - val_loss: 1.6216 - val_acc: 0.4076 - val_top5-acc: 0.8962 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 1.6841 - acc: 0.3946 - top5-acc: 0.8871 - val_loss: 1.6665 - val_acc: 0.4098 - val_top5-acc: 0.8926 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 1.6262 - acc: 0.4145 - top5-acc: 0.8949 - val_loss: 1.5528 - val_acc: 0.4356 - val_top5-acc: 0.9094 - lr: 0.0025\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 1.6088 - acc: 0.4202 - top5-acc: 0.8964 - val_loss: 1.5812 - val_acc: 0.4372 - val_top5-acc: 0.8996 - lr: 0.0025\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.6141 - acc: 0.4229 - top5-acc: 0.8957 - val_loss: 1.5631 - val_acc: 0.4308 - val_top5-acc: 0.9070 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 1.6093 - acc: 0.4237 - top5-acc: 0.8976 - val_loss: 1.5664 - val_acc: 0.4354 - val_top5-acc: 0.9050 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 1.6142 - acc: 0.4186 - top5-acc: 0.8964 - val_loss: 1.5643 - val_acc: 0.4392 - val_top5-acc: 0.9038 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 1.6167 - acc: 0.4178 - top5-acc: 0.8964 - val_loss: 1.5520 - val_acc: 0.4404 - val_top5-acc: 0.9066 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 1.6080 - acc: 0.4214 - top5-acc: 0.8970 - val_loss: 1.5674 - val_acc: 0.4460 - val_top5-acc: 0.9012 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 1.6131 - acc: 0.4208 - top5-acc: 0.8984 - val_loss: 1.5510 - val_acc: 0.4468 - val_top5-acc: 0.9048 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 1.6024 - acc: 0.4246 - top5-acc: 0.8971 - val_loss: 1.5603 - val_acc: 0.4400 - val_top5-acc: 0.9094 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 1.6046 - acc: 0.4204 - top5-acc: 0.8986 - val_loss: 1.5817 - val_acc: 0.4210 - val_top5-acc: 0.9016 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 1.6086 - acc: 0.4230 - top5-acc: 0.8972 - val_loss: 1.5586 - val_acc: 0.4458 - val_top5-acc: 0.9032 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 1.6182 - acc: 0.4192 - top5-acc: 0.8964 - val_loss: 1.5641 - val_acc: 0.4376 - val_top5-acc: 0.9074 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 1.6127 - acc: 0.4230 - top5-acc: 0.8961 - val_loss: 1.5699 - val_acc: 0.4378 - val_top5-acc: 0.9020 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 1.5843 - acc: 0.4312 - top5-acc: 0.9005 - val_loss: 1.5406 - val_acc: 0.4482 - val_top5-acc: 0.9046 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5849 - acc: 0.4326 - top5-acc: 0.8993 - val_loss: 1.5439 - val_acc: 0.4470 - val_top5-acc: 0.9084 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5823 - acc: 0.4351 - top5-acc: 0.9003 - val_loss: 1.5414 - val_acc: 0.4464 - val_top5-acc: 0.9056 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5838 - acc: 0.4339 - top5-acc: 0.8995 - val_loss: 1.5407 - val_acc: 0.4454 - val_top5-acc: 0.9080 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5834 - acc: 0.4305 - top5-acc: 0.9002 - val_loss: 1.5443 - val_acc: 0.4526 - val_top5-acc: 0.9072 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5844 - acc: 0.4317 - top5-acc: 0.9008 - val_loss: 1.5331 - val_acc: 0.4486 - val_top5-acc: 0.9090 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5873 - acc: 0.4305 - top5-acc: 0.9007 - val_loss: 1.5350 - val_acc: 0.4496 - val_top5-acc: 0.9056 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5842 - acc: 0.4338 - top5-acc: 0.8986 - val_loss: 1.5523 - val_acc: 0.4464 - val_top5-acc: 0.9018 - lr: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5890 - acc: 0.4302 - top5-acc: 0.9006 - val_loss: 1.5390 - val_acc: 0.4444 - val_top5-acc: 0.9084 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 1.5894 - acc: 0.4306 - top5-acc: 0.8999 - val_loss: 1.5344 - val_acc: 0.4484 - val_top5-acc: 0.9076 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 1.5811 - acc: 0.4324 - top5-acc: 0.9015 - val_loss: 1.5579 - val_acc: 0.4376 - val_top5-acc: 0.9052 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 1.5752 - acc: 0.4348 - top5-acc: 0.9000 - val_loss: 1.5332 - val_acc: 0.4436 - val_top5-acc: 0.9074 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 1.5716 - acc: 0.4387 - top5-acc: 0.9009 - val_loss: 1.5351 - val_acc: 0.4472 - val_top5-acc: 0.9072 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 1.5672 - acc: 0.4389 - top5-acc: 0.9031 - val_loss: 1.5299 - val_acc: 0.4574 - val_top5-acc: 0.9112 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 1.5743 - acc: 0.4365 - top5-acc: 0.9022 - val_loss: 1.5358 - val_acc: 0.4506 - val_top5-acc: 0.9062 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5716 - acc: 0.4380 - top5-acc: 0.9010 - val_loss: 1.5296 - val_acc: 0.4494 - val_top5-acc: 0.9098 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5732 - acc: 0.4358 - top5-acc: 0.9008 - val_loss: 1.5322 - val_acc: 0.4526 - val_top5-acc: 0.9066 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5749 - acc: 0.4377 - top5-acc: 0.9012 - val_loss: 1.5299 - val_acc: 0.4522 - val_top5-acc: 0.9106 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5704 - acc: 0.4402 - top5-acc: 0.9025 - val_loss: 1.5266 - val_acc: 0.4498 - val_top5-acc: 0.9088 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5740 - acc: 0.4381 - top5-acc: 0.9011 - val_loss: 1.5379 - val_acc: 0.4440 - val_top5-acc: 0.9062 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 1.5798 - acc: 0.4352 - top5-acc: 0.9006 - val_loss: 1.5361 - val_acc: 0.4470 - val_top5-acc: 0.9088 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 1.5510 - acc: 0.4457 - top5-acc: 0.9066\n",
      "Test accuracy: 44.57%\n",
      "Test top 5 accuracy: 90.66%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 5s 46ms/step - loss: 1.9914 - acc: 0.3192 - top5-acc: 0.8228 - val_loss: 1.6973 - val_acc: 0.3834 - val_top5-acc: 0.8892 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.7331 - acc: 0.3805 - top5-acc: 0.8785 - val_loss: 1.6478 - val_acc: 0.4106 - val_top5-acc: 0.8942 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.6878 - acc: 0.3965 - top5-acc: 0.8860 - val_loss: 1.5997 - val_acc: 0.4242 - val_top5-acc: 0.9010 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.6650 - acc: 0.4033 - top5-acc: 0.8906 - val_loss: 1.5832 - val_acc: 0.4302 - val_top5-acc: 0.9078 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.6545 - acc: 0.4062 - top5-acc: 0.8928 - val_loss: 1.5961 - val_acc: 0.4218 - val_top5-acc: 0.9044 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 4s 43ms/step - loss: 1.6292 - acc: 0.4136 - top5-acc: 0.8958 - val_loss: 1.6210 - val_acc: 0.4194 - val_top5-acc: 0.9056 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 4s 44ms/step - loss: 1.6356 - acc: 0.4172 - top5-acc: 0.8967 - val_loss: 1.6211 - val_acc: 0.4154 - val_top5-acc: 0.9090 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 4s 44ms/step - loss: 1.6097 - acc: 0.4239 - top5-acc: 0.8997 - val_loss: 1.5756 - val_acc: 0.4380 - val_top5-acc: 0.9094 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 1.6145 - acc: 0.4242 - top5-acc: 0.8997 - val_loss: 1.5391 - val_acc: 0.4534 - val_top5-acc: 0.9108 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 4s 46ms/step - loss: 1.5981 - acc: 0.4288 - top5-acc: 0.9012 - val_loss: 1.5412 - val_acc: 0.4492 - val_top5-acc: 0.9138 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 1.5890 - acc: 0.4308 - top5-acc: 0.9023 - val_loss: 1.5463 - val_acc: 0.4402 - val_top5-acc: 0.9142 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 1.5914 - acc: 0.4340 - top5-acc: 0.9012 - val_loss: 1.5198 - val_acc: 0.4474 - val_top5-acc: 0.9178 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 1.5926 - acc: 0.4324 - top5-acc: 0.9020 - val_loss: 1.5320 - val_acc: 0.4496 - val_top5-acc: 0.9108 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 4s 46ms/step - loss: 1.5756 - acc: 0.4409 - top5-acc: 0.9044 - val_loss: 1.5194 - val_acc: 0.4608 - val_top5-acc: 0.9166 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 4s 48ms/step - loss: 1.5961 - acc: 0.4324 - top5-acc: 0.9044 - val_loss: 1.5952 - val_acc: 0.4352 - val_top5-acc: 0.9096 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5878 - acc: 0.4351 - top5-acc: 0.9047 - val_loss: 1.5159 - val_acc: 0.4552 - val_top5-acc: 0.9162 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5567 - acc: 0.4455 - top5-acc: 0.9060 - val_loss: 1.5382 - val_acc: 0.4582 - val_top5-acc: 0.9142 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5689 - acc: 0.4428 - top5-acc: 0.9072 - val_loss: 1.5536 - val_acc: 0.4436 - val_top5-acc: 0.9130 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5702 - acc: 0.4391 - top5-acc: 0.9052 - val_loss: 1.5114 - val_acc: 0.4602 - val_top5-acc: 0.9198 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 4s 41ms/step - loss: 1.5579 - acc: 0.4458 - top5-acc: 0.9059 - val_loss: 1.4992 - val_acc: 0.4568 - val_top5-acc: 0.9246 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5598 - acc: 0.4442 - top5-acc: 0.9075 - val_loss: 1.5027 - val_acc: 0.4484 - val_top5-acc: 0.9226 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5622 - acc: 0.4414 - top5-acc: 0.9079 - val_loss: 1.5520 - val_acc: 0.4468 - val_top5-acc: 0.9114 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5589 - acc: 0.4456 - top5-acc: 0.9074 - val_loss: 1.4671 - val_acc: 0.4752 - val_top5-acc: 0.9204 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5591 - acc: 0.4447 - top5-acc: 0.9085 - val_loss: 1.4982 - val_acc: 0.4628 - val_top5-acc: 0.9196 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5536 - acc: 0.4466 - top5-acc: 0.9091 - val_loss: 1.5194 - val_acc: 0.4524 - val_top5-acc: 0.9218 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5592 - acc: 0.4442 - top5-acc: 0.9082 - val_loss: 1.4823 - val_acc: 0.4670 - val_top5-acc: 0.9238 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5659 - acc: 0.4439 - top5-acc: 0.9064 - val_loss: 1.4958 - val_acc: 0.4676 - val_top5-acc: 0.9150 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5568 - acc: 0.4488 - top5-acc: 0.9082 - val_loss: 1.4788 - val_acc: 0.4702 - val_top5-acc: 0.9222 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5111 - acc: 0.4598 - top5-acc: 0.9141 - val_loss: 1.4367 - val_acc: 0.4882 - val_top5-acc: 0.9220 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5110 - acc: 0.4601 - top5-acc: 0.9145 - val_loss: 1.4528 - val_acc: 0.4790 - val_top5-acc: 0.9250 - lr: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5091 - acc: 0.4624 - top5-acc: 0.9145 - val_loss: 1.4433 - val_acc: 0.4794 - val_top5-acc: 0.9252 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5128 - acc: 0.4616 - top5-acc: 0.9130 - val_loss: 1.4769 - val_acc: 0.4670 - val_top5-acc: 0.9282 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5107 - acc: 0.4606 - top5-acc: 0.9122 - val_loss: 1.4508 - val_acc: 0.4824 - val_top5-acc: 0.9278 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.5067 - acc: 0.4605 - top5-acc: 0.9153 - val_loss: 1.4644 - val_acc: 0.4786 - val_top5-acc: 0.9220 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4892 - acc: 0.4684 - top5-acc: 0.9156 - val_loss: 1.4261 - val_acc: 0.4972 - val_top5-acc: 0.9256 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4899 - acc: 0.4665 - top5-acc: 0.9175 - val_loss: 1.4392 - val_acc: 0.4856 - val_top5-acc: 0.9246 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4821 - acc: 0.4693 - top5-acc: 0.9168 - val_loss: 1.4368 - val_acc: 0.4934 - val_top5-acc: 0.9278 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4856 - acc: 0.4697 - top5-acc: 0.9165 - val_loss: 1.4301 - val_acc: 0.4880 - val_top5-acc: 0.9274 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4849 - acc: 0.4723 - top5-acc: 0.9165 - val_loss: 1.4356 - val_acc: 0.4952 - val_top5-acc: 0.9246 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4903 - acc: 0.4683 - top5-acc: 0.9165 - val_loss: 1.4360 - val_acc: 0.4920 - val_top5-acc: 0.9268 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4798 - acc: 0.4733 - top5-acc: 0.9169 - val_loss: 1.4250 - val_acc: 0.4900 - val_top5-acc: 0.9258 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4782 - acc: 0.4743 - top5-acc: 0.9175 - val_loss: 1.4244 - val_acc: 0.4950 - val_top5-acc: 0.9248 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4748 - acc: 0.4774 - top5-acc: 0.9187 - val_loss: 1.4218 - val_acc: 0.4982 - val_top5-acc: 0.9284 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4790 - acc: 0.4740 - top5-acc: 0.9180 - val_loss: 1.4268 - val_acc: 0.4974 - val_top5-acc: 0.9278 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4793 - acc: 0.4762 - top5-acc: 0.9173 - val_loss: 1.4350 - val_acc: 0.4888 - val_top5-acc: 0.9244 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4813 - acc: 0.4725 - top5-acc: 0.9161 - val_loss: 1.4292 - val_acc: 0.4926 - val_top5-acc: 0.9282 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4799 - acc: 0.4741 - top5-acc: 0.9170 - val_loss: 1.4303 - val_acc: 0.4880 - val_top5-acc: 0.9260 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4815 - acc: 0.4764 - top5-acc: 0.9164 - val_loss: 1.4246 - val_acc: 0.4974 - val_top5-acc: 0.9270 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 1.4723 - acc: 0.4777 - top5-acc: 0.9176 - val_loss: 1.4273 - val_acc: 0.4988 - val_top5-acc: 0.9274 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 4s 44ms/step - loss: 1.4743 - acc: 0.4762 - top5-acc: 0.9193 - val_loss: 1.4238 - val_acc: 0.4966 - val_top5-acc: 0.9264 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.4418 - acc: 0.4939 - top5-acc: 0.9241\n",
      "Test accuracy: 49.39%\n",
      "Test top 5 accuracy: 92.41%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 6s 57ms/step - loss: 1.9493 - acc: 0.3318 - top5-acc: 0.8394 - val_loss: 1.6391 - val_acc: 0.4088 - val_top5-acc: 0.8960 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.6818 - acc: 0.3934 - top5-acc: 0.8897 - val_loss: 1.6159 - val_acc: 0.4164 - val_top5-acc: 0.9034 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.6479 - acc: 0.4083 - top5-acc: 0.8965 - val_loss: 1.5666 - val_acc: 0.4378 - val_top5-acc: 0.9070 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 1.6190 - acc: 0.4208 - top5-acc: 0.8984 - val_loss: 1.5712 - val_acc: 0.4322 - val_top5-acc: 0.9060 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5952 - acc: 0.4286 - top5-acc: 0.9039 - val_loss: 1.5246 - val_acc: 0.4466 - val_top5-acc: 0.9174 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5782 - acc: 0.4354 - top5-acc: 0.9060 - val_loss: 1.5765 - val_acc: 0.4288 - val_top5-acc: 0.9096 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.5845 - acc: 0.4361 - top5-acc: 0.9056 - val_loss: 1.5276 - val_acc: 0.4444 - val_top5-acc: 0.9170 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5623 - acc: 0.4428 - top5-acc: 0.9092 - val_loss: 1.5060 - val_acc: 0.4580 - val_top5-acc: 0.9186 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5606 - acc: 0.4439 - top5-acc: 0.9075 - val_loss: 1.5578 - val_acc: 0.4406 - val_top5-acc: 0.9124 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 1.5556 - acc: 0.4451 - top5-acc: 0.9084 - val_loss: 1.4911 - val_acc: 0.4578 - val_top5-acc: 0.9256 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5370 - acc: 0.4504 - top5-acc: 0.9118 - val_loss: 1.4793 - val_acc: 0.4626 - val_top5-acc: 0.9264 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5400 - acc: 0.4499 - top5-acc: 0.9118 - val_loss: 1.5095 - val_acc: 0.4598 - val_top5-acc: 0.9194 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5307 - acc: 0.4563 - top5-acc: 0.9130 - val_loss: 1.4945 - val_acc: 0.4628 - val_top5-acc: 0.9228 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.5313 - acc: 0.4547 - top5-acc: 0.9133 - val_loss: 1.4673 - val_acc: 0.4704 - val_top5-acc: 0.9284 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 5s 56ms/step - loss: 1.5284 - acc: 0.4566 - top5-acc: 0.9137 - val_loss: 1.4559 - val_acc: 0.4760 - val_top5-acc: 0.9278 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 5s 57ms/step - loss: 1.5335 - acc: 0.4507 - top5-acc: 0.9130 - val_loss: 1.4525 - val_acc: 0.4736 - val_top5-acc: 0.9300 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 5s 59ms/step - loss: 1.5171 - acc: 0.4583 - top5-acc: 0.9138 - val_loss: 1.4420 - val_acc: 0.4718 - val_top5-acc: 0.9258 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 5s 56ms/step - loss: 1.5172 - acc: 0.4589 - top5-acc: 0.9156 - val_loss: 1.4694 - val_acc: 0.4714 - val_top5-acc: 0.9262 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 1.5231 - acc: 0.4589 - top5-acc: 0.9143 - val_loss: 1.4948 - val_acc: 0.4708 - val_top5-acc: 0.9206 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.5280 - acc: 0.4563 - top5-acc: 0.9148 - val_loss: 1.4923 - val_acc: 0.4660 - val_top5-acc: 0.9238 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.5094 - acc: 0.4625 - top5-acc: 0.9172 - val_loss: 1.4349 - val_acc: 0.4842 - val_top5-acc: 0.9284 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 1.5043 - acc: 0.4640 - top5-acc: 0.9161 - val_loss: 1.4616 - val_acc: 0.4770 - val_top5-acc: 0.9232 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 1.5197 - acc: 0.4620 - top5-acc: 0.9163 - val_loss: 1.4384 - val_acc: 0.4826 - val_top5-acc: 0.9282 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.5067 - acc: 0.4613 - top5-acc: 0.9174 - val_loss: 1.4570 - val_acc: 0.4844 - val_top5-acc: 0.9260 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.5122 - acc: 0.4619 - top5-acc: 0.9160 - val_loss: 1.4423 - val_acc: 0.4806 - val_top5-acc: 0.9290 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.5146 - acc: 0.4592 - top5-acc: 0.9162 - val_loss: 1.4594 - val_acc: 0.4788 - val_top5-acc: 0.9258 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4654 - acc: 0.4776 - top5-acc: 0.9212 - val_loss: 1.4140 - val_acc: 0.4898 - val_top5-acc: 0.9336 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 5s 56ms/step - loss: 1.4614 - acc: 0.4771 - top5-acc: 0.9219 - val_loss: 1.4475 - val_acc: 0.4758 - val_top5-acc: 0.9254 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.4674 - acc: 0.4771 - top5-acc: 0.9215 - val_loss: 1.4164 - val_acc: 0.4988 - val_top5-acc: 0.9328 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 1.4603 - acc: 0.4806 - top5-acc: 0.9208 - val_loss: 1.4153 - val_acc: 0.4906 - val_top5-acc: 0.9298 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.4601 - acc: 0.4786 - top5-acc: 0.9203 - val_loss: 1.4099 - val_acc: 0.4882 - val_top5-acc: 0.9334 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 5s 59ms/step - loss: 1.4693 - acc: 0.4765 - top5-acc: 0.9219 - val_loss: 1.4301 - val_acc: 0.4876 - val_top5-acc: 0.9316 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 5s 60ms/step - loss: 1.4643 - acc: 0.4756 - top5-acc: 0.9217 - val_loss: 1.4050 - val_acc: 0.4968 - val_top5-acc: 0.9318 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.4662 - acc: 0.4784 - top5-acc: 0.9214 - val_loss: 1.4028 - val_acc: 0.4984 - val_top5-acc: 0.9340 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 1.4647 - acc: 0.4745 - top5-acc: 0.9222 - val_loss: 1.4108 - val_acc: 0.4908 - val_top5-acc: 0.9324 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 1.4626 - acc: 0.4786 - top5-acc: 0.9218 - val_loss: 1.4154 - val_acc: 0.4962 - val_top5-acc: 0.9282 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 5s 57ms/step - loss: 1.4624 - acc: 0.4795 - top5-acc: 0.9221 - val_loss: 1.4091 - val_acc: 0.4948 - val_top5-acc: 0.9342 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.4655 - acc: 0.4763 - top5-acc: 0.9213 - val_loss: 1.3968 - val_acc: 0.5070 - val_top5-acc: 0.9336 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.4599 - acc: 0.4787 - top5-acc: 0.9217 - val_loss: 1.3940 - val_acc: 0.4996 - val_top5-acc: 0.9324 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 1.4626 - acc: 0.4782 - top5-acc: 0.9208 - val_loss: 1.3915 - val_acc: 0.4966 - val_top5-acc: 0.9370 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 1.4650 - acc: 0.4804 - top5-acc: 0.9214 - val_loss: 1.4055 - val_acc: 0.5034 - val_top5-acc: 0.9324 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 1.4705 - acc: 0.4757 - top5-acc: 0.9208 - val_loss: 1.4116 - val_acc: 0.4954 - val_top5-acc: 0.9326 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 5s 51ms/step - loss: 1.4601 - acc: 0.4759 - top5-acc: 0.9212 - val_loss: 1.4159 - val_acc: 0.4840 - val_top5-acc: 0.9310 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.4604 - acc: 0.4795 - top5-acc: 0.9215 - val_loss: 1.3967 - val_acc: 0.4964 - val_top5-acc: 0.9314 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.4612 - acc: 0.4778 - top5-acc: 0.9230 - val_loss: 1.3995 - val_acc: 0.5022 - val_top5-acc: 0.9316 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 5s 51ms/step - loss: 1.4420 - acc: 0.4869 - top5-acc: 0.9241 - val_loss: 1.3845 - val_acc: 0.5060 - val_top5-acc: 0.9334 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.4426 - acc: 0.4855 - top5-acc: 0.9235 - val_loss: 1.3802 - val_acc: 0.5108 - val_top5-acc: 0.9332 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 1.4439 - acc: 0.4866 - top5-acc: 0.9234 - val_loss: 1.4025 - val_acc: 0.4930 - val_top5-acc: 0.9330 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 1.4446 - acc: 0.4870 - top5-acc: 0.9236 - val_loss: 1.3945 - val_acc: 0.4972 - val_top5-acc: 0.9332 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 1.4410 - acc: 0.4867 - top5-acc: 0.9245 - val_loss: 1.3927 - val_acc: 0.5012 - val_top5-acc: 0.9340 - lr: 0.0012\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.4044 - acc: 0.5050 - top5-acc: 0.9308\n",
      "Test accuracy: 50.5%\n",
      "Test top 5 accuracy: 93.08%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 8s 69ms/step - loss: 1.9081 - acc: 0.3394 - top5-acc: 0.8431 - val_loss: 1.6018 - val_acc: 0.4214 - val_top5-acc: 0.9032 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 1.6260 - acc: 0.4157 - top5-acc: 0.8972 - val_loss: 1.5752 - val_acc: 0.4390 - val_top5-acc: 0.9080 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.5744 - acc: 0.4377 - top5-acc: 0.9076 - val_loss: 1.4909 - val_acc: 0.4668 - val_top5-acc: 0.9196 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5515 - acc: 0.4425 - top5-acc: 0.9103 - val_loss: 1.4679 - val_acc: 0.4774 - val_top5-acc: 0.9246 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5331 - acc: 0.4512 - top5-acc: 0.9142 - val_loss: 1.4683 - val_acc: 0.4720 - val_top5-acc: 0.9256 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5180 - acc: 0.4582 - top5-acc: 0.9163 - val_loss: 1.4488 - val_acc: 0.4868 - val_top5-acc: 0.9290 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.5078 - acc: 0.4596 - top5-acc: 0.9161 - val_loss: 1.4356 - val_acc: 0.4912 - val_top5-acc: 0.9288 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.4871 - acc: 0.4675 - top5-acc: 0.9196 - val_loss: 1.4243 - val_acc: 0.4942 - val_top5-acc: 0.9310 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.4914 - acc: 0.4656 - top5-acc: 0.9189 - val_loss: 1.4448 - val_acc: 0.4886 - val_top5-acc: 0.9242 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.4833 - acc: 0.4663 - top5-acc: 0.9203 - val_loss: 1.4215 - val_acc: 0.4960 - val_top5-acc: 0.9328 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.4902 - acc: 0.4653 - top5-acc: 0.9221 - val_loss: 1.4284 - val_acc: 0.4930 - val_top5-acc: 0.9244 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 1.4855 - acc: 0.4682 - top5-acc: 0.9210 - val_loss: 1.3966 - val_acc: 0.5002 - val_top5-acc: 0.9342 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4783 - acc: 0.4702 - top5-acc: 0.9228 - val_loss: 1.3984 - val_acc: 0.4954 - val_top5-acc: 0.9308 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.4726 - acc: 0.4741 - top5-acc: 0.9226 - val_loss: 1.3977 - val_acc: 0.4992 - val_top5-acc: 0.9328 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4689 - acc: 0.4744 - top5-acc: 0.9239 - val_loss: 1.4044 - val_acc: 0.4926 - val_top5-acc: 0.9306 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.4728 - acc: 0.4751 - top5-acc: 0.9243 - val_loss: 1.4104 - val_acc: 0.4976 - val_top5-acc: 0.9308 - lr: 0.0050\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 5s 62ms/step - loss: 1.4563 - acc: 0.4786 - top5-acc: 0.9254 - val_loss: 1.3640 - val_acc: 0.5144 - val_top5-acc: 0.9348 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 1.4554 - acc: 0.4797 - top5-acc: 0.9245 - val_loss: 1.3996 - val_acc: 0.5040 - val_top5-acc: 0.9326 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.4509 - acc: 0.4832 - top5-acc: 0.9242 - val_loss: 1.4043 - val_acc: 0.4940 - val_top5-acc: 0.9296 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.4628 - acc: 0.4756 - top5-acc: 0.9234 - val_loss: 1.4035 - val_acc: 0.4984 - val_top5-acc: 0.9334 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 1.4568 - acc: 0.4812 - top5-acc: 0.9235 - val_loss: 1.3829 - val_acc: 0.5102 - val_top5-acc: 0.9346 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 1.4439 - acc: 0.4846 - top5-acc: 0.9256 - val_loss: 1.4094 - val_acc: 0.4934 - val_top5-acc: 0.9322 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 6s 64ms/step - loss: 1.4164 - acc: 0.4942 - top5-acc: 0.9282 - val_loss: 1.3541 - val_acc: 0.5222 - val_top5-acc: 0.9374 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 5s 62ms/step - loss: 1.4124 - acc: 0.4946 - top5-acc: 0.9297 - val_loss: 1.3401 - val_acc: 0.5224 - val_top5-acc: 0.9406 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 6s 67ms/step - loss: 1.4154 - acc: 0.4951 - top5-acc: 0.9279 - val_loss: 1.3397 - val_acc: 0.5236 - val_top5-acc: 0.9360 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 6s 63ms/step - loss: 1.4130 - acc: 0.4942 - top5-acc: 0.9287 - val_loss: 1.3533 - val_acc: 0.5194 - val_top5-acc: 0.9362 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 1.4196 - acc: 0.4921 - top5-acc: 0.9274 - val_loss: 1.3343 - val_acc: 0.5280 - val_top5-acc: 0.9412 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 6s 65ms/step - loss: 1.4106 - acc: 0.4993 - top5-acc: 0.9286 - val_loss: 1.3481 - val_acc: 0.5152 - val_top5-acc: 0.9364 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 6s 67ms/step - loss: 1.4138 - acc: 0.4939 - top5-acc: 0.9286 - val_loss: 1.3360 - val_acc: 0.5252 - val_top5-acc: 0.9390 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.4122 - acc: 0.4941 - top5-acc: 0.9291 - val_loss: 1.3544 - val_acc: 0.5194 - val_top5-acc: 0.9374 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 5s 61ms/step - loss: 1.4173 - acc: 0.4910 - top5-acc: 0.9292 - val_loss: 1.3592 - val_acc: 0.5144 - val_top5-acc: 0.9394 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 6s 71ms/step - loss: 1.4077 - acc: 0.4954 - top5-acc: 0.9301 - val_loss: 1.3550 - val_acc: 0.5160 - val_top5-acc: 0.9352 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 6s 68ms/step - loss: 1.3969 - acc: 0.5015 - top5-acc: 0.9294 - val_loss: 1.3385 - val_acc: 0.5230 - val_top5-acc: 0.9386 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 6s 71ms/step - loss: 1.3931 - acc: 0.5016 - top5-acc: 0.9311 - val_loss: 1.3330 - val_acc: 0.5264 - val_top5-acc: 0.9384 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3939 - acc: 0.5042 - top5-acc: 0.9298 - val_loss: 1.3368 - val_acc: 0.5234 - val_top5-acc: 0.9388 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3928 - acc: 0.5026 - top5-acc: 0.9309 - val_loss: 1.3317 - val_acc: 0.5368 - val_top5-acc: 0.9390 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.4023 - acc: 0.4996 - top5-acc: 0.9291 - val_loss: 1.3377 - val_acc: 0.5348 - val_top5-acc: 0.9386 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3946 - acc: 0.5031 - top5-acc: 0.9306 - val_loss: 1.3374 - val_acc: 0.5244 - val_top5-acc: 0.9404 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3937 - acc: 0.5022 - top5-acc: 0.9318 - val_loss: 1.3353 - val_acc: 0.5288 - val_top5-acc: 0.9396 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3960 - acc: 0.5049 - top5-acc: 0.9304 - val_loss: 1.3347 - val_acc: 0.5242 - val_top5-acc: 0.9384 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3952 - acc: 0.4994 - top5-acc: 0.9310 - val_loss: 1.3301 - val_acc: 0.5266 - val_top5-acc: 0.9420 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3982 - acc: 0.5023 - top5-acc: 0.9311 - val_loss: 1.3383 - val_acc: 0.5270 - val_top5-acc: 0.9400 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3960 - acc: 0.5001 - top5-acc: 0.9312 - val_loss: 1.3346 - val_acc: 0.5210 - val_top5-acc: 0.9408 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3966 - acc: 0.5025 - top5-acc: 0.9305 - val_loss: 1.3346 - val_acc: 0.5244 - val_top5-acc: 0.9386 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3969 - acc: 0.5024 - top5-acc: 0.9305 - val_loss: 1.3408 - val_acc: 0.5230 - val_top5-acc: 0.9380 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.4005 - acc: 0.5019 - top5-acc: 0.9305 - val_loss: 1.3337 - val_acc: 0.5322 - val_top5-acc: 0.9406 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3930 - acc: 0.5017 - top5-acc: 0.9311 - val_loss: 1.3287 - val_acc: 0.5284 - val_top5-acc: 0.9394 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3897 - acc: 0.5066 - top5-acc: 0.9321 - val_loss: 1.3337 - val_acc: 0.5276 - val_top5-acc: 0.9398 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3903 - acc: 0.5023 - top5-acc: 0.9324 - val_loss: 1.3275 - val_acc: 0.5310 - val_top5-acc: 0.9386 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3880 - acc: 0.5058 - top5-acc: 0.9328 - val_loss: 1.3279 - val_acc: 0.5340 - val_top5-acc: 0.9408 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 1.3458 - acc: 0.5233 - top5-acc: 0.9385\n",
      "Test accuracy: 52.33%\n",
      "Test top 5 accuracy: 93.85%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 10s 93ms/step - loss: 1.8017 - acc: 0.3612 - top5-acc: 0.8594 - val_loss: 1.5968 - val_acc: 0.4376 - val_top5-acc: 0.9064 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 7s 75ms/step - loss: 1.5986 - acc: 0.4266 - top5-acc: 0.9036 - val_loss: 1.4942 - val_acc: 0.4610 - val_top5-acc: 0.9222 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.5472 - acc: 0.4460 - top5-acc: 0.9099 - val_loss: 1.4816 - val_acc: 0.4672 - val_top5-acc: 0.9250 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.5381 - acc: 0.4525 - top5-acc: 0.9133 - val_loss: 1.4536 - val_acc: 0.4772 - val_top5-acc: 0.9250 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 7s 84ms/step - loss: 1.5121 - acc: 0.4606 - top5-acc: 0.9169 - val_loss: 1.4418 - val_acc: 0.4914 - val_top5-acc: 0.9284 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4997 - acc: 0.4653 - top5-acc: 0.9194 - val_loss: 1.4449 - val_acc: 0.4858 - val_top5-acc: 0.9264 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4850 - acc: 0.4694 - top5-acc: 0.9200 - val_loss: 1.4177 - val_acc: 0.5028 - val_top5-acc: 0.9292 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 6s 74ms/step - loss: 1.4737 - acc: 0.4741 - top5-acc: 0.9224 - val_loss: 1.3993 - val_acc: 0.5072 - val_top5-acc: 0.9348 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4752 - acc: 0.4763 - top5-acc: 0.9215 - val_loss: 1.4103 - val_acc: 0.4974 - val_top5-acc: 0.9350 - lr: 0.0050\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4525 - acc: 0.4816 - top5-acc: 0.9235 - val_loss: 1.3902 - val_acc: 0.5086 - val_top5-acc: 0.9378 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 6s 71ms/step - loss: 1.4582 - acc: 0.4792 - top5-acc: 0.9246 - val_loss: 1.3915 - val_acc: 0.5082 - val_top5-acc: 0.9326 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.4480 - acc: 0.4822 - top5-acc: 0.9264 - val_loss: 1.3766 - val_acc: 0.5100 - val_top5-acc: 0.9314 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 6s 68ms/step - loss: 1.4470 - acc: 0.4830 - top5-acc: 0.9260 - val_loss: 1.3664 - val_acc: 0.5070 - val_top5-acc: 0.9356 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 6s 72ms/step - loss: 1.4505 - acc: 0.4834 - top5-acc: 0.9248 - val_loss: 1.4056 - val_acc: 0.4942 - val_top5-acc: 0.9338 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 6s 73ms/step - loss: 1.4541 - acc: 0.4798 - top5-acc: 0.9256 - val_loss: 1.4110 - val_acc: 0.5102 - val_top5-acc: 0.9320 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4503 - acc: 0.4837 - top5-acc: 0.9261 - val_loss: 1.3814 - val_acc: 0.5128 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4328 - acc: 0.4885 - top5-acc: 0.9277 - val_loss: 1.3563 - val_acc: 0.5230 - val_top5-acc: 0.9364 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 6s 73ms/step - loss: 1.4292 - acc: 0.4890 - top5-acc: 0.9285 - val_loss: 1.3869 - val_acc: 0.5062 - val_top5-acc: 0.9326 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 7s 76ms/step - loss: 1.4278 - acc: 0.4922 - top5-acc: 0.9278 - val_loss: 1.4072 - val_acc: 0.4924 - val_top5-acc: 0.9360 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4373 - acc: 0.4898 - top5-acc: 0.9275 - val_loss: 1.3644 - val_acc: 0.5164 - val_top5-acc: 0.9360 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 7s 76ms/step - loss: 1.4264 - acc: 0.4922 - top5-acc: 0.9278 - val_loss: 1.3743 - val_acc: 0.5122 - val_top5-acc: 0.9360 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 1.4259 - acc: 0.4941 - top5-acc: 0.9279 - val_loss: 1.3412 - val_acc: 0.5310 - val_top5-acc: 0.9378 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 1.4257 - acc: 0.4891 - top5-acc: 0.9282 - val_loss: 1.3370 - val_acc: 0.5246 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 7s 76ms/step - loss: 1.4244 - acc: 0.4920 - top5-acc: 0.9300 - val_loss: 1.3838 - val_acc: 0.5130 - val_top5-acc: 0.9362 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 7s 76ms/step - loss: 1.4343 - acc: 0.4872 - top5-acc: 0.9290 - val_loss: 1.3363 - val_acc: 0.5240 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4259 - acc: 0.4925 - top5-acc: 0.9296 - val_loss: 1.3330 - val_acc: 0.5354 - val_top5-acc: 0.9424 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 8s 91ms/step - loss: 1.4245 - acc: 0.4916 - top5-acc: 0.9297 - val_loss: 1.3344 - val_acc: 0.5254 - val_top5-acc: 0.9380 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 7s 81ms/step - loss: 1.4172 - acc: 0.4932 - top5-acc: 0.9295 - val_loss: 1.3306 - val_acc: 0.5344 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 1.4249 - acc: 0.4919 - top5-acc: 0.9277 - val_loss: 1.3496 - val_acc: 0.5190 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 7s 80ms/step - loss: 1.4225 - acc: 0.4922 - top5-acc: 0.9297 - val_loss: 1.3736 - val_acc: 0.5092 - val_top5-acc: 0.9360 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 7s 76ms/step - loss: 1.4173 - acc: 0.4951 - top5-acc: 0.9289 - val_loss: 1.3321 - val_acc: 0.5274 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4240 - acc: 0.4923 - top5-acc: 0.9308 - val_loss: 1.3351 - val_acc: 0.5326 - val_top5-acc: 0.9392 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.4075 - acc: 0.5000 - top5-acc: 0.9311 - val_loss: 1.3358 - val_acc: 0.5356 - val_top5-acc: 0.9374 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3860 - acc: 0.5026 - top5-acc: 0.9328 - val_loss: 1.3029 - val_acc: 0.5392 - val_top5-acc: 0.9452 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3889 - acc: 0.5047 - top5-acc: 0.9349 - val_loss: 1.3126 - val_acc: 0.5348 - val_top5-acc: 0.9432 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3880 - acc: 0.5034 - top5-acc: 0.9332 - val_loss: 1.3235 - val_acc: 0.5220 - val_top5-acc: 0.9394 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3836 - acc: 0.5075 - top5-acc: 0.9327 - val_loss: 1.3244 - val_acc: 0.5324 - val_top5-acc: 0.9434 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3788 - acc: 0.5077 - top5-acc: 0.9337 - val_loss: 1.3121 - val_acc: 0.5404 - val_top5-acc: 0.9424 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 6s 70ms/step - loss: 1.3808 - acc: 0.5041 - top5-acc: 0.9338 - val_loss: 1.2946 - val_acc: 0.5420 - val_top5-acc: 0.9430 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3820 - acc: 0.5069 - top5-acc: 0.9331 - val_loss: 1.3148 - val_acc: 0.5362 - val_top5-acc: 0.9436 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3827 - acc: 0.5056 - top5-acc: 0.9329 - val_loss: 1.3224 - val_acc: 0.5348 - val_top5-acc: 0.9396 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 6s 68ms/step - loss: 1.3812 - acc: 0.5085 - top5-acc: 0.9326 - val_loss: 1.3124 - val_acc: 0.5364 - val_top5-acc: 0.9406 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3862 - acc: 0.5058 - top5-acc: 0.9326 - val_loss: 1.3201 - val_acc: 0.5342 - val_top5-acc: 0.9372 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3837 - acc: 0.5046 - top5-acc: 0.9332 - val_loss: 1.3184 - val_acc: 0.5400 - val_top5-acc: 0.9438 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3706 - acc: 0.5105 - top5-acc: 0.9334 - val_loss: 1.2990 - val_acc: 0.5406 - val_top5-acc: 0.9418 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3657 - acc: 0.5152 - top5-acc: 0.9344 - val_loss: 1.3019 - val_acc: 0.5408 - val_top5-acc: 0.9428 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3650 - acc: 0.5132 - top5-acc: 0.9342 - val_loss: 1.3010 - val_acc: 0.5436 - val_top5-acc: 0.9434 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3690 - acc: 0.5095 - top5-acc: 0.9333 - val_loss: 1.3040 - val_acc: 0.5400 - val_top5-acc: 0.9442 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3723 - acc: 0.5080 - top5-acc: 0.9351 - val_loss: 1.3135 - val_acc: 0.5342 - val_top5-acc: 0.9418 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 6s 69ms/step - loss: 1.3608 - acc: 0.5152 - top5-acc: 0.9354 - val_loss: 1.2915 - val_acc: 0.5488 - val_top5-acc: 0.9440 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 1.3193 - acc: 0.5326 - top5-acc: 0.9430\n",
      "Test accuracy: 53.26%\n",
      "Test top 5 accuracy: 94.3%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 9s 84ms/step - loss: 1.8161 - acc: 0.3679 - top5-acc: 0.8596 - val_loss: 1.5515 - val_acc: 0.4344 - val_top5-acc: 0.9202 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.5710 - acc: 0.4349 - top5-acc: 0.9094 - val_loss: 1.4853 - val_acc: 0.4686 - val_top5-acc: 0.9192 - lr: 0.0050\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 7s 77ms/step - loss: 1.5139 - acc: 0.4588 - top5-acc: 0.9160 - val_loss: 1.4432 - val_acc: 0.4844 - val_top5-acc: 0.9288 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.4927 - acc: 0.4657 - top5-acc: 0.9200 - val_loss: 1.3992 - val_acc: 0.4974 - val_top5-acc: 0.9340 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4701 - acc: 0.4744 - top5-acc: 0.9216 - val_loss: 1.4156 - val_acc: 0.4874 - val_top5-acc: 0.9272 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4586 - acc: 0.4802 - top5-acc: 0.9233 - val_loss: 1.3920 - val_acc: 0.5010 - val_top5-acc: 0.9356 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4590 - acc: 0.4773 - top5-acc: 0.9248 - val_loss: 1.3976 - val_acc: 0.5004 - val_top5-acc: 0.9356 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4557 - acc: 0.4814 - top5-acc: 0.9251 - val_loss: 1.3764 - val_acc: 0.5026 - val_top5-acc: 0.9344 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4269 - acc: 0.4911 - top5-acc: 0.9278 - val_loss: 1.3419 - val_acc: 0.5244 - val_top5-acc: 0.9412 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4255 - acc: 0.4880 - top5-acc: 0.9283 - val_loss: 1.3564 - val_acc: 0.5126 - val_top5-acc: 0.9378 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4202 - acc: 0.4934 - top5-acc: 0.9286 - val_loss: 1.3461 - val_acc: 0.5176 - val_top5-acc: 0.9396 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4199 - acc: 0.4934 - top5-acc: 0.9290 - val_loss: 1.3679 - val_acc: 0.5002 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4108 - acc: 0.4979 - top5-acc: 0.9294 - val_loss: 1.3460 - val_acc: 0.5246 - val_top5-acc: 0.9368 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4081 - acc: 0.4950 - top5-acc: 0.9318 - val_loss: 1.3218 - val_acc: 0.5276 - val_top5-acc: 0.9406 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4023 - acc: 0.5006 - top5-acc: 0.9309 - val_loss: 1.3360 - val_acc: 0.5168 - val_top5-acc: 0.9346 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4083 - acc: 0.4960 - top5-acc: 0.9304 - val_loss: 1.3280 - val_acc: 0.5262 - val_top5-acc: 0.9362 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4176 - acc: 0.4955 - top5-acc: 0.9291 - val_loss: 1.3515 - val_acc: 0.5142 - val_top5-acc: 0.9328 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4038 - acc: 0.4998 - top5-acc: 0.9294 - val_loss: 1.3474 - val_acc: 0.5216 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.4041 - acc: 0.4996 - top5-acc: 0.9291 - val_loss: 1.3321 - val_acc: 0.5250 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3691 - acc: 0.5129 - top5-acc: 0.9343 - val_loss: 1.3007 - val_acc: 0.5320 - val_top5-acc: 0.9434 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3697 - acc: 0.5123 - top5-acc: 0.9339 - val_loss: 1.3019 - val_acc: 0.5310 - val_top5-acc: 0.9394 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3690 - acc: 0.5117 - top5-acc: 0.9348 - val_loss: 1.2996 - val_acc: 0.5376 - val_top5-acc: 0.9396 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3663 - acc: 0.5128 - top5-acc: 0.9351 - val_loss: 1.2961 - val_acc: 0.5360 - val_top5-acc: 0.9424 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3682 - acc: 0.5129 - top5-acc: 0.9333 - val_loss: 1.3050 - val_acc: 0.5248 - val_top5-acc: 0.9426 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3683 - acc: 0.5126 - top5-acc: 0.9351 - val_loss: 1.3031 - val_acc: 0.5372 - val_top5-acc: 0.9406 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3677 - acc: 0.5120 - top5-acc: 0.9346 - val_loss: 1.3030 - val_acc: 0.5342 - val_top5-acc: 0.9424 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3675 - acc: 0.5109 - top5-acc: 0.9341 - val_loss: 1.3003 - val_acc: 0.5416 - val_top5-acc: 0.9408 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3705 - acc: 0.5078 - top5-acc: 0.9345 - val_loss: 1.2853 - val_acc: 0.5426 - val_top5-acc: 0.9420 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3631 - acc: 0.5123 - top5-acc: 0.9359 - val_loss: 1.2877 - val_acc: 0.5412 - val_top5-acc: 0.9422 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3716 - acc: 0.5089 - top5-acc: 0.9342 - val_loss: 1.2960 - val_acc: 0.5348 - val_top5-acc: 0.9436 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3612 - acc: 0.5156 - top5-acc: 0.9351 - val_loss: 1.2871 - val_acc: 0.5454 - val_top5-acc: 0.9426 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3658 - acc: 0.5103 - top5-acc: 0.9343 - val_loss: 1.3075 - val_acc: 0.5318 - val_top5-acc: 0.9408 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3666 - acc: 0.5114 - top5-acc: 0.9366 - val_loss: 1.2979 - val_acc: 0.5384 - val_top5-acc: 0.9392 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3524 - acc: 0.5198 - top5-acc: 0.9357 - val_loss: 1.2835 - val_acc: 0.5452 - val_top5-acc: 0.9410 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3470 - acc: 0.5199 - top5-acc: 0.9372 - val_loss: 1.2820 - val_acc: 0.5472 - val_top5-acc: 0.9436 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3479 - acc: 0.5205 - top5-acc: 0.9365 - val_loss: 1.2817 - val_acc: 0.5466 - val_top5-acc: 0.9422 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 7s 77ms/step - loss: 1.3486 - acc: 0.5181 - top5-acc: 0.9358 - val_loss: 1.2859 - val_acc: 0.5450 - val_top5-acc: 0.9418 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 7s 82ms/step - loss: 1.3488 - acc: 0.5211 - top5-acc: 0.9370 - val_loss: 1.2841 - val_acc: 0.5486 - val_top5-acc: 0.9438 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3537 - acc: 0.5201 - top5-acc: 0.9361 - val_loss: 1.2882 - val_acc: 0.5446 - val_top5-acc: 0.9408 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 7s 82ms/step - loss: 1.3518 - acc: 0.5176 - top5-acc: 0.9373 - val_loss: 1.2917 - val_acc: 0.5390 - val_top5-acc: 0.9426 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3465 - acc: 0.5204 - top5-acc: 0.9375 - val_loss: 1.2866 - val_acc: 0.5474 - val_top5-acc: 0.9414 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3397 - acc: 0.5203 - top5-acc: 0.9381 - val_loss: 1.2798 - val_acc: 0.5464 - val_top5-acc: 0.9436 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3440 - acc: 0.5205 - top5-acc: 0.9375 - val_loss: 1.2776 - val_acc: 0.5498 - val_top5-acc: 0.9428 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3400 - acc: 0.5252 - top5-acc: 0.9372 - val_loss: 1.2794 - val_acc: 0.5492 - val_top5-acc: 0.9414 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3444 - acc: 0.5236 - top5-acc: 0.9363 - val_loss: 1.2779 - val_acc: 0.5466 - val_top5-acc: 0.9450 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3442 - acc: 0.5208 - top5-acc: 0.9365 - val_loss: 1.2801 - val_acc: 0.5474 - val_top5-acc: 0.9436 - lr: 6.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3464 - acc: 0.5213 - top5-acc: 0.9359 - val_loss: 1.2820 - val_acc: 0.5452 - val_top5-acc: 0.9438 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3419 - acc: 0.5241 - top5-acc: 0.9367 - val_loss: 1.2801 - val_acc: 0.5444 - val_top5-acc: 0.9448 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 7s 78ms/step - loss: 1.3392 - acc: 0.5258 - top5-acc: 0.9378 - val_loss: 1.2802 - val_acc: 0.5424 - val_top5-acc: 0.9444 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 7s 79ms/step - loss: 1.3400 - acc: 0.5247 - top5-acc: 0.9366 - val_loss: 1.2805 - val_acc: 0.5496 - val_top5-acc: 0.9430 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 10s 30ms/step - loss: 1.2957 - acc: 0.5384 - top5-acc: 0.9440\n",
      "Test accuracy: 53.84%\n",
      "Test top 5 accuracy: 94.4%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 12s 93ms/step - loss: 1.8012 - acc: 0.3706 - top5-acc: 0.8583 - val_loss: 1.5453 - val_acc: 0.4476 - val_top5-acc: 0.9144 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.5553 - acc: 0.4469 - top5-acc: 0.9092 - val_loss: 1.4841 - val_acc: 0.4694 - val_top5-acc: 0.9254 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 1.5108 - acc: 0.4645 - top5-acc: 0.9153 - val_loss: 1.4696 - val_acc: 0.4758 - val_top5-acc: 0.9228 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4759 - acc: 0.4740 - top5-acc: 0.9199 - val_loss: 1.4190 - val_acc: 0.4912 - val_top5-acc: 0.9316 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 1.4556 - acc: 0.4842 - top5-acc: 0.9238 - val_loss: 1.3980 - val_acc: 0.5060 - val_top5-acc: 0.9338 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4487 - acc: 0.4872 - top5-acc: 0.9261 - val_loss: 1.3701 - val_acc: 0.5174 - val_top5-acc: 0.9324 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4303 - acc: 0.4933 - top5-acc: 0.9269 - val_loss: 1.3811 - val_acc: 0.5116 - val_top5-acc: 0.9358 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4268 - acc: 0.4953 - top5-acc: 0.9274 - val_loss: 1.3546 - val_acc: 0.5218 - val_top5-acc: 0.9376 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4192 - acc: 0.4961 - top5-acc: 0.9296 - val_loss: 1.3520 - val_acc: 0.5210 - val_top5-acc: 0.9350 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4106 - acc: 0.4980 - top5-acc: 0.9310 - val_loss: 1.3550 - val_acc: 0.5094 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4087 - acc: 0.4986 - top5-acc: 0.9306 - val_loss: 1.3518 - val_acc: 0.5228 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.4174 - acc: 0.4988 - top5-acc: 0.9287 - val_loss: 1.3494 - val_acc: 0.5130 - val_top5-acc: 0.9368 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3998 - acc: 0.5028 - top5-acc: 0.9316 - val_loss: 1.3321 - val_acc: 0.5244 - val_top5-acc: 0.9412 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3859 - acc: 0.5078 - top5-acc: 0.9328 - val_loss: 1.3421 - val_acc: 0.5232 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3877 - acc: 0.5095 - top5-acc: 0.9331 - val_loss: 1.3207 - val_acc: 0.5272 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3859 - acc: 0.5062 - top5-acc: 0.9315 - val_loss: 1.3044 - val_acc: 0.5420 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3818 - acc: 0.5084 - top5-acc: 0.9320 - val_loss: 1.3165 - val_acc: 0.5342 - val_top5-acc: 0.9394 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3865 - acc: 0.5064 - top5-acc: 0.9329 - val_loss: 1.2999 - val_acc: 0.5436 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3744 - acc: 0.5108 - top5-acc: 0.9327 - val_loss: 1.3065 - val_acc: 0.5416 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3851 - acc: 0.5078 - top5-acc: 0.9328 - val_loss: 1.2983 - val_acc: 0.5466 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 1.3801 - acc: 0.5100 - top5-acc: 0.9330 - val_loss: 1.3042 - val_acc: 0.5426 - val_top5-acc: 0.9408 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3741 - acc: 0.5126 - top5-acc: 0.9348 - val_loss: 1.3089 - val_acc: 0.5360 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3702 - acc: 0.5164 - top5-acc: 0.9340 - val_loss: 1.3123 - val_acc: 0.5366 - val_top5-acc: 0.9416 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3738 - acc: 0.5112 - top5-acc: 0.9341 - val_loss: 1.2948 - val_acc: 0.5452 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3718 - acc: 0.5115 - top5-acc: 0.9335 - val_loss: 1.3216 - val_acc: 0.5354 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3757 - acc: 0.5107 - top5-acc: 0.9343 - val_loss: 1.2868 - val_acc: 0.5444 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 1.3741 - acc: 0.5108 - top5-acc: 0.9342 - val_loss: 1.2960 - val_acc: 0.5458 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3725 - acc: 0.5140 - top5-acc: 0.9344 - val_loss: 1.2897 - val_acc: 0.5370 - val_top5-acc: 0.9430 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3658 - acc: 0.5150 - top5-acc: 0.9348 - val_loss: 1.2862 - val_acc: 0.5406 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3627 - acc: 0.5176 - top5-acc: 0.9365 - val_loss: 1.2834 - val_acc: 0.5436 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 1.3597 - acc: 0.5188 - top5-acc: 0.9343 - val_loss: 1.3050 - val_acc: 0.5402 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 1.3705 - acc: 0.5129 - top5-acc: 0.9347 - val_loss: 1.2904 - val_acc: 0.5436 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3687 - acc: 0.5137 - top5-acc: 0.9358 - val_loss: 1.2959 - val_acc: 0.5364 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3677 - acc: 0.5139 - top5-acc: 0.9358 - val_loss: 1.2981 - val_acc: 0.5452 - val_top5-acc: 0.9414 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3601 - acc: 0.5157 - top5-acc: 0.9360 - val_loss: 1.2857 - val_acc: 0.5434 - val_top5-acc: 0.9430 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 1.3417 - acc: 0.5231 - top5-acc: 0.9378 - val_loss: 1.2581 - val_acc: 0.5528 - val_top5-acc: 0.9420 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 1.3391 - acc: 0.5253 - top5-acc: 0.9370 - val_loss: 1.2670 - val_acc: 0.5468 - val_top5-acc: 0.9456 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3417 - acc: 0.5216 - top5-acc: 0.9372 - val_loss: 1.2766 - val_acc: 0.5516 - val_top5-acc: 0.9430 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3380 - acc: 0.5235 - top5-acc: 0.9377 - val_loss: 1.2635 - val_acc: 0.5506 - val_top5-acc: 0.9438 - lr: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 1.3385 - acc: 0.5260 - top5-acc: 0.9373 - val_loss: 1.2592 - val_acc: 0.5536 - val_top5-acc: 0.9466 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3386 - acc: 0.5233 - top5-acc: 0.9362 - val_loss: 1.2606 - val_acc: 0.5552 - val_top5-acc: 0.9468 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 1.3274 - acc: 0.5279 - top5-acc: 0.9389 - val_loss: 1.2577 - val_acc: 0.5590 - val_top5-acc: 0.9494 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3298 - acc: 0.5290 - top5-acc: 0.9387 - val_loss: 1.2608 - val_acc: 0.5546 - val_top5-acc: 0.9472 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3300 - acc: 0.5267 - top5-acc: 0.9379 - val_loss: 1.2512 - val_acc: 0.5566 - val_top5-acc: 0.9472 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3263 - acc: 0.5278 - top5-acc: 0.9401 - val_loss: 1.2688 - val_acc: 0.5492 - val_top5-acc: 0.9452 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3287 - acc: 0.5277 - top5-acc: 0.9383 - val_loss: 1.2637 - val_acc: 0.5520 - val_top5-acc: 0.9474 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3293 - acc: 0.5269 - top5-acc: 0.9385 - val_loss: 1.2549 - val_acc: 0.5630 - val_top5-acc: 0.9472 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 1.3287 - acc: 0.5276 - top5-acc: 0.9380 - val_loss: 1.2562 - val_acc: 0.5562 - val_top5-acc: 0.9478 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 8s 89ms/step - loss: 1.3292 - acc: 0.5247 - top5-acc: 0.9404 - val_loss: 1.2648 - val_acc: 0.5538 - val_top5-acc: 0.9456 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 8s 90ms/step - loss: 1.3200 - acc: 0.5296 - top5-acc: 0.9398 - val_loss: 1.2529 - val_acc: 0.5602 - val_top5-acc: 0.9474 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2644 - acc: 0.5529 - top5-acc: 0.9440\n",
      "Test accuracy: 55.29%\n",
      "Test top 5 accuracy: 94.4%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 11s 103ms/step - loss: 1.8230 - acc: 0.3614 - top5-acc: 0.8533 - val_loss: 1.5421 - val_acc: 0.4556 - val_top5-acc: 0.9160 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.5557 - acc: 0.4462 - top5-acc: 0.9094 - val_loss: 1.4854 - val_acc: 0.4752 - val_top5-acc: 0.9248 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.5077 - acc: 0.4643 - top5-acc: 0.9179 - val_loss: 1.4650 - val_acc: 0.4742 - val_top5-acc: 0.9252 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.4771 - acc: 0.4738 - top5-acc: 0.9219 - val_loss: 1.3997 - val_acc: 0.5018 - val_top5-acc: 0.9338 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.4565 - acc: 0.4814 - top5-acc: 0.9233 - val_loss: 1.3941 - val_acc: 0.4994 - val_top5-acc: 0.9352 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.4418 - acc: 0.4930 - top5-acc: 0.9236 - val_loss: 1.4033 - val_acc: 0.5030 - val_top5-acc: 0.9346 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.4445 - acc: 0.4870 - top5-acc: 0.9256 - val_loss: 1.3733 - val_acc: 0.5140 - val_top5-acc: 0.9370 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.4247 - acc: 0.4960 - top5-acc: 0.9282 - val_loss: 1.3656 - val_acc: 0.5116 - val_top5-acc: 0.9388 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.4173 - acc: 0.4978 - top5-acc: 0.9296 - val_loss: 1.3395 - val_acc: 0.5294 - val_top5-acc: 0.9396 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.4118 - acc: 0.4983 - top5-acc: 0.9292 - val_loss: 1.3621 - val_acc: 0.5178 - val_top5-acc: 0.9376 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.4116 - acc: 0.5004 - top5-acc: 0.9306 - val_loss: 1.3755 - val_acc: 0.5032 - val_top5-acc: 0.9380 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.4116 - acc: 0.4959 - top5-acc: 0.9307 - val_loss: 1.3828 - val_acc: 0.5034 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3978 - acc: 0.5047 - top5-acc: 0.9314 - val_loss: 1.3398 - val_acc: 0.5240 - val_top5-acc: 0.9408 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3918 - acc: 0.5082 - top5-acc: 0.9318 - val_loss: 1.3331 - val_acc: 0.5198 - val_top5-acc: 0.9422 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3861 - acc: 0.5086 - top5-acc: 0.9318 - val_loss: 1.3371 - val_acc: 0.5300 - val_top5-acc: 0.9406 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3861 - acc: 0.5075 - top5-acc: 0.9336 - val_loss: 1.3336 - val_acc: 0.5228 - val_top5-acc: 0.9416 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3918 - acc: 0.5037 - top5-acc: 0.9320 - val_loss: 1.3132 - val_acc: 0.5318 - val_top5-acc: 0.9408 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3947 - acc: 0.5079 - top5-acc: 0.9316 - val_loss: 1.3282 - val_acc: 0.5212 - val_top5-acc: 0.9404 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3885 - acc: 0.5079 - top5-acc: 0.9329 - val_loss: 1.3172 - val_acc: 0.5334 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3807 - acc: 0.5093 - top5-acc: 0.9332 - val_loss: 1.3303 - val_acc: 0.5372 - val_top5-acc: 0.9422 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3737 - acc: 0.5141 - top5-acc: 0.9340 - val_loss: 1.3095 - val_acc: 0.5332 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.3840 - acc: 0.5077 - top5-acc: 0.9335 - val_loss: 1.3319 - val_acc: 0.5276 - val_top5-acc: 0.9386 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3742 - acc: 0.5106 - top5-acc: 0.9331 - val_loss: 1.2985 - val_acc: 0.5388 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3733 - acc: 0.5141 - top5-acc: 0.9321 - val_loss: 1.3006 - val_acc: 0.5304 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.3783 - acc: 0.5092 - top5-acc: 0.9331 - val_loss: 1.3187 - val_acc: 0.5310 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3796 - acc: 0.5094 - top5-acc: 0.9327 - val_loss: 1.2967 - val_acc: 0.5360 - val_top5-acc: 0.9414 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.3688 - acc: 0.5138 - top5-acc: 0.9337 - val_loss: 1.2949 - val_acc: 0.5422 - val_top5-acc: 0.9448 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.3671 - acc: 0.5160 - top5-acc: 0.9340 - val_loss: 1.3034 - val_acc: 0.5348 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 8s 95ms/step - loss: 1.3768 - acc: 0.5092 - top5-acc: 0.9339 - val_loss: 1.2915 - val_acc: 0.5402 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 9s 98ms/step - loss: 1.3717 - acc: 0.5112 - top5-acc: 0.9357 - val_loss: 1.3154 - val_acc: 0.5276 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 8s 97ms/step - loss: 1.3674 - acc: 0.5146 - top5-acc: 0.9354 - val_loss: 1.3195 - val_acc: 0.5368 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3672 - acc: 0.5163 - top5-acc: 0.9349 - val_loss: 1.3063 - val_acc: 0.5318 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 8s 91ms/step - loss: 1.3732 - acc: 0.5101 - top5-acc: 0.9351 - val_loss: 1.3172 - val_acc: 0.5270 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3769 - acc: 0.5115 - top5-acc: 0.9347 - val_loss: 1.3039 - val_acc: 0.5352 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3460 - acc: 0.5224 - top5-acc: 0.9369 - val_loss: 1.2731 - val_acc: 0.5450 - val_top5-acc: 0.9468 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3369 - acc: 0.5264 - top5-acc: 0.9384 - val_loss: 1.2755 - val_acc: 0.5492 - val_top5-acc: 0.9470 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.3363 - acc: 0.5260 - top5-acc: 0.9361 - val_loss: 1.2655 - val_acc: 0.5502 - val_top5-acc: 0.9476 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3430 - acc: 0.5214 - top5-acc: 0.9370 - val_loss: 1.2660 - val_acc: 0.5496 - val_top5-acc: 0.9458 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3409 - acc: 0.5236 - top5-acc: 0.9366 - val_loss: 1.2788 - val_acc: 0.5454 - val_top5-acc: 0.9462 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3422 - acc: 0.5196 - top5-acc: 0.9379 - val_loss: 1.2762 - val_acc: 0.5458 - val_top5-acc: 0.9482 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3420 - acc: 0.5225 - top5-acc: 0.9383 - val_loss: 1.2792 - val_acc: 0.5454 - val_top5-acc: 0.9484 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3439 - acc: 0.5208 - top5-acc: 0.9365 - val_loss: 1.2711 - val_acc: 0.5514 - val_top5-acc: 0.9474 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3308 - acc: 0.5292 - top5-acc: 0.9391 - val_loss: 1.2624 - val_acc: 0.5484 - val_top5-acc: 0.9494 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3244 - acc: 0.5290 - top5-acc: 0.9393 - val_loss: 1.2636 - val_acc: 0.5528 - val_top5-acc: 0.9484 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3323 - acc: 0.5266 - top5-acc: 0.9382 - val_loss: 1.2692 - val_acc: 0.5466 - val_top5-acc: 0.9480 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3336 - acc: 0.5259 - top5-acc: 0.9378 - val_loss: 1.2652 - val_acc: 0.5436 - val_top5-acc: 0.9484 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 1.3314 - acc: 0.5263 - top5-acc: 0.9392 - val_loss: 1.2616 - val_acc: 0.5522 - val_top5-acc: 0.9490 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3271 - acc: 0.5299 - top5-acc: 0.9387 - val_loss: 1.2647 - val_acc: 0.5470 - val_top5-acc: 0.9468 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 8s 92ms/step - loss: 1.3335 - acc: 0.5287 - top5-acc: 0.9375 - val_loss: 1.2661 - val_acc: 0.5514 - val_top5-acc: 0.9500 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 8s 91ms/step - loss: 1.3300 - acc: 0.5304 - top5-acc: 0.9376 - val_loss: 1.2643 - val_acc: 0.5492 - val_top5-acc: 0.9478 - lr: 0.0012\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2781 - acc: 0.5467 - top5-acc: 0.9460\n",
      "Test accuracy: 54.67%\n",
      "Test top 5 accuracy: 94.6%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 12s 112ms/step - loss: 1.8445 - acc: 0.3565 - top5-acc: 0.8502 - val_loss: 1.5442 - val_acc: 0.4490 - val_top5-acc: 0.9168 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 1.5455 - acc: 0.4510 - top5-acc: 0.9122 - val_loss: 1.4575 - val_acc: 0.4824 - val_top5-acc: 0.9238 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 10s 108ms/step - loss: 1.4879 - acc: 0.4699 - top5-acc: 0.9214 - val_loss: 1.4284 - val_acc: 0.4914 - val_top5-acc: 0.9278 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 1.4685 - acc: 0.4781 - top5-acc: 0.9216 - val_loss: 1.4149 - val_acc: 0.4934 - val_top5-acc: 0.9352 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 1.4413 - acc: 0.4878 - top5-acc: 0.9268 - val_loss: 1.3978 - val_acc: 0.5002 - val_top5-acc: 0.9326 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.4372 - acc: 0.4909 - top5-acc: 0.9268 - val_loss: 1.3953 - val_acc: 0.4988 - val_top5-acc: 0.9380 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.4233 - acc: 0.4940 - top5-acc: 0.9302 - val_loss: 1.3733 - val_acc: 0.5090 - val_top5-acc: 0.9368 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.4238 - acc: 0.4922 - top5-acc: 0.9305 - val_loss: 1.3703 - val_acc: 0.5058 - val_top5-acc: 0.9392 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.4023 - acc: 0.4993 - top5-acc: 0.9317 - val_loss: 1.3461 - val_acc: 0.5146 - val_top5-acc: 0.9396 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.4017 - acc: 0.5013 - top5-acc: 0.9316 - val_loss: 1.3285 - val_acc: 0.5268 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.3915 - acc: 0.5059 - top5-acc: 0.9322 - val_loss: 1.3344 - val_acc: 0.5206 - val_top5-acc: 0.9424 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.3924 - acc: 0.5035 - top5-acc: 0.9317 - val_loss: 1.3281 - val_acc: 0.5278 - val_top5-acc: 0.9416 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.3842 - acc: 0.5075 - top5-acc: 0.9340 - val_loss: 1.3237 - val_acc: 0.5336 - val_top5-acc: 0.9396 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.3794 - acc: 0.5130 - top5-acc: 0.9314 - val_loss: 1.3211 - val_acc: 0.5342 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.3735 - acc: 0.5106 - top5-acc: 0.9350 - val_loss: 1.3140 - val_acc: 0.5326 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.3745 - acc: 0.5104 - top5-acc: 0.9336 - val_loss: 1.3460 - val_acc: 0.5232 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.3698 - acc: 0.5130 - top5-acc: 0.9350 - val_loss: 1.3191 - val_acc: 0.5354 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 1.3738 - acc: 0.5122 - top5-acc: 0.9346 - val_loss: 1.3446 - val_acc: 0.5248 - val_top5-acc: 0.9434 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 9s 107ms/step - loss: 1.3693 - acc: 0.5145 - top5-acc: 0.9333 - val_loss: 1.3035 - val_acc: 0.5396 - val_top5-acc: 0.9426 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 1.3669 - acc: 0.5144 - top5-acc: 0.9357 - val_loss: 1.3244 - val_acc: 0.5326 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 9s 105ms/step - loss: 1.3676 - acc: 0.5165 - top5-acc: 0.9351 - val_loss: 1.3151 - val_acc: 0.5278 - val_top5-acc: 0.9448 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 1.3624 - acc: 0.5160 - top5-acc: 0.9365 - val_loss: 1.3154 - val_acc: 0.5320 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3650 - acc: 0.5149 - top5-acc: 0.9351 - val_loss: 1.3353 - val_acc: 0.5296 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 10s 111ms/step - loss: 1.3604 - acc: 0.5171 - top5-acc: 0.9338 - val_loss: 1.3007 - val_acc: 0.5380 - val_top5-acc: 0.9472 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 9s 104ms/step - loss: 1.3580 - acc: 0.5173 - top5-acc: 0.9364 - val_loss: 1.3146 - val_acc: 0.5304 - val_top5-acc: 0.9440 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 1.3581 - acc: 0.5161 - top5-acc: 0.9364 - val_loss: 1.3009 - val_acc: 0.5346 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 1.3638 - acc: 0.5152 - top5-acc: 0.9354 - val_loss: 1.2870 - val_acc: 0.5402 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 1.3599 - acc: 0.5184 - top5-acc: 0.9351 - val_loss: 1.2783 - val_acc: 0.5440 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 10s 110ms/step - loss: 1.3574 - acc: 0.5183 - top5-acc: 0.9352 - val_loss: 1.2869 - val_acc: 0.5470 - val_top5-acc: 0.9448 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 9s 106ms/step - loss: 1.3570 - acc: 0.5168 - top5-acc: 0.9370 - val_loss: 1.3040 - val_acc: 0.5330 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 10s 110ms/step - loss: 1.3612 - acc: 0.5149 - top5-acc: 0.9348 - val_loss: 1.2794 - val_acc: 0.5404 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 1.3613 - acc: 0.5138 - top5-acc: 0.9361 - val_loss: 1.2868 - val_acc: 0.5442 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 10s 110ms/step - loss: 1.3610 - acc: 0.5158 - top5-acc: 0.9369 - val_loss: 1.2813 - val_acc: 0.5434 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 10s 114ms/step - loss: 1.3288 - acc: 0.5272 - top5-acc: 0.9387 - val_loss: 1.2645 - val_acc: 0.5480 - val_top5-acc: 0.9484 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.3316 - acc: 0.5268 - top5-acc: 0.9376 - val_loss: 1.2674 - val_acc: 0.5426 - val_top5-acc: 0.9486 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 10s 115ms/step - loss: 1.3303 - acc: 0.5288 - top5-acc: 0.9386 - val_loss: 1.2562 - val_acc: 0.5510 - val_top5-acc: 0.9478 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 10s 118ms/step - loss: 1.3346 - acc: 0.5262 - top5-acc: 0.9370 - val_loss: 1.2712 - val_acc: 0.5440 - val_top5-acc: 0.9474 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 10s 118ms/step - loss: 1.3296 - acc: 0.5262 - top5-acc: 0.9390 - val_loss: 1.2675 - val_acc: 0.5538 - val_top5-acc: 0.9504 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3298 - acc: 0.5281 - top5-acc: 0.9386 - val_loss: 1.2609 - val_acc: 0.5568 - val_top5-acc: 0.9488 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 9s 105ms/step - loss: 1.3299 - acc: 0.5285 - top5-acc: 0.9374 - val_loss: 1.2516 - val_acc: 0.5556 - val_top5-acc: 0.9506 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 9s 105ms/step - loss: 1.3266 - acc: 0.5257 - top5-acc: 0.9389 - val_loss: 1.2581 - val_acc: 0.5552 - val_top5-acc: 0.9496 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 9s 104ms/step - loss: 1.3294 - acc: 0.5285 - top5-acc: 0.9393 - val_loss: 1.2633 - val_acc: 0.5472 - val_top5-acc: 0.9506 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 10s 113ms/step - loss: 1.3338 - acc: 0.5232 - top5-acc: 0.9380 - val_loss: 1.2609 - val_acc: 0.5516 - val_top5-acc: 0.9496 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 10s 118ms/step - loss: 1.3282 - acc: 0.5303 - top5-acc: 0.9383 - val_loss: 1.2619 - val_acc: 0.5472 - val_top5-acc: 0.9484 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 10s 119ms/step - loss: 1.3271 - acc: 0.5285 - top5-acc: 0.9392 - val_loss: 1.2747 - val_acc: 0.5426 - val_top5-acc: 0.9482 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 11s 119ms/step - loss: 1.3187 - acc: 0.5306 - top5-acc: 0.9392 - val_loss: 1.2535 - val_acc: 0.5576 - val_top5-acc: 0.9510 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3179 - acc: 0.5347 - top5-acc: 0.9396 - val_loss: 1.2655 - val_acc: 0.5500 - val_top5-acc: 0.9520 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.3152 - acc: 0.5333 - top5-acc: 0.9409 - val_loss: 1.2540 - val_acc: 0.5620 - val_top5-acc: 0.9508 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 10s 111ms/step - loss: 1.3189 - acc: 0.5311 - top5-acc: 0.9399 - val_loss: 1.2527 - val_acc: 0.5570 - val_top5-acc: 0.9512 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 10s 110ms/step - loss: 1.3223 - acc: 0.5310 - top5-acc: 0.9386 - val_loss: 1.2538 - val_acc: 0.5636 - val_top5-acc: 0.9522 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 1.2685 - acc: 0.5523 - top5-acc: 0.9447\n",
      "Test accuracy: 55.23%\n",
      "Test top 5 accuracy: 94.47%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 15s 137ms/step - loss: 1.7980 - acc: 0.3732 - top5-acc: 0.8599 - val_loss: 1.5171 - val_acc: 0.4564 - val_top5-acc: 0.9172 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 10s 118ms/step - loss: 1.5248 - acc: 0.4581 - top5-acc: 0.9146 - val_loss: 1.4605 - val_acc: 0.4792 - val_top5-acc: 0.9256 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 11s 121ms/step - loss: 1.4652 - acc: 0.4801 - top5-acc: 0.9234 - val_loss: 1.4127 - val_acc: 0.4862 - val_top5-acc: 0.9344 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 10s 115ms/step - loss: 1.4318 - acc: 0.4920 - top5-acc: 0.9261 - val_loss: 1.3787 - val_acc: 0.5062 - val_top5-acc: 0.9344 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 10s 119ms/step - loss: 1.4179 - acc: 0.4991 - top5-acc: 0.9280 - val_loss: 1.3289 - val_acc: 0.5362 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 11s 127ms/step - loss: 1.3971 - acc: 0.5059 - top5-acc: 0.9310 - val_loss: 1.3575 - val_acc: 0.5126 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 11s 126ms/step - loss: 1.3922 - acc: 0.5032 - top5-acc: 0.9317 - val_loss: 1.3525 - val_acc: 0.5100 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 11s 125ms/step - loss: 1.3823 - acc: 0.5075 - top5-acc: 0.9326 - val_loss: 1.3482 - val_acc: 0.5144 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 11s 120ms/step - loss: 1.3759 - acc: 0.5107 - top5-acc: 0.9346 - val_loss: 1.3219 - val_acc: 0.5298 - val_top5-acc: 0.9448 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 11s 126ms/step - loss: 1.3699 - acc: 0.5142 - top5-acc: 0.9345 - val_loss: 1.3112 - val_acc: 0.5312 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 11s 125ms/step - loss: 1.3589 - acc: 0.5163 - top5-acc: 0.9363 - val_loss: 1.3001 - val_acc: 0.5362 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 11s 126ms/step - loss: 1.3582 - acc: 0.5147 - top5-acc: 0.9345 - val_loss: 1.2718 - val_acc: 0.5490 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 1.3553 - acc: 0.5161 - top5-acc: 0.9361 - val_loss: 1.2883 - val_acc: 0.5390 - val_top5-acc: 0.9472 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3546 - acc: 0.5179 - top5-acc: 0.9359 - val_loss: 1.2780 - val_acc: 0.5494 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.3500 - acc: 0.5215 - top5-acc: 0.9363 - val_loss: 1.2855 - val_acc: 0.5350 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 11s 124ms/step - loss: 1.3375 - acc: 0.5243 - top5-acc: 0.9384 - val_loss: 1.2674 - val_acc: 0.5524 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 1.3486 - acc: 0.5186 - top5-acc: 0.9375 - val_loss: 1.2818 - val_acc: 0.5440 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 1.3344 - acc: 0.5247 - top5-acc: 0.9385 - val_loss: 1.2632 - val_acc: 0.5496 - val_top5-acc: 0.9528 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 1.3343 - acc: 0.5254 - top5-acc: 0.9383 - val_loss: 1.2729 - val_acc: 0.5502 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 11s 120ms/step - loss: 1.3368 - acc: 0.5233 - top5-acc: 0.9382 - val_loss: 1.2880 - val_acc: 0.5406 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 10s 119ms/step - loss: 1.3362 - acc: 0.5256 - top5-acc: 0.9381 - val_loss: 1.2742 - val_acc: 0.5460 - val_top5-acc: 0.9484 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 11s 123ms/step - loss: 1.3307 - acc: 0.5283 - top5-acc: 0.9392 - val_loss: 1.2653 - val_acc: 0.5484 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 11s 120ms/step - loss: 1.3318 - acc: 0.5269 - top5-acc: 0.9371 - val_loss: 1.3000 - val_acc: 0.5342 - val_top5-acc: 0.9458 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 11s 126ms/step - loss: 1.3108 - acc: 0.5333 - top5-acc: 0.9400 - val_loss: 1.2352 - val_acc: 0.5594 - val_top5-acc: 0.9520 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 11s 122ms/step - loss: 1.3117 - acc: 0.5341 - top5-acc: 0.9401 - val_loss: 1.2338 - val_acc: 0.5618 - val_top5-acc: 0.9520 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 10s 115ms/step - loss: 1.3076 - acc: 0.5360 - top5-acc: 0.9399 - val_loss: 1.2443 - val_acc: 0.5570 - val_top5-acc: 0.9514 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 11s 126ms/step - loss: 1.3069 - acc: 0.5337 - top5-acc: 0.9412 - val_loss: 1.2441 - val_acc: 0.5524 - val_top5-acc: 0.9514 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 1.3059 - acc: 0.5379 - top5-acc: 0.9411 - val_loss: 1.2396 - val_acc: 0.5594 - val_top5-acc: 0.9540 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 1.3069 - acc: 0.5349 - top5-acc: 0.9422 - val_loss: 1.2383 - val_acc: 0.5596 - val_top5-acc: 0.9528 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 1.3035 - acc: 0.5355 - top5-acc: 0.9405 - val_loss: 1.2361 - val_acc: 0.5596 - val_top5-acc: 0.9510 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 10s 113ms/step - loss: 1.2961 - acc: 0.5393 - top5-acc: 0.9421 - val_loss: 1.2258 - val_acc: 0.5654 - val_top5-acc: 0.9532 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 1.2897 - acc: 0.5410 - top5-acc: 0.9435 - val_loss: 1.2338 - val_acc: 0.5588 - val_top5-acc: 0.9524 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.2937 - acc: 0.5422 - top5-acc: 0.9415 - val_loss: 1.2317 - val_acc: 0.5654 - val_top5-acc: 0.9536 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 10s 114ms/step - loss: 1.2999 - acc: 0.5388 - top5-acc: 0.9417 - val_loss: 1.2345 - val_acc: 0.5648 - val_top5-acc: 0.9538 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 1.2949 - acc: 0.5398 - top5-acc: 0.9425 - val_loss: 1.2322 - val_acc: 0.5652 - val_top5-acc: 0.9536 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 10s 115ms/step - loss: 1.2940 - acc: 0.5419 - top5-acc: 0.9419 - val_loss: 1.2314 - val_acc: 0.5620 - val_top5-acc: 0.9534 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 10s 114ms/step - loss: 1.2944 - acc: 0.5414 - top5-acc: 0.9419 - val_loss: 1.2265 - val_acc: 0.5646 - val_top5-acc: 0.9526 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.2881 - acc: 0.5427 - top5-acc: 0.9419 - val_loss: 1.2269 - val_acc: 0.5662 - val_top5-acc: 0.9528 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.2904 - acc: 0.5420 - top5-acc: 0.9429 - val_loss: 1.2291 - val_acc: 0.5646 - val_top5-acc: 0.9534 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 11s 119ms/step - loss: 1.2914 - acc: 0.5445 - top5-acc: 0.9430 - val_loss: 1.2364 - val_acc: 0.5624 - val_top5-acc: 0.9520 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 10s 118ms/step - loss: 1.2881 - acc: 0.5424 - top5-acc: 0.9437 - val_loss: 1.2320 - val_acc: 0.5642 - val_top5-acc: 0.9528 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 10s 117ms/step - loss: 1.2856 - acc: 0.5454 - top5-acc: 0.9438 - val_loss: 1.2260 - val_acc: 0.5674 - val_top5-acc: 0.9536 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 10s 118ms/step - loss: 1.2870 - acc: 0.5467 - top5-acc: 0.9430 - val_loss: 1.2266 - val_acc: 0.5672 - val_top5-acc: 0.9552 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 10s 113ms/step - loss: 1.2910 - acc: 0.5426 - top5-acc: 0.9440 - val_loss: 1.2296 - val_acc: 0.5672 - val_top5-acc: 0.9538 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 1.2883 - acc: 0.5455 - top5-acc: 0.9437 - val_loss: 1.2262 - val_acc: 0.5686 - val_top5-acc: 0.9546 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 1.2900 - acc: 0.5430 - top5-acc: 0.9433 - val_loss: 1.2360 - val_acc: 0.5644 - val_top5-acc: 0.9554 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 1.2896 - acc: 0.5439 - top5-acc: 0.9426 - val_loss: 1.2298 - val_acc: 0.5668 - val_top5-acc: 0.9534 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 1.2931 - acc: 0.5429 - top5-acc: 0.9414 - val_loss: 1.2344 - val_acc: 0.5626 - val_top5-acc: 0.9528 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 10s 113ms/step - loss: 1.2924 - acc: 0.5452 - top5-acc: 0.9426 - val_loss: 1.2316 - val_acc: 0.5690 - val_top5-acc: 0.9538 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 1.2921 - acc: 0.5465 - top5-acc: 0.9426 - val_loss: 1.2336 - val_acc: 0.5672 - val_top5-acc: 0.9536 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 1.2433 - acc: 0.5594 - top5-acc: 0.9503\n",
      "Test accuracy: 55.94%\n",
      "Test top 5 accuracy: 95.03%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 16s 141ms/step - loss: 1.7374 - acc: 0.3868 - top5-acc: 0.8689 - val_loss: 1.4848 - val_acc: 0.4696 - val_top5-acc: 0.9232 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.4967 - acc: 0.4686 - top5-acc: 0.9176 - val_loss: 1.4258 - val_acc: 0.4868 - val_top5-acc: 0.9310 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 12s 134ms/step - loss: 1.4379 - acc: 0.4877 - top5-acc: 0.9268 - val_loss: 1.3933 - val_acc: 0.4986 - val_top5-acc: 0.9378 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 1.4069 - acc: 0.5009 - top5-acc: 0.9301 - val_loss: 1.3599 - val_acc: 0.5224 - val_top5-acc: 0.9406 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 1.3972 - acc: 0.5042 - top5-acc: 0.9302 - val_loss: 1.3301 - val_acc: 0.5282 - val_top5-acc: 0.9414 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 1.3828 - acc: 0.5073 - top5-acc: 0.9336 - val_loss: 1.3505 - val_acc: 0.5200 - val_top5-acc: 0.9428 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.3704 - acc: 0.5133 - top5-acc: 0.9339 - val_loss: 1.3212 - val_acc: 0.5226 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.3649 - acc: 0.5156 - top5-acc: 0.9338 - val_loss: 1.3018 - val_acc: 0.5456 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.3485 - acc: 0.5193 - top5-acc: 0.9370 - val_loss: 1.2768 - val_acc: 0.5476 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 12s 140ms/step - loss: 1.3439 - acc: 0.5253 - top5-acc: 0.9370 - val_loss: 1.2809 - val_acc: 0.5476 - val_top5-acc: 0.9458 - lr: 0.0050\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3483 - acc: 0.5180 - top5-acc: 0.9368 - val_loss: 1.2612 - val_acc: 0.5542 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 12s 134ms/step - loss: 1.3414 - acc: 0.5250 - top5-acc: 0.9379 - val_loss: 1.2820 - val_acc: 0.5426 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 1.3445 - acc: 0.5250 - top5-acc: 0.9379 - val_loss: 1.2681 - val_acc: 0.5558 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 1.3387 - acc: 0.5262 - top5-acc: 0.9366 - val_loss: 1.2500 - val_acc: 0.5588 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3300 - acc: 0.5292 - top5-acc: 0.9385 - val_loss: 1.2641 - val_acc: 0.5536 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.3273 - acc: 0.5283 - top5-acc: 0.9390 - val_loss: 1.2744 - val_acc: 0.5448 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 12s 134ms/step - loss: 1.3241 - acc: 0.5310 - top5-acc: 0.9403 - val_loss: 1.2382 - val_acc: 0.5564 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.3261 - acc: 0.5297 - top5-acc: 0.9398 - val_loss: 1.2453 - val_acc: 0.5580 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 1.3155 - acc: 0.5349 - top5-acc: 0.9394 - val_loss: 1.2445 - val_acc: 0.5544 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3213 - acc: 0.5320 - top5-acc: 0.9395 - val_loss: 1.2752 - val_acc: 0.5430 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 13s 149ms/step - loss: 1.3202 - acc: 0.5306 - top5-acc: 0.9403 - val_loss: 1.2661 - val_acc: 0.5416 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3154 - acc: 0.5337 - top5-acc: 0.9407 - val_loss: 1.2516 - val_acc: 0.5552 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 12s 138ms/step - loss: 1.2976 - acc: 0.5407 - top5-acc: 0.9421 - val_loss: 1.2215 - val_acc: 0.5668 - val_top5-acc: 0.9538 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.2946 - acc: 0.5424 - top5-acc: 0.9437 - val_loss: 1.2268 - val_acc: 0.5548 - val_top5-acc: 0.9536 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.2959 - acc: 0.5412 - top5-acc: 0.9423 - val_loss: 1.2247 - val_acc: 0.5666 - val_top5-acc: 0.9532 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2963 - acc: 0.5386 - top5-acc: 0.9426 - val_loss: 1.2296 - val_acc: 0.5658 - val_top5-acc: 0.9546 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2981 - acc: 0.5406 - top5-acc: 0.9418 - val_loss: 1.2282 - val_acc: 0.5610 - val_top5-acc: 0.9526 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 12s 140ms/step - loss: 1.3027 - acc: 0.5379 - top5-acc: 0.9412 - val_loss: 1.2241 - val_acc: 0.5618 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.2819 - acc: 0.5415 - top5-acc: 0.9444 - val_loss: 1.2146 - val_acc: 0.5720 - val_top5-acc: 0.9554 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2848 - acc: 0.5441 - top5-acc: 0.9431 - val_loss: 1.2128 - val_acc: 0.5698 - val_top5-acc: 0.9544 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2820 - acc: 0.5451 - top5-acc: 0.9423 - val_loss: 1.2184 - val_acc: 0.5676 - val_top5-acc: 0.9560 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2846 - acc: 0.5430 - top5-acc: 0.9428 - val_loss: 1.2092 - val_acc: 0.5750 - val_top5-acc: 0.9542 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.2836 - acc: 0.5428 - top5-acc: 0.9445 - val_loss: 1.2107 - val_acc: 0.5716 - val_top5-acc: 0.9542 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2886 - acc: 0.5442 - top5-acc: 0.9421 - val_loss: 1.2199 - val_acc: 0.5656 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 12s 140ms/step - loss: 1.2856 - acc: 0.5454 - top5-acc: 0.9428 - val_loss: 1.2085 - val_acc: 0.5758 - val_top5-acc: 0.9544 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2844 - acc: 0.5451 - top5-acc: 0.9436 - val_loss: 1.2113 - val_acc: 0.5730 - val_top5-acc: 0.9536 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 13s 148ms/step - loss: 1.2866 - acc: 0.5460 - top5-acc: 0.9435 - val_loss: 1.2227 - val_acc: 0.5724 - val_top5-acc: 0.9542 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.2831 - acc: 0.5460 - top5-acc: 0.9438 - val_loss: 1.2124 - val_acc: 0.5742 - val_top5-acc: 0.9554 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2840 - acc: 0.5469 - top5-acc: 0.9423 - val_loss: 1.2138 - val_acc: 0.5718 - val_top5-acc: 0.9556 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2837 - acc: 0.5462 - top5-acc: 0.9435 - val_loss: 1.2128 - val_acc: 0.5706 - val_top5-acc: 0.9542 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2809 - acc: 0.5469 - top5-acc: 0.9444 - val_loss: 1.2137 - val_acc: 0.5692 - val_top5-acc: 0.9562 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.2825 - acc: 0.5465 - top5-acc: 0.9438 - val_loss: 1.2125 - val_acc: 0.5784 - val_top5-acc: 0.9540 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 12s 138ms/step - loss: 1.2818 - acc: 0.5481 - top5-acc: 0.9425 - val_loss: 1.2191 - val_acc: 0.5678 - val_top5-acc: 0.9536 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 13s 147ms/step - loss: 1.2842 - acc: 0.5457 - top5-acc: 0.9435 - val_loss: 1.2090 - val_acc: 0.5764 - val_top5-acc: 0.9546 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.2793 - acc: 0.5472 - top5-acc: 0.9437 - val_loss: 1.2156 - val_acc: 0.5722 - val_top5-acc: 0.9550 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.2794 - acc: 0.5486 - top5-acc: 0.9436 - val_loss: 1.2123 - val_acc: 0.5730 - val_top5-acc: 0.9570 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 1.2774 - acc: 0.5516 - top5-acc: 0.9448 - val_loss: 1.2127 - val_acc: 0.5750 - val_top5-acc: 0.9564 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.2778 - acc: 0.5493 - top5-acc: 0.9451 - val_loss: 1.2114 - val_acc: 0.5756 - val_top5-acc: 0.9546 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.2778 - acc: 0.5482 - top5-acc: 0.9439 - val_loss: 1.2109 - val_acc: 0.5722 - val_top5-acc: 0.9546 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 13s 147ms/step - loss: 1.2799 - acc: 0.5491 - top5-acc: 0.9430 - val_loss: 1.2136 - val_acc: 0.5724 - val_top5-acc: 0.9554 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 18s 56ms/step - loss: 1.2295 - acc: 0.5673 - top5-acc: 0.9517\n",
      "Test accuracy: 56.73%\n",
      "Test top 5 accuracy: 95.17%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 18s 161ms/step - loss: 1.7381 - acc: 0.3881 - top5-acc: 0.8668 - val_loss: 1.5058 - val_acc: 0.4598 - val_top5-acc: 0.9210 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.4941 - acc: 0.4687 - top5-acc: 0.9183 - val_loss: 1.4077 - val_acc: 0.4982 - val_top5-acc: 0.9322 - lr: 0.0050\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 13s 145ms/step - loss: 1.4320 - acc: 0.4891 - top5-acc: 0.9280 - val_loss: 1.3788 - val_acc: 0.5044 - val_top5-acc: 0.9386 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 14s 155ms/step - loss: 1.4051 - acc: 0.5026 - top5-acc: 0.9311 - val_loss: 1.3415 - val_acc: 0.5174 - val_top5-acc: 0.9414 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.3850 - acc: 0.5065 - top5-acc: 0.9332 - val_loss: 1.3307 - val_acc: 0.5226 - val_top5-acc: 0.9446 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 13s 149ms/step - loss: 1.3717 - acc: 0.5139 - top5-acc: 0.9342 - val_loss: 1.3065 - val_acc: 0.5368 - val_top5-acc: 0.9418 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 13s 150ms/step - loss: 1.3518 - acc: 0.5208 - top5-acc: 0.9368 - val_loss: 1.2987 - val_acc: 0.5360 - val_top5-acc: 0.9426 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 13s 147ms/step - loss: 1.3548 - acc: 0.5189 - top5-acc: 0.9367 - val_loss: 1.2993 - val_acc: 0.5380 - val_top5-acc: 0.9490 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 12s 140ms/step - loss: 1.3411 - acc: 0.5272 - top5-acc: 0.9378 - val_loss: 1.2819 - val_acc: 0.5378 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3381 - acc: 0.5254 - top5-acc: 0.9390 - val_loss: 1.2554 - val_acc: 0.5592 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 13s 148ms/step - loss: 1.3253 - acc: 0.5311 - top5-acc: 0.9405 - val_loss: 1.2884 - val_acc: 0.5386 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 14s 154ms/step - loss: 1.3342 - acc: 0.5254 - top5-acc: 0.9381 - val_loss: 1.2647 - val_acc: 0.5458 - val_top5-acc: 0.9450 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.3230 - acc: 0.5313 - top5-acc: 0.9394 - val_loss: 1.2329 - val_acc: 0.5616 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3177 - acc: 0.5313 - top5-acc: 0.9407 - val_loss: 1.2574 - val_acc: 0.5464 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.3230 - acc: 0.5319 - top5-acc: 0.9400 - val_loss: 1.2549 - val_acc: 0.5566 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.3162 - acc: 0.5334 - top5-acc: 0.9408 - val_loss: 1.2560 - val_acc: 0.5510 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 14s 160ms/step - loss: 1.3134 - acc: 0.5359 - top5-acc: 0.9411 - val_loss: 1.2478 - val_acc: 0.5564 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.3105 - acc: 0.5339 - top5-acc: 0.9412 - val_loss: 1.2357 - val_acc: 0.5616 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 14s 154ms/step - loss: 1.2871 - acc: 0.5425 - top5-acc: 0.9435 - val_loss: 1.2225 - val_acc: 0.5646 - val_top5-acc: 0.9530 - lr: 0.0025\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.2850 - acc: 0.5435 - top5-acc: 0.9441 - val_loss: 1.2270 - val_acc: 0.5602 - val_top5-acc: 0.9512 - lr: 0.0025\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2857 - acc: 0.5443 - top5-acc: 0.9425 - val_loss: 1.2113 - val_acc: 0.5714 - val_top5-acc: 0.9522 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 14s 160ms/step - loss: 1.2883 - acc: 0.5415 - top5-acc: 0.9430 - val_loss: 1.2307 - val_acc: 0.5614 - val_top5-acc: 0.9506 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 13s 149ms/step - loss: 1.2909 - acc: 0.5418 - top5-acc: 0.9439 - val_loss: 1.2208 - val_acc: 0.5664 - val_top5-acc: 0.9512 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.2839 - acc: 0.5472 - top5-acc: 0.9425 - val_loss: 1.2220 - val_acc: 0.5616 - val_top5-acc: 0.9528 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.2857 - acc: 0.5414 - top5-acc: 0.9440 - val_loss: 1.2163 - val_acc: 0.5724 - val_top5-acc: 0.9542 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2811 - acc: 0.5438 - top5-acc: 0.9447 - val_loss: 1.2206 - val_acc: 0.5672 - val_top5-acc: 0.9534 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 13s 150ms/step - loss: 1.2743 - acc: 0.5495 - top5-acc: 0.9442 - val_loss: 1.2069 - val_acc: 0.5774 - val_top5-acc: 0.9542 - lr: 0.0012\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 13s 147ms/step - loss: 1.2710 - acc: 0.5477 - top5-acc: 0.9456 - val_loss: 1.2108 - val_acc: 0.5758 - val_top5-acc: 0.9532 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 14s 154ms/step - loss: 1.2750 - acc: 0.5472 - top5-acc: 0.9440 - val_loss: 1.2046 - val_acc: 0.5736 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 15s 169ms/step - loss: 1.2745 - acc: 0.5472 - top5-acc: 0.9461 - val_loss: 1.2132 - val_acc: 0.5614 - val_top5-acc: 0.9520 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 14s 155ms/step - loss: 1.2670 - acc: 0.5526 - top5-acc: 0.9459 - val_loss: 1.2097 - val_acc: 0.5712 - val_top5-acc: 0.9538 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2760 - acc: 0.5475 - top5-acc: 0.9439 - val_loss: 1.2045 - val_acc: 0.5742 - val_top5-acc: 0.9568 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2777 - acc: 0.5462 - top5-acc: 0.9443 - val_loss: 1.2063 - val_acc: 0.5748 - val_top5-acc: 0.9566 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2762 - acc: 0.5492 - top5-acc: 0.9444 - val_loss: 1.2097 - val_acc: 0.5708 - val_top5-acc: 0.9548 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 14s 154ms/step - loss: 1.2712 - acc: 0.5508 - top5-acc: 0.9447 - val_loss: 1.2084 - val_acc: 0.5690 - val_top5-acc: 0.9544 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 14s 155ms/step - loss: 1.2774 - acc: 0.5508 - top5-acc: 0.9451 - val_loss: 1.2097 - val_acc: 0.5708 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2749 - acc: 0.5478 - top5-acc: 0.9451 - val_loss: 1.2206 - val_acc: 0.5606 - val_top5-acc: 0.9542 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2718 - acc: 0.5506 - top5-acc: 0.9442 - val_loss: 1.2027 - val_acc: 0.5762 - val_top5-acc: 0.9554 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2674 - acc: 0.5523 - top5-acc: 0.9460 - val_loss: 1.2040 - val_acc: 0.5718 - val_top5-acc: 0.9558 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2707 - acc: 0.5494 - top5-acc: 0.9455 - val_loss: 1.2038 - val_acc: 0.5738 - val_top5-acc: 0.9556 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.2699 - acc: 0.5497 - top5-acc: 0.9450 - val_loss: 1.2032 - val_acc: 0.5744 - val_top5-acc: 0.9556 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.2674 - acc: 0.5531 - top5-acc: 0.9447 - val_loss: 1.2050 - val_acc: 0.5742 - val_top5-acc: 0.9570 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 13s 147ms/step - loss: 1.2704 - acc: 0.5478 - top5-acc: 0.9449 - val_loss: 1.2069 - val_acc: 0.5712 - val_top5-acc: 0.9536 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.2668 - acc: 0.5527 - top5-acc: 0.9450 - val_loss: 1.2034 - val_acc: 0.5768 - val_top5-acc: 0.9552 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 13s 149ms/step - loss: 1.2679 - acc: 0.5502 - top5-acc: 0.9447 - val_loss: 1.2080 - val_acc: 0.5712 - val_top5-acc: 0.9552 - lr: 3.1250e-04\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 13s 149ms/step - loss: 1.2663 - acc: 0.5526 - top5-acc: 0.9458 - val_loss: 1.2047 - val_acc: 0.5760 - val_top5-acc: 0.9566 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 13s 142ms/step - loss: 1.2664 - acc: 0.5527 - top5-acc: 0.9466 - val_loss: 1.2067 - val_acc: 0.5728 - val_top5-acc: 0.9542 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 13s 142ms/step - loss: 1.2692 - acc: 0.5514 - top5-acc: 0.9469 - val_loss: 1.2075 - val_acc: 0.5744 - val_top5-acc: 0.9552 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2711 - acc: 0.5517 - top5-acc: 0.9450 - val_loss: 1.2072 - val_acc: 0.5750 - val_top5-acc: 0.9544 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.2695 - acc: 0.5536 - top5-acc: 0.9457 - val_loss: 1.2072 - val_acc: 0.5748 - val_top5-acc: 0.9564 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 19s 59ms/step - loss: 1.2261 - acc: 0.5684 - top5-acc: 0.9522\n",
      "Test accuracy: 56.84%\n",
      "Test top 5 accuracy: 95.22%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 17s 155ms/step - loss: 1.7375 - acc: 0.3934 - top5-acc: 0.8688 - val_loss: 1.4803 - val_acc: 0.4580 - val_top5-acc: 0.9260 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.4601 - acc: 0.4806 - top5-acc: 0.9234 - val_loss: 1.3696 - val_acc: 0.5072 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 1.4126 - acc: 0.4956 - top5-acc: 0.9297 - val_loss: 1.3717 - val_acc: 0.5054 - val_top5-acc: 0.9386 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.3828 - acc: 0.5091 - top5-acc: 0.9327 - val_loss: 1.3148 - val_acc: 0.5346 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.3635 - acc: 0.5152 - top5-acc: 0.9347 - val_loss: 1.2939 - val_acc: 0.5402 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.3488 - acc: 0.5204 - top5-acc: 0.9367 - val_loss: 1.2877 - val_acc: 0.5382 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.3375 - acc: 0.5242 - top5-acc: 0.9372 - val_loss: 1.3050 - val_acc: 0.5270 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.3381 - acc: 0.5239 - top5-acc: 0.9378 - val_loss: 1.2752 - val_acc: 0.5366 - val_top5-acc: 0.9488 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 13s 150ms/step - loss: 1.3243 - acc: 0.5329 - top5-acc: 0.9395 - val_loss: 1.2801 - val_acc: 0.5408 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 13s 149ms/step - loss: 1.3222 - acc: 0.5325 - top5-acc: 0.9392 - val_loss: 1.2522 - val_acc: 0.5518 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.3051 - acc: 0.5366 - top5-acc: 0.9412 - val_loss: 1.2395 - val_acc: 0.5508 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 13s 148ms/step - loss: 1.3142 - acc: 0.5337 - top5-acc: 0.9413 - val_loss: 1.2459 - val_acc: 0.5542 - val_top5-acc: 0.9490 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.3051 - acc: 0.5387 - top5-acc: 0.9414 - val_loss: 1.2392 - val_acc: 0.5512 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 13s 151ms/step - loss: 1.3025 - acc: 0.5358 - top5-acc: 0.9419 - val_loss: 1.2253 - val_acc: 0.5602 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.2969 - acc: 0.5386 - top5-acc: 0.9412 - val_loss: 1.2569 - val_acc: 0.5466 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 12s 140ms/step - loss: 1.2979 - acc: 0.5399 - top5-acc: 0.9418 - val_loss: 1.2360 - val_acc: 0.5574 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.2917 - acc: 0.5401 - top5-acc: 0.9438 - val_loss: 1.2131 - val_acc: 0.5652 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2879 - acc: 0.5398 - top5-acc: 0.9434 - val_loss: 1.2336 - val_acc: 0.5542 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2948 - acc: 0.5406 - top5-acc: 0.9429 - val_loss: 1.2112 - val_acc: 0.5696 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2926 - acc: 0.5394 - top5-acc: 0.9434 - val_loss: 1.2339 - val_acc: 0.5578 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 13s 149ms/step - loss: 1.2903 - acc: 0.5436 - top5-acc: 0.9427 - val_loss: 1.2189 - val_acc: 0.5598 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 12s 139ms/step - loss: 1.2852 - acc: 0.5438 - top5-acc: 0.9428 - val_loss: 1.2360 - val_acc: 0.5574 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.2889 - acc: 0.5425 - top5-acc: 0.9428 - val_loss: 1.2229 - val_acc: 0.5624 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2889 - acc: 0.5429 - top5-acc: 0.9429 - val_loss: 1.2043 - val_acc: 0.5730 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2799 - acc: 0.5473 - top5-acc: 0.9440 - val_loss: 1.2076 - val_acc: 0.5660 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2820 - acc: 0.5456 - top5-acc: 0.9433 - val_loss: 1.2383 - val_acc: 0.5582 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2849 - acc: 0.5420 - top5-acc: 0.9431 - val_loss: 1.2025 - val_acc: 0.5762 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 12s 138ms/step - loss: 1.2909 - acc: 0.5428 - top5-acc: 0.9430 - val_loss: 1.2085 - val_acc: 0.5654 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2749 - acc: 0.5464 - top5-acc: 0.9453 - val_loss: 1.1892 - val_acc: 0.5718 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2765 - acc: 0.5452 - top5-acc: 0.9446 - val_loss: 1.2018 - val_acc: 0.5658 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2739 - acc: 0.5478 - top5-acc: 0.9433 - val_loss: 1.2127 - val_acc: 0.5628 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 12s 142ms/step - loss: 1.2819 - acc: 0.5447 - top5-acc: 0.9435 - val_loss: 1.2156 - val_acc: 0.5696 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 12s 141ms/step - loss: 1.2808 - acc: 0.5472 - top5-acc: 0.9445 - val_loss: 1.2412 - val_acc: 0.5608 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2810 - acc: 0.5450 - top5-acc: 0.9435 - val_loss: 1.1991 - val_acc: 0.5684 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2588 - acc: 0.5523 - top5-acc: 0.9460 - val_loss: 1.1898 - val_acc: 0.5724 - val_top5-acc: 0.9592 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2574 - acc: 0.5567 - top5-acc: 0.9464 - val_loss: 1.1921 - val_acc: 0.5786 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2586 - acc: 0.5542 - top5-acc: 0.9447 - val_loss: 1.1906 - val_acc: 0.5750 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2540 - acc: 0.5539 - top5-acc: 0.9476 - val_loss: 1.1861 - val_acc: 0.5712 - val_top5-acc: 0.9540 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2606 - acc: 0.5526 - top5-acc: 0.9467 - val_loss: 1.2009 - val_acc: 0.5674 - val_top5-acc: 0.9542 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2659 - acc: 0.5500 - top5-acc: 0.9457 - val_loss: 1.1863 - val_acc: 0.5804 - val_top5-acc: 0.9576 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 12s 137ms/step - loss: 1.2566 - acc: 0.5529 - top5-acc: 0.9464 - val_loss: 1.1994 - val_acc: 0.5702 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2545 - acc: 0.5558 - top5-acc: 0.9458 - val_loss: 1.1810 - val_acc: 0.5730 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2547 - acc: 0.5548 - top5-acc: 0.9480 - val_loss: 1.2018 - val_acc: 0.5746 - val_top5-acc: 0.9546 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2589 - acc: 0.5512 - top5-acc: 0.9461 - val_loss: 1.1853 - val_acc: 0.5734 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 1.2616 - acc: 0.5524 - top5-acc: 0.9454 - val_loss: 1.1826 - val_acc: 0.5760 - val_top5-acc: 0.9544 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 1.2581 - acc: 0.5526 - top5-acc: 0.9465 - val_loss: 1.1767 - val_acc: 0.5792 - val_top5-acc: 0.9570 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 1.2598 - acc: 0.5512 - top5-acc: 0.9448 - val_loss: 1.1940 - val_acc: 0.5756 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 1.2667 - acc: 0.5514 - top5-acc: 0.9445 - val_loss: 1.1895 - val_acc: 0.5730 - val_top5-acc: 0.9576 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 1.2549 - acc: 0.5541 - top5-acc: 0.9463 - val_loss: 1.1914 - val_acc: 0.5744 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 12s 136ms/step - loss: 1.2571 - acc: 0.5542 - top5-acc: 0.9450 - val_loss: 1.1881 - val_acc: 0.5778 - val_top5-acc: 0.9538 - lr: 0.0025\n",
      "313/313 [==============================] - 19s 61ms/step - loss: 1.2068 - acc: 0.5693 - top5-acc: 0.9513\n",
      "Test accuracy: 56.93%\n",
      "Test top 5 accuracy: 95.13%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 18s 156ms/step - loss: 1.6838 - acc: 0.4016 - top5-acc: 0.8766 - val_loss: 1.4475 - val_acc: 0.4836 - val_top5-acc: 0.9282 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.4467 - acc: 0.4842 - top5-acc: 0.9236 - val_loss: 1.3963 - val_acc: 0.5006 - val_top5-acc: 0.9318 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.3938 - acc: 0.5042 - top5-acc: 0.9312 - val_loss: 1.3297 - val_acc: 0.5254 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.3701 - acc: 0.5129 - top5-acc: 0.9342 - val_loss: 1.3146 - val_acc: 0.5266 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3478 - acc: 0.5227 - top5-acc: 0.9358 - val_loss: 1.3057 - val_acc: 0.5302 - val_top5-acc: 0.9470 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3340 - acc: 0.5270 - top5-acc: 0.9388 - val_loss: 1.2761 - val_acc: 0.5402 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.3227 - acc: 0.5309 - top5-acc: 0.9392 - val_loss: 1.2543 - val_acc: 0.5554 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.3134 - acc: 0.5326 - top5-acc: 0.9400 - val_loss: 1.2477 - val_acc: 0.5528 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.3068 - acc: 0.5354 - top5-acc: 0.9416 - val_loss: 1.2388 - val_acc: 0.5540 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.3026 - acc: 0.5384 - top5-acc: 0.9400 - val_loss: 1.2521 - val_acc: 0.5496 - val_top5-acc: 0.9488 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2921 - acc: 0.5410 - top5-acc: 0.9438 - val_loss: 1.2285 - val_acc: 0.5632 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2928 - acc: 0.5425 - top5-acc: 0.9422 - val_loss: 1.2359 - val_acc: 0.5574 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2927 - acc: 0.5425 - top5-acc: 0.9427 - val_loss: 1.2229 - val_acc: 0.5614 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2934 - acc: 0.5432 - top5-acc: 0.9407 - val_loss: 1.2241 - val_acc: 0.5574 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2895 - acc: 0.5438 - top5-acc: 0.9431 - val_loss: 1.2214 - val_acc: 0.5626 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2817 - acc: 0.5442 - top5-acc: 0.9436 - val_loss: 1.2120 - val_acc: 0.5670 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2772 - acc: 0.5492 - top5-acc: 0.9431 - val_loss: 1.2151 - val_acc: 0.5644 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2841 - acc: 0.5449 - top5-acc: 0.9456 - val_loss: 1.2034 - val_acc: 0.5702 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2852 - acc: 0.5447 - top5-acc: 0.9430 - val_loss: 1.2296 - val_acc: 0.5558 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2786 - acc: 0.5458 - top5-acc: 0.9438 - val_loss: 1.2043 - val_acc: 0.5654 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2767 - acc: 0.5445 - top5-acc: 0.9450 - val_loss: 1.2093 - val_acc: 0.5632 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2772 - acc: 0.5472 - top5-acc: 0.9447 - val_loss: 1.1873 - val_acc: 0.5788 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2747 - acc: 0.5486 - top5-acc: 0.9448 - val_loss: 1.2147 - val_acc: 0.5582 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2803 - acc: 0.5452 - top5-acc: 0.9450 - val_loss: 1.2081 - val_acc: 0.5636 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2691 - acc: 0.5480 - top5-acc: 0.9451 - val_loss: 1.2253 - val_acc: 0.5618 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2787 - acc: 0.5472 - top5-acc: 0.9441 - val_loss: 1.2095 - val_acc: 0.5630 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2718 - acc: 0.5495 - top5-acc: 0.9453 - val_loss: 1.1954 - val_acc: 0.5652 - val_top5-acc: 0.9578 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2565 - acc: 0.5542 - top5-acc: 0.9466 - val_loss: 1.1789 - val_acc: 0.5736 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2465 - acc: 0.5566 - top5-acc: 0.9478 - val_loss: 1.1755 - val_acc: 0.5772 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2527 - acc: 0.5590 - top5-acc: 0.9456 - val_loss: 1.1850 - val_acc: 0.5778 - val_top5-acc: 0.9566 - lr: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2518 - acc: 0.5559 - top5-acc: 0.9478 - val_loss: 1.1731 - val_acc: 0.5798 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2525 - acc: 0.5568 - top5-acc: 0.9463 - val_loss: 1.1928 - val_acc: 0.5740 - val_top5-acc: 0.9556 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2495 - acc: 0.5580 - top5-acc: 0.9465 - val_loss: 1.1919 - val_acc: 0.5730 - val_top5-acc: 0.9556 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2516 - acc: 0.5550 - top5-acc: 0.9465 - val_loss: 1.1799 - val_acc: 0.5790 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2516 - acc: 0.5581 - top5-acc: 0.9472 - val_loss: 1.1849 - val_acc: 0.5722 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2457 - acc: 0.5587 - top5-acc: 0.9462 - val_loss: 1.1719 - val_acc: 0.5774 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2547 - acc: 0.5550 - top5-acc: 0.9476 - val_loss: 1.1717 - val_acc: 0.5792 - val_top5-acc: 0.9582 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 13s 146ms/step - loss: 1.2506 - acc: 0.5572 - top5-acc: 0.9459 - val_loss: 1.1750 - val_acc: 0.5806 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2516 - acc: 0.5575 - top5-acc: 0.9464 - val_loss: 1.1761 - val_acc: 0.5818 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2557 - acc: 0.5551 - top5-acc: 0.9460 - val_loss: 1.1972 - val_acc: 0.5728 - val_top5-acc: 0.9536 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2516 - acc: 0.5569 - top5-acc: 0.9477 - val_loss: 1.1840 - val_acc: 0.5736 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2509 - acc: 0.5584 - top5-acc: 0.9466 - val_loss: 1.1853 - val_acc: 0.5810 - val_top5-acc: 0.9536 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 13s 143ms/step - loss: 1.2409 - acc: 0.5588 - top5-acc: 0.9473 - val_loss: 1.1687 - val_acc: 0.5802 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2388 - acc: 0.5604 - top5-acc: 0.9478 - val_loss: 1.1670 - val_acc: 0.5814 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2395 - acc: 0.5633 - top5-acc: 0.9477 - val_loss: 1.1700 - val_acc: 0.5814 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 13s 144ms/step - loss: 1.2421 - acc: 0.5591 - top5-acc: 0.9480 - val_loss: 1.1684 - val_acc: 0.5834 - val_top5-acc: 0.9568 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2434 - acc: 0.5602 - top5-acc: 0.9483 - val_loss: 1.1611 - val_acc: 0.5848 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2389 - acc: 0.5618 - top5-acc: 0.9479 - val_loss: 1.1710 - val_acc: 0.5832 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2395 - acc: 0.5622 - top5-acc: 0.9483 - val_loss: 1.1712 - val_acc: 0.5848 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 13s 145ms/step - loss: 1.2413 - acc: 0.5603 - top5-acc: 0.9478 - val_loss: 1.1732 - val_acc: 0.5814 - val_top5-acc: 0.9566 - lr: 0.0012\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 1.1883 - acc: 0.5802 - top5-acc: 0.9556\n",
      "Test accuracy: 58.02%\n",
      "Test top 5 accuracy: 95.56%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 19s 167ms/step - loss: 1.6851 - acc: 0.4019 - top5-acc: 0.8736 - val_loss: 1.4495 - val_acc: 0.4704 - val_top5-acc: 0.9286 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 13s 152ms/step - loss: 1.4452 - acc: 0.4870 - top5-acc: 0.9232 - val_loss: 1.3696 - val_acc: 0.5096 - val_top5-acc: 0.9338 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.3960 - acc: 0.5052 - top5-acc: 0.9302 - val_loss: 1.3388 - val_acc: 0.5200 - val_top5-acc: 0.9422 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.3679 - acc: 0.5163 - top5-acc: 0.9341 - val_loss: 1.2939 - val_acc: 0.5342 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.3431 - acc: 0.5248 - top5-acc: 0.9370 - val_loss: 1.2805 - val_acc: 0.5428 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.3325 - acc: 0.5259 - top5-acc: 0.9376 - val_loss: 1.2781 - val_acc: 0.5376 - val_top5-acc: 0.9480 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 14s 156ms/step - loss: 1.3148 - acc: 0.5345 - top5-acc: 0.9395 - val_loss: 1.2625 - val_acc: 0.5434 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.3168 - acc: 0.5309 - top5-acc: 0.9406 - val_loss: 1.2423 - val_acc: 0.5582 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 14s 160ms/step - loss: 1.3021 - acc: 0.5378 - top5-acc: 0.9418 - val_loss: 1.2566 - val_acc: 0.5468 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.3024 - acc: 0.5370 - top5-acc: 0.9411 - val_loss: 1.2270 - val_acc: 0.5634 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2934 - acc: 0.5411 - top5-acc: 0.9436 - val_loss: 1.2385 - val_acc: 0.5540 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2943 - acc: 0.5404 - top5-acc: 0.9418 - val_loss: 1.2214 - val_acc: 0.5514 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2881 - acc: 0.5420 - top5-acc: 0.9438 - val_loss: 1.2365 - val_acc: 0.5546 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 14s 154ms/step - loss: 1.2902 - acc: 0.5398 - top5-acc: 0.9428 - val_loss: 1.2171 - val_acc: 0.5620 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2872 - acc: 0.5425 - top5-acc: 0.9440 - val_loss: 1.2106 - val_acc: 0.5622 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.2790 - acc: 0.5478 - top5-acc: 0.9451 - val_loss: 1.2092 - val_acc: 0.5646 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2796 - acc: 0.5474 - top5-acc: 0.9448 - val_loss: 1.2224 - val_acc: 0.5604 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.2775 - acc: 0.5456 - top5-acc: 0.9449 - val_loss: 1.2002 - val_acc: 0.5660 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 14s 156ms/step - loss: 1.2740 - acc: 0.5486 - top5-acc: 0.9438 - val_loss: 1.2206 - val_acc: 0.5618 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.2716 - acc: 0.5504 - top5-acc: 0.9443 - val_loss: 1.2200 - val_acc: 0.5634 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.2758 - acc: 0.5462 - top5-acc: 0.9445 - val_loss: 1.2067 - val_acc: 0.5716 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 14s 154ms/step - loss: 1.2707 - acc: 0.5501 - top5-acc: 0.9451 - val_loss: 1.1882 - val_acc: 0.5722 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 15s 166ms/step - loss: 1.2713 - acc: 0.5484 - top5-acc: 0.9450 - val_loss: 1.2032 - val_acc: 0.5664 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 14s 156ms/step - loss: 1.2767 - acc: 0.5477 - top5-acc: 0.9433 - val_loss: 1.1965 - val_acc: 0.5738 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 14s 155ms/step - loss: 1.2660 - acc: 0.5509 - top5-acc: 0.9453 - val_loss: 1.2105 - val_acc: 0.5628 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.2658 - acc: 0.5537 - top5-acc: 0.9465 - val_loss: 1.1940 - val_acc: 0.5730 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2643 - acc: 0.5526 - top5-acc: 0.9465 - val_loss: 1.1794 - val_acc: 0.5772 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 14s 155ms/step - loss: 1.2713 - acc: 0.5461 - top5-acc: 0.9446 - val_loss: 1.1900 - val_acc: 0.5736 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 14s 155ms/step - loss: 1.2644 - acc: 0.5495 - top5-acc: 0.9466 - val_loss: 1.2037 - val_acc: 0.5706 - val_top5-acc: 0.9580 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2654 - acc: 0.5501 - top5-acc: 0.9444 - val_loss: 1.1926 - val_acc: 0.5682 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 1.2646 - acc: 0.5521 - top5-acc: 0.9470 - val_loss: 1.1765 - val_acc: 0.5774 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2622 - acc: 0.5513 - top5-acc: 0.9451 - val_loss: 1.1816 - val_acc: 0.5722 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 14s 160ms/step - loss: 1.2648 - acc: 0.5513 - top5-acc: 0.9458 - val_loss: 1.1839 - val_acc: 0.5752 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2635 - acc: 0.5510 - top5-acc: 0.9445 - val_loss: 1.1725 - val_acc: 0.5780 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2630 - acc: 0.5508 - top5-acc: 0.9461 - val_loss: 1.1969 - val_acc: 0.5712 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2684 - acc: 0.5503 - top5-acc: 0.9465 - val_loss: 1.1897 - val_acc: 0.5732 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2633 - acc: 0.5519 - top5-acc: 0.9463 - val_loss: 1.1919 - val_acc: 0.5700 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 1.2627 - acc: 0.5513 - top5-acc: 0.9462 - val_loss: 1.1941 - val_acc: 0.5688 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.2629 - acc: 0.5526 - top5-acc: 0.9463 - val_loss: 1.1783 - val_acc: 0.5702 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2397 - acc: 0.5608 - top5-acc: 0.9481 - val_loss: 1.1608 - val_acc: 0.5834 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 14s 156ms/step - loss: 1.2398 - acc: 0.5611 - top5-acc: 0.9498 - val_loss: 1.1688 - val_acc: 0.5804 - val_top5-acc: 0.9570 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.2418 - acc: 0.5628 - top5-acc: 0.9461 - val_loss: 1.1669 - val_acc: 0.5830 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 14s 163ms/step - loss: 1.2386 - acc: 0.5609 - top5-acc: 0.9485 - val_loss: 1.1583 - val_acc: 0.5852 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2408 - acc: 0.5622 - top5-acc: 0.9471 - val_loss: 1.1734 - val_acc: 0.5784 - val_top5-acc: 0.9580 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2472 - acc: 0.5567 - top5-acc: 0.9482 - val_loss: 1.1721 - val_acc: 0.5744 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2409 - acc: 0.5620 - top5-acc: 0.9483 - val_loss: 1.1636 - val_acc: 0.5854 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 14s 156ms/step - loss: 1.2401 - acc: 0.5621 - top5-acc: 0.9472 - val_loss: 1.1612 - val_acc: 0.5828 - val_top5-acc: 0.9568 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2433 - acc: 0.5609 - top5-acc: 0.9481 - val_loss: 1.1594 - val_acc: 0.5810 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.2370 - acc: 0.5611 - top5-acc: 0.9472 - val_loss: 1.1613 - val_acc: 0.5838 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 13s 153ms/step - loss: 1.2299 - acc: 0.5664 - top5-acc: 0.9500 - val_loss: 1.1666 - val_acc: 0.5848 - val_top5-acc: 0.9568 - lr: 0.0012\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 1.1874 - acc: 0.5810 - top5-acc: 0.9559\n",
      "Test accuracy: 58.1%\n",
      "Test top 5 accuracy: 95.59%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 22s 196ms/step - loss: 1.7106 - acc: 0.3995 - top5-acc: 0.8692 - val_loss: 1.4451 - val_acc: 0.4768 - val_top5-acc: 0.9300 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.4356 - acc: 0.4921 - top5-acc: 0.9250 - val_loss: 1.3398 - val_acc: 0.5230 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 15s 169ms/step - loss: 1.3857 - acc: 0.5073 - top5-acc: 0.9333 - val_loss: 1.3152 - val_acc: 0.5280 - val_top5-acc: 0.9446 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.3474 - acc: 0.5202 - top5-acc: 0.9371 - val_loss: 1.2820 - val_acc: 0.5428 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 1.3314 - acc: 0.5277 - top5-acc: 0.9392 - val_loss: 1.2716 - val_acc: 0.5426 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 16s 178ms/step - loss: 1.3234 - acc: 0.5305 - top5-acc: 0.9394 - val_loss: 1.2521 - val_acc: 0.5512 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.3081 - acc: 0.5359 - top5-acc: 0.9414 - val_loss: 1.2476 - val_acc: 0.5500 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2959 - acc: 0.5376 - top5-acc: 0.9434 - val_loss: 1.2636 - val_acc: 0.5516 - val_top5-acc: 0.9488 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2935 - acc: 0.5413 - top5-acc: 0.9425 - val_loss: 1.2229 - val_acc: 0.5628 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 15s 170ms/step - loss: 1.2831 - acc: 0.5440 - top5-acc: 0.9440 - val_loss: 1.2279 - val_acc: 0.5576 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 15s 169ms/step - loss: 1.2770 - acc: 0.5490 - top5-acc: 0.9429 - val_loss: 1.2221 - val_acc: 0.5568 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2837 - acc: 0.5433 - top5-acc: 0.9448 - val_loss: 1.1998 - val_acc: 0.5666 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2758 - acc: 0.5483 - top5-acc: 0.9449 - val_loss: 1.2219 - val_acc: 0.5568 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 1.2671 - acc: 0.5496 - top5-acc: 0.9464 - val_loss: 1.1939 - val_acc: 0.5774 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.2773 - acc: 0.5469 - top5-acc: 0.9444 - val_loss: 1.2031 - val_acc: 0.5656 - val_top5-acc: 0.9550 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2657 - acc: 0.5499 - top5-acc: 0.9459 - val_loss: 1.1997 - val_acc: 0.5716 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 14s 162ms/step - loss: 1.2654 - acc: 0.5497 - top5-acc: 0.9450 - val_loss: 1.2331 - val_acc: 0.5566 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 1.2632 - acc: 0.5508 - top5-acc: 0.9463 - val_loss: 1.2017 - val_acc: 0.5668 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 1.2632 - acc: 0.5503 - top5-acc: 0.9458 - val_loss: 1.1882 - val_acc: 0.5654 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2529 - acc: 0.5555 - top5-acc: 0.9476 - val_loss: 1.1921 - val_acc: 0.5704 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 15s 169ms/step - loss: 1.2659 - acc: 0.5512 - top5-acc: 0.9444 - val_loss: 1.1787 - val_acc: 0.5770 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2621 - acc: 0.5537 - top5-acc: 0.9465 - val_loss: 1.1924 - val_acc: 0.5666 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 15s 170ms/step - loss: 1.2586 - acc: 0.5515 - top5-acc: 0.9475 - val_loss: 1.1918 - val_acc: 0.5684 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 16s 176ms/step - loss: 1.2567 - acc: 0.5523 - top5-acc: 0.9458 - val_loss: 1.2080 - val_acc: 0.5680 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 16s 178ms/step - loss: 1.2536 - acc: 0.5553 - top5-acc: 0.9484 - val_loss: 1.1907 - val_acc: 0.5658 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2555 - acc: 0.5531 - top5-acc: 0.9462 - val_loss: 1.1668 - val_acc: 0.5834 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.2500 - acc: 0.5554 - top5-acc: 0.9476 - val_loss: 1.1904 - val_acc: 0.5716 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2554 - acc: 0.5569 - top5-acc: 0.9466 - val_loss: 1.1702 - val_acc: 0.5798 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 14s 159ms/step - loss: 1.2538 - acc: 0.5534 - top5-acc: 0.9481 - val_loss: 1.1948 - val_acc: 0.5698 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2466 - acc: 0.5582 - top5-acc: 0.9485 - val_loss: 1.1887 - val_acc: 0.5690 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 14s 161ms/step - loss: 1.2535 - acc: 0.5535 - top5-acc: 0.9480 - val_loss: 1.1796 - val_acc: 0.5730 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 14s 156ms/step - loss: 1.2338 - acc: 0.5621 - top5-acc: 0.9493 - val_loss: 1.1522 - val_acc: 0.5834 - val_top5-acc: 0.9582 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2312 - acc: 0.5644 - top5-acc: 0.9488 - val_loss: 1.1708 - val_acc: 0.5772 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2331 - acc: 0.5592 - top5-acc: 0.9504 - val_loss: 1.1579 - val_acc: 0.5882 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2315 - acc: 0.5638 - top5-acc: 0.9490 - val_loss: 1.1576 - val_acc: 0.5870 - val_top5-acc: 0.9580 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2312 - acc: 0.5632 - top5-acc: 0.9505 - val_loss: 1.1646 - val_acc: 0.5804 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2273 - acc: 0.5655 - top5-acc: 0.9489 - val_loss: 1.1547 - val_acc: 0.5854 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2223 - acc: 0.5663 - top5-acc: 0.9498 - val_loss: 1.1514 - val_acc: 0.5862 - val_top5-acc: 0.9564 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2242 - acc: 0.5638 - top5-acc: 0.9499 - val_loss: 1.1527 - val_acc: 0.5860 - val_top5-acc: 0.9566 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2253 - acc: 0.5648 - top5-acc: 0.9500 - val_loss: 1.1515 - val_acc: 0.5908 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2183 - acc: 0.5701 - top5-acc: 0.9510 - val_loss: 1.1624 - val_acc: 0.5808 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2200 - acc: 0.5661 - top5-acc: 0.9484 - val_loss: 1.1561 - val_acc: 0.5864 - val_top5-acc: 0.9572 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2234 - acc: 0.5662 - top5-acc: 0.9498 - val_loss: 1.1590 - val_acc: 0.5832 - val_top5-acc: 0.9568 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2219 - acc: 0.5688 - top5-acc: 0.9496 - val_loss: 1.1482 - val_acc: 0.5910 - val_top5-acc: 0.9576 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2203 - acc: 0.5673 - top5-acc: 0.9506 - val_loss: 1.1583 - val_acc: 0.5862 - val_top5-acc: 0.9566 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 14s 157ms/step - loss: 1.2205 - acc: 0.5688 - top5-acc: 0.9508 - val_loss: 1.1568 - val_acc: 0.5866 - val_top5-acc: 0.9586 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2195 - acc: 0.5662 - top5-acc: 0.9500 - val_loss: 1.1531 - val_acc: 0.5878 - val_top5-acc: 0.9584 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 14s 158ms/step - loss: 1.2209 - acc: 0.5692 - top5-acc: 0.9495 - val_loss: 1.1532 - val_acc: 0.5890 - val_top5-acc: 0.9566 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.2238 - acc: 0.5657 - top5-acc: 0.9499 - val_loss: 1.1579 - val_acc: 0.5870 - val_top5-acc: 0.9554 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 14s 160ms/step - loss: 1.2211 - acc: 0.5681 - top5-acc: 0.9490 - val_loss: 1.1518 - val_acc: 0.5908 - val_top5-acc: 0.9574 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 1.1628 - acc: 0.5874 - top5-acc: 0.9575\n",
      "Test accuracy: 58.74%\n",
      "Test top 5 accuracy: 95.75%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 24s 215ms/step - loss: 1.7018 - acc: 0.4013 - top5-acc: 0.8697 - val_loss: 1.4353 - val_acc: 0.4844 - val_top5-acc: 0.9280 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.4492 - acc: 0.4860 - top5-acc: 0.9226 - val_loss: 1.3612 - val_acc: 0.5146 - val_top5-acc: 0.9366 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.3895 - acc: 0.5049 - top5-acc: 0.9322 - val_loss: 1.3052 - val_acc: 0.5362 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.3527 - acc: 0.5188 - top5-acc: 0.9355 - val_loss: 1.2807 - val_acc: 0.5444 - val_top5-acc: 0.9470 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.3334 - acc: 0.5286 - top5-acc: 0.9374 - val_loss: 1.2642 - val_acc: 0.5526 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.3200 - acc: 0.5319 - top5-acc: 0.9390 - val_loss: 1.2536 - val_acc: 0.5514 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.3105 - acc: 0.5353 - top5-acc: 0.9403 - val_loss: 1.2379 - val_acc: 0.5598 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 18s 200ms/step - loss: 1.2984 - acc: 0.5404 - top5-acc: 0.9410 - val_loss: 1.2609 - val_acc: 0.5470 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.2893 - acc: 0.5444 - top5-acc: 0.9436 - val_loss: 1.2281 - val_acc: 0.5650 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 18s 201ms/step - loss: 1.2933 - acc: 0.5413 - top5-acc: 0.9421 - val_loss: 1.2077 - val_acc: 0.5754 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 18s 202ms/step - loss: 1.2793 - acc: 0.5469 - top5-acc: 0.9435 - val_loss: 1.2372 - val_acc: 0.5564 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 18s 202ms/step - loss: 1.2801 - acc: 0.5451 - top5-acc: 0.9440 - val_loss: 1.2116 - val_acc: 0.5630 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 18s 201ms/step - loss: 1.2698 - acc: 0.5499 - top5-acc: 0.9454 - val_loss: 1.2051 - val_acc: 0.5674 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 16s 177ms/step - loss: 1.2672 - acc: 0.5511 - top5-acc: 0.9449 - val_loss: 1.2109 - val_acc: 0.5716 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 15s 167ms/step - loss: 1.2731 - acc: 0.5500 - top5-acc: 0.9449 - val_loss: 1.2036 - val_acc: 0.5688 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 15s 168ms/step - loss: 1.2696 - acc: 0.5515 - top5-acc: 0.9440 - val_loss: 1.2021 - val_acc: 0.5768 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2672 - acc: 0.5518 - top5-acc: 0.9454 - val_loss: 1.1952 - val_acc: 0.5692 - val_top5-acc: 0.9580 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2569 - acc: 0.5541 - top5-acc: 0.9476 - val_loss: 1.1892 - val_acc: 0.5802 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2545 - acc: 0.5563 - top5-acc: 0.9464 - val_loss: 1.1961 - val_acc: 0.5764 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2572 - acc: 0.5544 - top5-acc: 0.9472 - val_loss: 1.1741 - val_acc: 0.5772 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2548 - acc: 0.5534 - top5-acc: 0.9464 - val_loss: 1.1863 - val_acc: 0.5748 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2600 - acc: 0.5547 - top5-acc: 0.9464 - val_loss: 1.1809 - val_acc: 0.5740 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2584 - acc: 0.5538 - top5-acc: 0.9469 - val_loss: 1.1972 - val_acc: 0.5732 - val_top5-acc: 0.9572 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 16s 177ms/step - loss: 1.2559 - acc: 0.5536 - top5-acc: 0.9470 - val_loss: 1.1746 - val_acc: 0.5758 - val_top5-acc: 0.9588 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2573 - acc: 0.5556 - top5-acc: 0.9467 - val_loss: 1.2038 - val_acc: 0.5626 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2372 - acc: 0.5638 - top5-acc: 0.9470 - val_loss: 1.1763 - val_acc: 0.5810 - val_top5-acc: 0.9586 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2334 - acc: 0.5630 - top5-acc: 0.9485 - val_loss: 1.1600 - val_acc: 0.5808 - val_top5-acc: 0.9588 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2406 - acc: 0.5616 - top5-acc: 0.9476 - val_loss: 1.1728 - val_acc: 0.5802 - val_top5-acc: 0.9592 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2367 - acc: 0.5616 - top5-acc: 0.9487 - val_loss: 1.1653 - val_acc: 0.5824 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2347 - acc: 0.5638 - top5-acc: 0.9478 - val_loss: 1.1643 - val_acc: 0.5830 - val_top5-acc: 0.9586 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2351 - acc: 0.5626 - top5-acc: 0.9480 - val_loss: 1.1670 - val_acc: 0.5822 - val_top5-acc: 0.9584 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2339 - acc: 0.5639 - top5-acc: 0.9473 - val_loss: 1.1605 - val_acc: 0.5850 - val_top5-acc: 0.9570 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2294 - acc: 0.5653 - top5-acc: 0.9494 - val_loss: 1.1528 - val_acc: 0.5874 - val_top5-acc: 0.9602 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2319 - acc: 0.5652 - top5-acc: 0.9491 - val_loss: 1.1540 - val_acc: 0.5886 - val_top5-acc: 0.9602 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2209 - acc: 0.5675 - top5-acc: 0.9489 - val_loss: 1.1558 - val_acc: 0.5852 - val_top5-acc: 0.9582 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2272 - acc: 0.5658 - top5-acc: 0.9488 - val_loss: 1.1633 - val_acc: 0.5814 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2284 - acc: 0.5686 - top5-acc: 0.9483 - val_loss: 1.1522 - val_acc: 0.5898 - val_top5-acc: 0.9598 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2271 - acc: 0.5656 - top5-acc: 0.9476 - val_loss: 1.1589 - val_acc: 0.5900 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2284 - acc: 0.5648 - top5-acc: 0.9483 - val_loss: 1.1585 - val_acc: 0.5848 - val_top5-acc: 0.9596 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2237 - acc: 0.5660 - top5-acc: 0.9492 - val_loss: 1.1678 - val_acc: 0.5806 - val_top5-acc: 0.9588 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2261 - acc: 0.5678 - top5-acc: 0.9482 - val_loss: 1.1577 - val_acc: 0.5872 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2233 - acc: 0.5681 - top5-acc: 0.9485 - val_loss: 1.1660 - val_acc: 0.5852 - val_top5-acc: 0.9608 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2229 - acc: 0.5672 - top5-acc: 0.9492 - val_loss: 1.1545 - val_acc: 0.5906 - val_top5-acc: 0.9602 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2232 - acc: 0.5648 - top5-acc: 0.9502 - val_loss: 1.1564 - val_acc: 0.5892 - val_top5-acc: 0.9596 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2239 - acc: 0.5668 - top5-acc: 0.9498 - val_loss: 1.1606 - val_acc: 0.5878 - val_top5-acc: 0.9596 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2237 - acc: 0.5670 - top5-acc: 0.9499 - val_loss: 1.1565 - val_acc: 0.5866 - val_top5-acc: 0.9580 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2234 - acc: 0.5688 - top5-acc: 0.9493 - val_loss: 1.1570 - val_acc: 0.5884 - val_top5-acc: 0.9586 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 15s 170ms/step - loss: 1.2213 - acc: 0.5695 - top5-acc: 0.9487 - val_loss: 1.1541 - val_acc: 0.5916 - val_top5-acc: 0.9590 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 16s 179ms/step - loss: 1.2216 - acc: 0.5689 - top5-acc: 0.9499 - val_loss: 1.1568 - val_acc: 0.5900 - val_top5-acc: 0.9574 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2262 - acc: 0.5694 - top5-acc: 0.9479 - val_loss: 1.1602 - val_acc: 0.5858 - val_top5-acc: 0.9580 - lr: 3.1250e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 24s 76ms/step - loss: 1.1726 - acc: 0.5867 - top5-acc: 0.9549\n",
      "Test accuracy: 58.67%\n",
      "Test top 5 accuracy: 95.49%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 22s 192ms/step - loss: 1.7207 - acc: 0.3962 - top5-acc: 0.8643 - val_loss: 1.4569 - val_acc: 0.4714 - val_top5-acc: 0.9286 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.4463 - acc: 0.4874 - top5-acc: 0.9227 - val_loss: 1.3631 - val_acc: 0.5108 - val_top5-acc: 0.9414 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.3924 - acc: 0.5038 - top5-acc: 0.9303 - val_loss: 1.2988 - val_acc: 0.5448 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.3571 - acc: 0.5199 - top5-acc: 0.9350 - val_loss: 1.2795 - val_acc: 0.5526 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.3352 - acc: 0.5291 - top5-acc: 0.9378 - val_loss: 1.2665 - val_acc: 0.5516 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.3234 - acc: 0.5304 - top5-acc: 0.9392 - val_loss: 1.2391 - val_acc: 0.5576 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.3127 - acc: 0.5360 - top5-acc: 0.9407 - val_loss: 1.2671 - val_acc: 0.5494 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.3055 - acc: 0.5370 - top5-acc: 0.9398 - val_loss: 1.2419 - val_acc: 0.5550 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2976 - acc: 0.5431 - top5-acc: 0.9416 - val_loss: 1.2377 - val_acc: 0.5578 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2847 - acc: 0.5446 - top5-acc: 0.9421 - val_loss: 1.2293 - val_acc: 0.5636 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2804 - acc: 0.5485 - top5-acc: 0.9438 - val_loss: 1.2136 - val_acc: 0.5674 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2849 - acc: 0.5463 - top5-acc: 0.9436 - val_loss: 1.2066 - val_acc: 0.5724 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2766 - acc: 0.5466 - top5-acc: 0.9441 - val_loss: 1.2157 - val_acc: 0.5640 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2747 - acc: 0.5486 - top5-acc: 0.9436 - val_loss: 1.1968 - val_acc: 0.5730 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2752 - acc: 0.5496 - top5-acc: 0.9461 - val_loss: 1.2017 - val_acc: 0.5716 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2723 - acc: 0.5507 - top5-acc: 0.9444 - val_loss: 1.1806 - val_acc: 0.5818 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2730 - acc: 0.5503 - top5-acc: 0.9433 - val_loss: 1.1760 - val_acc: 0.5876 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2637 - acc: 0.5530 - top5-acc: 0.9451 - val_loss: 1.1887 - val_acc: 0.5742 - val_top5-acc: 0.9572 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2680 - acc: 0.5507 - top5-acc: 0.9442 - val_loss: 1.2044 - val_acc: 0.5666 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2598 - acc: 0.5546 - top5-acc: 0.9455 - val_loss: 1.1887 - val_acc: 0.5808 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2639 - acc: 0.5512 - top5-acc: 0.9455 - val_loss: 1.1748 - val_acc: 0.5798 - val_top5-acc: 0.9592 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 15s 176ms/step - loss: 1.2601 - acc: 0.5538 - top5-acc: 0.9438 - val_loss: 1.1941 - val_acc: 0.5738 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 16s 177ms/step - loss: 1.2536 - acc: 0.5547 - top5-acc: 0.9462 - val_loss: 1.1980 - val_acc: 0.5714 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 16s 180ms/step - loss: 1.2559 - acc: 0.5559 - top5-acc: 0.9455 - val_loss: 1.1789 - val_acc: 0.5792 - val_top5-acc: 0.9570 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2521 - acc: 0.5558 - top5-acc: 0.9466 - val_loss: 1.1920 - val_acc: 0.5736 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2660 - acc: 0.5492 - top5-acc: 0.9447 - val_loss: 1.1825 - val_acc: 0.5770 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2395 - acc: 0.5590 - top5-acc: 0.9486 - val_loss: 1.1664 - val_acc: 0.5830 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2396 - acc: 0.5600 - top5-acc: 0.9486 - val_loss: 1.1663 - val_acc: 0.5826 - val_top5-acc: 0.9584 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2326 - acc: 0.5631 - top5-acc: 0.9472 - val_loss: 1.1714 - val_acc: 0.5868 - val_top5-acc: 0.9602 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2388 - acc: 0.5602 - top5-acc: 0.9477 - val_loss: 1.1709 - val_acc: 0.5828 - val_top5-acc: 0.9592 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2389 - acc: 0.5642 - top5-acc: 0.9467 - val_loss: 1.1566 - val_acc: 0.5928 - val_top5-acc: 0.9584 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2351 - acc: 0.5646 - top5-acc: 0.9475 - val_loss: 1.1667 - val_acc: 0.5838 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2353 - acc: 0.5650 - top5-acc: 0.9472 - val_loss: 1.1582 - val_acc: 0.5862 - val_top5-acc: 0.9598 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 15s 174ms/step - loss: 1.2347 - acc: 0.5626 - top5-acc: 0.9481 - val_loss: 1.1567 - val_acc: 0.5856 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2354 - acc: 0.5625 - top5-acc: 0.9484 - val_loss: 1.1585 - val_acc: 0.5894 - val_top5-acc: 0.9588 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2350 - acc: 0.5618 - top5-acc: 0.9484 - val_loss: 1.1867 - val_acc: 0.5776 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2267 - acc: 0.5668 - top5-acc: 0.9480 - val_loss: 1.1530 - val_acc: 0.5930 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2300 - acc: 0.5647 - top5-acc: 0.9494 - val_loss: 1.1542 - val_acc: 0.5898 - val_top5-acc: 0.9602 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 15s 172ms/step - loss: 1.2270 - acc: 0.5649 - top5-acc: 0.9491 - val_loss: 1.1606 - val_acc: 0.5836 - val_top5-acc: 0.9588 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2263 - acc: 0.5689 - top5-acc: 0.9498 - val_loss: 1.1564 - val_acc: 0.5910 - val_top5-acc: 0.9604 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2292 - acc: 0.5657 - top5-acc: 0.9477 - val_loss: 1.1596 - val_acc: 0.5958 - val_top5-acc: 0.9588 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2296 - acc: 0.5662 - top5-acc: 0.9501 - val_loss: 1.1550 - val_acc: 0.5908 - val_top5-acc: 0.9592 - lr: 0.0012\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 15s 171ms/step - loss: 1.2245 - acc: 0.5689 - top5-acc: 0.9488 - val_loss: 1.1528 - val_acc: 0.5926 - val_top5-acc: 0.9592 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2265 - acc: 0.5669 - top5-acc: 0.9479 - val_loss: 1.1510 - val_acc: 0.5928 - val_top5-acc: 0.9612 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2266 - acc: 0.5693 - top5-acc: 0.9475 - val_loss: 1.1571 - val_acc: 0.5912 - val_top5-acc: 0.9588 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2256 - acc: 0.5676 - top5-acc: 0.9490 - val_loss: 1.1552 - val_acc: 0.5882 - val_top5-acc: 0.9596 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2222 - acc: 0.5698 - top5-acc: 0.9494 - val_loss: 1.1531 - val_acc: 0.5904 - val_top5-acc: 0.9600 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2286 - acc: 0.5657 - top5-acc: 0.9497 - val_loss: 1.1591 - val_acc: 0.5890 - val_top5-acc: 0.9594 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2318 - acc: 0.5662 - top5-acc: 0.9488 - val_loss: 1.1550 - val_acc: 0.5914 - val_top5-acc: 0.9606 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 15s 173ms/step - loss: 1.2248 - acc: 0.5705 - top5-acc: 0.9494 - val_loss: 1.1518 - val_acc: 0.5952 - val_top5-acc: 0.9610 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 1.1684 - acc: 0.5857 - top5-acc: 0.9540\n",
      "Test accuracy: 58.57%\n",
      "Test top 5 accuracy: 95.4%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 22s 198ms/step - loss: 1.6920 - acc: 0.4044 - top5-acc: 0.8711 - val_loss: 1.4286 - val_acc: 0.4916 - val_top5-acc: 0.9340 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.4330 - acc: 0.4963 - top5-acc: 0.9252 - val_loss: 1.3743 - val_acc: 0.5008 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.3804 - acc: 0.5083 - top5-acc: 0.9323 - val_loss: 1.3048 - val_acc: 0.5308 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.3522 - acc: 0.5230 - top5-acc: 0.9351 - val_loss: 1.2883 - val_acc: 0.5464 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.3217 - acc: 0.5333 - top5-acc: 0.9374 - val_loss: 1.2630 - val_acc: 0.5478 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.3089 - acc: 0.5410 - top5-acc: 0.9396 - val_loss: 1.2347 - val_acc: 0.5606 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.3040 - acc: 0.5370 - top5-acc: 0.9409 - val_loss: 1.2451 - val_acc: 0.5586 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2917 - acc: 0.5424 - top5-acc: 0.9423 - val_loss: 1.2118 - val_acc: 0.5648 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2786 - acc: 0.5469 - top5-acc: 0.9439 - val_loss: 1.2376 - val_acc: 0.5602 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2741 - acc: 0.5478 - top5-acc: 0.9433 - val_loss: 1.2166 - val_acc: 0.5626 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2729 - acc: 0.5486 - top5-acc: 0.9432 - val_loss: 1.2164 - val_acc: 0.5576 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 16s 185ms/step - loss: 1.2653 - acc: 0.5520 - top5-acc: 0.9454 - val_loss: 1.1986 - val_acc: 0.5694 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 16s 186ms/step - loss: 1.2579 - acc: 0.5562 - top5-acc: 0.9454 - val_loss: 1.2041 - val_acc: 0.5676 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2656 - acc: 0.5491 - top5-acc: 0.9445 - val_loss: 1.2052 - val_acc: 0.5672 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 16s 185ms/step - loss: 1.2583 - acc: 0.5539 - top5-acc: 0.9452 - val_loss: 1.1928 - val_acc: 0.5722 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.2545 - acc: 0.5531 - top5-acc: 0.9458 - val_loss: 1.1767 - val_acc: 0.5832 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 16s 179ms/step - loss: 1.2520 - acc: 0.5578 - top5-acc: 0.9461 - val_loss: 1.1727 - val_acc: 0.5804 - val_top5-acc: 0.9578 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 16s 185ms/step - loss: 1.2542 - acc: 0.5566 - top5-acc: 0.9473 - val_loss: 1.1755 - val_acc: 0.5796 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2488 - acc: 0.5563 - top5-acc: 0.9459 - val_loss: 1.1834 - val_acc: 0.5688 - val_top5-acc: 0.9594 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 16s 186ms/step - loss: 1.2521 - acc: 0.5592 - top5-acc: 0.9463 - val_loss: 1.1744 - val_acc: 0.5760 - val_top5-acc: 0.9614 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2421 - acc: 0.5591 - top5-acc: 0.9476 - val_loss: 1.1819 - val_acc: 0.5758 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2489 - acc: 0.5590 - top5-acc: 0.9467 - val_loss: 1.1676 - val_acc: 0.5764 - val_top5-acc: 0.9594 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.2399 - acc: 0.5633 - top5-acc: 0.9459 - val_loss: 1.1744 - val_acc: 0.5758 - val_top5-acc: 0.9592 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.2491 - acc: 0.5576 - top5-acc: 0.9481 - val_loss: 1.1672 - val_acc: 0.5802 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 16s 180ms/step - loss: 1.2532 - acc: 0.5526 - top5-acc: 0.9472 - val_loss: 1.1687 - val_acc: 0.5838 - val_top5-acc: 0.9604 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2446 - acc: 0.5576 - top5-acc: 0.9472 - val_loss: 1.1814 - val_acc: 0.5784 - val_top5-acc: 0.9584 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.2374 - acc: 0.5619 - top5-acc: 0.9490 - val_loss: 1.1703 - val_acc: 0.5796 - val_top5-acc: 0.9582 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2349 - acc: 0.5607 - top5-acc: 0.9480 - val_loss: 1.1729 - val_acc: 0.5752 - val_top5-acc: 0.9608 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 16s 184ms/step - loss: 1.2463 - acc: 0.5589 - top5-acc: 0.9468 - val_loss: 1.1773 - val_acc: 0.5744 - val_top5-acc: 0.9602 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 16s 186ms/step - loss: 1.2250 - acc: 0.5649 - top5-acc: 0.9480 - val_loss: 1.1488 - val_acc: 0.5876 - val_top5-acc: 0.9614 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2235 - acc: 0.5663 - top5-acc: 0.9485 - val_loss: 1.1464 - val_acc: 0.5902 - val_top5-acc: 0.9600 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2246 - acc: 0.5664 - top5-acc: 0.9487 - val_loss: 1.1542 - val_acc: 0.5858 - val_top5-acc: 0.9606 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2250 - acc: 0.5668 - top5-acc: 0.9483 - val_loss: 1.1564 - val_acc: 0.5826 - val_top5-acc: 0.9598 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 16s 184ms/step - loss: 1.2219 - acc: 0.5667 - top5-acc: 0.9482 - val_loss: 1.1806 - val_acc: 0.5738 - val_top5-acc: 0.9618 - lr: 0.0025\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 16s 187ms/step - loss: 1.2255 - acc: 0.5680 - top5-acc: 0.9498 - val_loss: 1.1563 - val_acc: 0.5862 - val_top5-acc: 0.9608 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2240 - acc: 0.5659 - top5-acc: 0.9489 - val_loss: 1.1500 - val_acc: 0.5802 - val_top5-acc: 0.9602 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2165 - acc: 0.5735 - top5-acc: 0.9500 - val_loss: 1.1477 - val_acc: 0.5870 - val_top5-acc: 0.9618 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2192 - acc: 0.5650 - top5-acc: 0.9502 - val_loss: 1.1543 - val_acc: 0.5842 - val_top5-acc: 0.9620 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2186 - acc: 0.5688 - top5-acc: 0.9503 - val_loss: 1.1471 - val_acc: 0.5872 - val_top5-acc: 0.9614 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2127 - acc: 0.5735 - top5-acc: 0.9510 - val_loss: 1.1478 - val_acc: 0.5906 - val_top5-acc: 0.9610 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2201 - acc: 0.5700 - top5-acc: 0.9482 - val_loss: 1.1527 - val_acc: 0.5882 - val_top5-acc: 0.9614 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2143 - acc: 0.5703 - top5-acc: 0.9505 - val_loss: 1.1415 - val_acc: 0.5916 - val_top5-acc: 0.9642 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2106 - acc: 0.5728 - top5-acc: 0.9498 - val_loss: 1.1492 - val_acc: 0.5874 - val_top5-acc: 0.9610 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2085 - acc: 0.5744 - top5-acc: 0.9516 - val_loss: 1.1480 - val_acc: 0.5862 - val_top5-acc: 0.9634 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2146 - acc: 0.5721 - top5-acc: 0.9501 - val_loss: 1.1532 - val_acc: 0.5852 - val_top5-acc: 0.9616 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2142 - acc: 0.5723 - top5-acc: 0.9493 - val_loss: 1.1456 - val_acc: 0.5902 - val_top5-acc: 0.9618 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2127 - acc: 0.5735 - top5-acc: 0.9514 - val_loss: 1.1462 - val_acc: 0.5886 - val_top5-acc: 0.9622 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2111 - acc: 0.5733 - top5-acc: 0.9500 - val_loss: 1.1433 - val_acc: 0.5868 - val_top5-acc: 0.9624 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.2133 - acc: 0.5712 - top5-acc: 0.9498 - val_loss: 1.1505 - val_acc: 0.5888 - val_top5-acc: 0.9634 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2153 - acc: 0.5714 - top5-acc: 0.9511 - val_loss: 1.1454 - val_acc: 0.5908 - val_top5-acc: 0.9632 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 27s 87ms/step - loss: 1.1572 - acc: 0.5883 - top5-acc: 0.9569\n",
      "Test accuracy: 58.83%\n",
      "Test top 5 accuracy: 95.69%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 22s 207ms/step - loss: 1.7115 - acc: 0.3980 - top5-acc: 0.8655 - val_loss: 1.4568 - val_acc: 0.4846 - val_top5-acc: 0.9258 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.4508 - acc: 0.4893 - top5-acc: 0.9230 - val_loss: 1.3854 - val_acc: 0.5126 - val_top5-acc: 0.9308 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.3899 - acc: 0.5102 - top5-acc: 0.9304 - val_loss: 1.3100 - val_acc: 0.5354 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.3597 - acc: 0.5202 - top5-acc: 0.9329 - val_loss: 1.2952 - val_acc: 0.5368 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.3393 - acc: 0.5258 - top5-acc: 0.9361 - val_loss: 1.2525 - val_acc: 0.5578 - val_top5-acc: 0.9488 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.3246 - acc: 0.5308 - top5-acc: 0.9370 - val_loss: 1.2670 - val_acc: 0.5460 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.3147 - acc: 0.5328 - top5-acc: 0.9385 - val_loss: 1.2584 - val_acc: 0.5424 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.3038 - acc: 0.5370 - top5-acc: 0.9405 - val_loss: 1.2329 - val_acc: 0.5604 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 17s 195ms/step - loss: 1.2946 - acc: 0.5431 - top5-acc: 0.9412 - val_loss: 1.2215 - val_acc: 0.5684 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.2927 - acc: 0.5417 - top5-acc: 0.9387 - val_loss: 1.2239 - val_acc: 0.5606 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2858 - acc: 0.5432 - top5-acc: 0.9419 - val_loss: 1.2228 - val_acc: 0.5546 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 17s 194ms/step - loss: 1.2855 - acc: 0.5454 - top5-acc: 0.9421 - val_loss: 1.2059 - val_acc: 0.5676 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2772 - acc: 0.5470 - top5-acc: 0.9424 - val_loss: 1.1997 - val_acc: 0.5674 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2772 - acc: 0.5498 - top5-acc: 0.9435 - val_loss: 1.2104 - val_acc: 0.5652 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 17s 193ms/step - loss: 1.2749 - acc: 0.5495 - top5-acc: 0.9431 - val_loss: 1.1948 - val_acc: 0.5714 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2731 - acc: 0.5493 - top5-acc: 0.9430 - val_loss: 1.2093 - val_acc: 0.5666 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2721 - acc: 0.5508 - top5-acc: 0.9444 - val_loss: 1.1993 - val_acc: 0.5696 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2681 - acc: 0.5503 - top5-acc: 0.9439 - val_loss: 1.2064 - val_acc: 0.5640 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2731 - acc: 0.5482 - top5-acc: 0.9450 - val_loss: 1.2187 - val_acc: 0.5682 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2654 - acc: 0.5529 - top5-acc: 0.9440 - val_loss: 1.1930 - val_acc: 0.5708 - val_top5-acc: 0.9578 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2670 - acc: 0.5501 - top5-acc: 0.9454 - val_loss: 1.1822 - val_acc: 0.5816 - val_top5-acc: 0.9576 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2614 - acc: 0.5527 - top5-acc: 0.9442 - val_loss: 1.1718 - val_acc: 0.5852 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 17s 188ms/step - loss: 1.2657 - acc: 0.5507 - top5-acc: 0.9450 - val_loss: 1.2088 - val_acc: 0.5598 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2618 - acc: 0.5529 - top5-acc: 0.9444 - val_loss: 1.1828 - val_acc: 0.5780 - val_top5-acc: 0.9580 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2616 - acc: 0.5516 - top5-acc: 0.9449 - val_loss: 1.2027 - val_acc: 0.5702 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2579 - acc: 0.5515 - top5-acc: 0.9440 - val_loss: 1.1646 - val_acc: 0.5812 - val_top5-acc: 0.9576 - lr: 0.0050\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2619 - acc: 0.5493 - top5-acc: 0.9456 - val_loss: 1.2080 - val_acc: 0.5700 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2606 - acc: 0.5523 - top5-acc: 0.9441 - val_loss: 1.1675 - val_acc: 0.5820 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2626 - acc: 0.5517 - top5-acc: 0.9449 - val_loss: 1.1894 - val_acc: 0.5770 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2554 - acc: 0.5543 - top5-acc: 0.9447 - val_loss: 1.1800 - val_acc: 0.5772 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2611 - acc: 0.5509 - top5-acc: 0.9452 - val_loss: 1.1878 - val_acc: 0.5772 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2369 - acc: 0.5618 - top5-acc: 0.9464 - val_loss: 1.1590 - val_acc: 0.5860 - val_top5-acc: 0.9576 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2393 - acc: 0.5641 - top5-acc: 0.9485 - val_loss: 1.1666 - val_acc: 0.5854 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2356 - acc: 0.5620 - top5-acc: 0.9466 - val_loss: 1.1658 - val_acc: 0.5780 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2410 - acc: 0.5618 - top5-acc: 0.9464 - val_loss: 1.1831 - val_acc: 0.5772 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2450 - acc: 0.5577 - top5-acc: 0.9470 - val_loss: 1.1701 - val_acc: 0.5790 - val_top5-acc: 0.9586 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2431 - acc: 0.5574 - top5-acc: 0.9469 - val_loss: 1.1675 - val_acc: 0.5828 - val_top5-acc: 0.9584 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2343 - acc: 0.5652 - top5-acc: 0.9466 - val_loss: 1.1588 - val_acc: 0.5828 - val_top5-acc: 0.9588 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2315 - acc: 0.5631 - top5-acc: 0.9490 - val_loss: 1.1645 - val_acc: 0.5864 - val_top5-acc: 0.9582 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2322 - acc: 0.5630 - top5-acc: 0.9478 - val_loss: 1.1665 - val_acc: 0.5804 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2371 - acc: 0.5632 - top5-acc: 0.9483 - val_loss: 1.1644 - val_acc: 0.5818 - val_top5-acc: 0.9572 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2371 - acc: 0.5637 - top5-acc: 0.9466 - val_loss: 1.1581 - val_acc: 0.5882 - val_top5-acc: 0.9596 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 17s 189ms/step - loss: 1.2330 - acc: 0.5640 - top5-acc: 0.9481 - val_loss: 1.1609 - val_acc: 0.5868 - val_top5-acc: 0.9578 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2356 - acc: 0.5603 - top5-acc: 0.9466 - val_loss: 1.1629 - val_acc: 0.5820 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2373 - acc: 0.5629 - top5-acc: 0.9473 - val_loss: 1.1548 - val_acc: 0.5880 - val_top5-acc: 0.9578 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2366 - acc: 0.5623 - top5-acc: 0.9484 - val_loss: 1.1681 - val_acc: 0.5822 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.2400 - acc: 0.5613 - top5-acc: 0.9469 - val_loss: 1.1631 - val_acc: 0.5830 - val_top5-acc: 0.9578 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2393 - acc: 0.5613 - top5-acc: 0.9472 - val_loss: 1.1626 - val_acc: 0.5866 - val_top5-acc: 0.9564 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 17s 190ms/step - loss: 1.2338 - acc: 0.5649 - top5-acc: 0.9473 - val_loss: 1.1628 - val_acc: 0.5834 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2340 - acc: 0.5645 - top5-acc: 0.9478 - val_loss: 1.1595 - val_acc: 0.5910 - val_top5-acc: 0.9582 - lr: 0.0012\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 1.1773 - acc: 0.5811 - top5-acc: 0.9538\n",
      "Test accuracy: 58.11%\n",
      "Test top 5 accuracy: 95.38%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 23s 216ms/step - loss: 1.7208 - acc: 0.4004 - top5-acc: 0.8625 - val_loss: 1.4530 - val_acc: 0.4830 - val_top5-acc: 0.9246 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.4625 - acc: 0.4859 - top5-acc: 0.9182 - val_loss: 1.3762 - val_acc: 0.5094 - val_top5-acc: 0.9342 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.3993 - acc: 0.5069 - top5-acc: 0.9274 - val_loss: 1.3353 - val_acc: 0.5246 - val_top5-acc: 0.9390 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.3719 - acc: 0.5151 - top5-acc: 0.9331 - val_loss: 1.2936 - val_acc: 0.5466 - val_top5-acc: 0.9442 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.3381 - acc: 0.5295 - top5-acc: 0.9346 - val_loss: 1.2771 - val_acc: 0.5356 - val_top5-acc: 0.9458 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.3311 - acc: 0.5317 - top5-acc: 0.9371 - val_loss: 1.2740 - val_acc: 0.5468 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 18s 206ms/step - loss: 1.3226 - acc: 0.5326 - top5-acc: 0.9370 - val_loss: 1.2442 - val_acc: 0.5510 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 18s 202ms/step - loss: 1.3138 - acc: 0.5352 - top5-acc: 0.9377 - val_loss: 1.2356 - val_acc: 0.5510 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 18s 201ms/step - loss: 1.3037 - acc: 0.5401 - top5-acc: 0.9397 - val_loss: 1.2253 - val_acc: 0.5676 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2985 - acc: 0.5397 - top5-acc: 0.9391 - val_loss: 1.2135 - val_acc: 0.5678 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.3018 - acc: 0.5424 - top5-acc: 0.9391 - val_loss: 1.2386 - val_acc: 0.5526 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 18s 202ms/step - loss: 1.2907 - acc: 0.5435 - top5-acc: 0.9416 - val_loss: 1.2093 - val_acc: 0.5702 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.2844 - acc: 0.5456 - top5-acc: 0.9428 - val_loss: 1.1974 - val_acc: 0.5786 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 17s 195ms/step - loss: 1.2878 - acc: 0.5416 - top5-acc: 0.9415 - val_loss: 1.2195 - val_acc: 0.5622 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.2802 - acc: 0.5469 - top5-acc: 0.9428 - val_loss: 1.1990 - val_acc: 0.5688 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.2789 - acc: 0.5480 - top5-acc: 0.9418 - val_loss: 1.2247 - val_acc: 0.5604 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.2793 - acc: 0.5473 - top5-acc: 0.9426 - val_loss: 1.1837 - val_acc: 0.5792 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2808 - acc: 0.5491 - top5-acc: 0.9420 - val_loss: 1.1851 - val_acc: 0.5704 - val_top5-acc: 0.9582 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.2758 - acc: 0.5462 - top5-acc: 0.9443 - val_loss: 1.2187 - val_acc: 0.5668 - val_top5-acc: 0.9556 - lr: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "88/88 [==============================] - 18s 201ms/step - loss: 1.2845 - acc: 0.5436 - top5-acc: 0.9434 - val_loss: 1.1884 - val_acc: 0.5804 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 18s 206ms/step - loss: 1.2725 - acc: 0.5497 - top5-acc: 0.9436 - val_loss: 1.1881 - val_acc: 0.5722 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.2693 - acc: 0.5501 - top5-acc: 0.9452 - val_loss: 1.2061 - val_acc: 0.5596 - val_top5-acc: 0.9608 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.2501 - acc: 0.5566 - top5-acc: 0.9458 - val_loss: 1.1653 - val_acc: 0.5896 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.2545 - acc: 0.5575 - top5-acc: 0.9448 - val_loss: 1.1660 - val_acc: 0.5872 - val_top5-acc: 0.9588 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 18s 202ms/step - loss: 1.2521 - acc: 0.5590 - top5-acc: 0.9464 - val_loss: 1.1778 - val_acc: 0.5770 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2566 - acc: 0.5544 - top5-acc: 0.9445 - val_loss: 1.1842 - val_acc: 0.5784 - val_top5-acc: 0.9586 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2508 - acc: 0.5561 - top5-acc: 0.9450 - val_loss: 1.1727 - val_acc: 0.5826 - val_top5-acc: 0.9580 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2526 - acc: 0.5567 - top5-acc: 0.9457 - val_loss: 1.1685 - val_acc: 0.5814 - val_top5-acc: 0.9602 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 18s 204ms/step - loss: 1.2456 - acc: 0.5594 - top5-acc: 0.9454 - val_loss: 1.1607 - val_acc: 0.5878 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 18s 201ms/step - loss: 1.2429 - acc: 0.5623 - top5-acc: 0.9466 - val_loss: 1.1689 - val_acc: 0.5820 - val_top5-acc: 0.9592 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2469 - acc: 0.5590 - top5-acc: 0.9465 - val_loss: 1.1607 - val_acc: 0.5896 - val_top5-acc: 0.9604 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2432 - acc: 0.5595 - top5-acc: 0.9473 - val_loss: 1.1731 - val_acc: 0.5836 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2407 - acc: 0.5632 - top5-acc: 0.9466 - val_loss: 1.1636 - val_acc: 0.5866 - val_top5-acc: 0.9598 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.2452 - acc: 0.5603 - top5-acc: 0.9467 - val_loss: 1.1648 - val_acc: 0.5892 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.2444 - acc: 0.5613 - top5-acc: 0.9471 - val_loss: 1.1643 - val_acc: 0.5882 - val_top5-acc: 0.9592 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2406 - acc: 0.5632 - top5-acc: 0.9471 - val_loss: 1.1610 - val_acc: 0.5878 - val_top5-acc: 0.9588 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.2393 - acc: 0.5634 - top5-acc: 0.9474 - val_loss: 1.1630 - val_acc: 0.5876 - val_top5-acc: 0.9582 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2448 - acc: 0.5620 - top5-acc: 0.9470 - val_loss: 1.1707 - val_acc: 0.5892 - val_top5-acc: 0.9604 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 17s 197ms/step - loss: 1.2423 - acc: 0.5618 - top5-acc: 0.9464 - val_loss: 1.1630 - val_acc: 0.5856 - val_top5-acc: 0.9584 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2421 - acc: 0.5648 - top5-acc: 0.9459 - val_loss: 1.1654 - val_acc: 0.5898 - val_top5-acc: 0.9592 - lr: 3.1250e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2405 - acc: 0.5625 - top5-acc: 0.9478 - val_loss: 1.1635 - val_acc: 0.5896 - val_top5-acc: 0.9572 - lr: 3.1250e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 17s 199ms/step - loss: 1.2398 - acc: 0.5626 - top5-acc: 0.9472 - val_loss: 1.1671 - val_acc: 0.5880 - val_top5-acc: 0.9578 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2414 - acc: 0.5641 - top5-acc: 0.9469 - val_loss: 1.1678 - val_acc: 0.5876 - val_top5-acc: 0.9580 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2434 - acc: 0.5643 - top5-acc: 0.9463 - val_loss: 1.1661 - val_acc: 0.5856 - val_top5-acc: 0.9574 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2427 - acc: 0.5636 - top5-acc: 0.9475 - val_loss: 1.1654 - val_acc: 0.5902 - val_top5-acc: 0.9588 - lr: 1.5625e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2473 - acc: 0.5650 - top5-acc: 0.9450 - val_loss: 1.1659 - val_acc: 0.5916 - val_top5-acc: 0.9584 - lr: 1.5625e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2446 - acc: 0.5637 - top5-acc: 0.9462 - val_loss: 1.1678 - val_acc: 0.5904 - val_top5-acc: 0.9576 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 18s 199ms/step - loss: 1.2427 - acc: 0.5638 - top5-acc: 0.9469 - val_loss: 1.1687 - val_acc: 0.5866 - val_top5-acc: 0.9586 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 17s 198ms/step - loss: 1.2475 - acc: 0.5638 - top5-acc: 0.9447 - val_loss: 1.1701 - val_acc: 0.5864 - val_top5-acc: 0.9574 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 18s 200ms/step - loss: 1.2453 - acc: 0.5645 - top5-acc: 0.9472 - val_loss: 1.1717 - val_acc: 0.5888 - val_top5-acc: 0.9588 - lr: 7.8125e-05\n",
      "313/313 [==============================] - 29s 94ms/step - loss: 1.1920 - acc: 0.5797 - top5-acc: 0.9539\n",
      "Test accuracy: 57.97%\n",
      "Test top 5 accuracy: 95.39%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 24s 225ms/step - loss: 1.7401 - acc: 0.3889 - top5-acc: 0.8544 - val_loss: 1.4731 - val_acc: 0.4740 - val_top5-acc: 0.9226 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.4764 - acc: 0.4791 - top5-acc: 0.9182 - val_loss: 1.3895 - val_acc: 0.5056 - val_top5-acc: 0.9358 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.4168 - acc: 0.4994 - top5-acc: 0.9282 - val_loss: 1.3449 - val_acc: 0.5176 - val_top5-acc: 0.9322 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.3856 - acc: 0.5098 - top5-acc: 0.9297 - val_loss: 1.3092 - val_acc: 0.5366 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.3603 - acc: 0.5218 - top5-acc: 0.9330 - val_loss: 1.2793 - val_acc: 0.5494 - val_top5-acc: 0.9450 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.3467 - acc: 0.5202 - top5-acc: 0.9357 - val_loss: 1.2827 - val_acc: 0.5480 - val_top5-acc: 0.9424 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.3326 - acc: 0.5312 - top5-acc: 0.9355 - val_loss: 1.2639 - val_acc: 0.5476 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 19s 215ms/step - loss: 1.3289 - acc: 0.5276 - top5-acc: 0.9368 - val_loss: 1.2804 - val_acc: 0.5372 - val_top5-acc: 0.9480 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.3203 - acc: 0.5317 - top5-acc: 0.9373 - val_loss: 1.2682 - val_acc: 0.5462 - val_top5-acc: 0.9446 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.3102 - acc: 0.5360 - top5-acc: 0.9394 - val_loss: 1.2594 - val_acc: 0.5454 - val_top5-acc: 0.9472 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.3078 - acc: 0.5370 - top5-acc: 0.9399 - val_loss: 1.2274 - val_acc: 0.5620 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 18s 208ms/step - loss: 1.3016 - acc: 0.5392 - top5-acc: 0.9408 - val_loss: 1.2730 - val_acc: 0.5428 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.2969 - acc: 0.5402 - top5-acc: 0.9413 - val_loss: 1.2299 - val_acc: 0.5516 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 19s 213ms/step - loss: 1.2909 - acc: 0.5421 - top5-acc: 0.9395 - val_loss: 1.2145 - val_acc: 0.5660 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 19s 215ms/step - loss: 1.2865 - acc: 0.5441 - top5-acc: 0.9417 - val_loss: 1.2130 - val_acc: 0.5680 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 18s 206ms/step - loss: 1.2864 - acc: 0.5462 - top5-acc: 0.9422 - val_loss: 1.2168 - val_acc: 0.5624 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.2840 - acc: 0.5440 - top5-acc: 0.9429 - val_loss: 1.2107 - val_acc: 0.5604 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.2823 - acc: 0.5454 - top5-acc: 0.9432 - val_loss: 1.2129 - val_acc: 0.5660 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.2863 - acc: 0.5459 - top5-acc: 0.9417 - val_loss: 1.2223 - val_acc: 0.5636 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.2791 - acc: 0.5466 - top5-acc: 0.9430 - val_loss: 1.2217 - val_acc: 0.5616 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.2839 - acc: 0.5447 - top5-acc: 0.9424 - val_loss: 1.2044 - val_acc: 0.5694 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 18s 206ms/step - loss: 1.2789 - acc: 0.5479 - top5-acc: 0.9432 - val_loss: 1.1936 - val_acc: 0.5728 - val_top5-acc: 0.9512 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.2791 - acc: 0.5457 - top5-acc: 0.9436 - val_loss: 1.1925 - val_acc: 0.5756 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.2770 - acc: 0.5454 - top5-acc: 0.9432 - val_loss: 1.1932 - val_acc: 0.5768 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.2729 - acc: 0.5499 - top5-acc: 0.9426 - val_loss: 1.1959 - val_acc: 0.5716 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.2770 - acc: 0.5495 - top5-acc: 0.9426 - val_loss: 1.2129 - val_acc: 0.5596 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 18s 207ms/step - loss: 1.2710 - acc: 0.5502 - top5-acc: 0.9427 - val_loss: 1.1910 - val_acc: 0.5698 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2735 - acc: 0.5470 - top5-acc: 0.9434 - val_loss: 1.1752 - val_acc: 0.5814 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.2732 - acc: 0.5478 - top5-acc: 0.9428 - val_loss: 1.1935 - val_acc: 0.5672 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 19s 214ms/step - loss: 1.2766 - acc: 0.5456 - top5-acc: 0.9439 - val_loss: 1.1922 - val_acc: 0.5702 - val_top5-acc: 0.9518 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 18s 208ms/step - loss: 1.2718 - acc: 0.5483 - top5-acc: 0.9436 - val_loss: 1.1771 - val_acc: 0.5792 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 18s 209ms/step - loss: 1.2776 - acc: 0.5461 - top5-acc: 0.9428 - val_loss: 1.1901 - val_acc: 0.5748 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 19s 220ms/step - loss: 1.2705 - acc: 0.5479 - top5-acc: 0.9436 - val_loss: 1.1960 - val_acc: 0.5692 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2556 - acc: 0.5552 - top5-acc: 0.9433 - val_loss: 1.1690 - val_acc: 0.5824 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 62s 704ms/step - loss: 1.2525 - acc: 0.5587 - top5-acc: 0.9448 - val_loss: 1.1680 - val_acc: 0.5844 - val_top5-acc: 0.9550 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 75s 854ms/step - loss: 1.2523 - acc: 0.5558 - top5-acc: 0.9465 - val_loss: 1.1761 - val_acc: 0.5824 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 76s 867ms/step - loss: 1.2576 - acc: 0.5538 - top5-acc: 0.9456 - val_loss: 1.1837 - val_acc: 0.5746 - val_top5-acc: 0.9564 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 76s 866ms/step - loss: 1.2612 - acc: 0.5523 - top5-acc: 0.9439 - val_loss: 1.1694 - val_acc: 0.5818 - val_top5-acc: 0.9546 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 76s 868ms/step - loss: 1.2618 - acc: 0.5529 - top5-acc: 0.9448 - val_loss: 1.1753 - val_acc: 0.5776 - val_top5-acc: 0.9572 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 75s 850ms/step - loss: 1.2566 - acc: 0.5543 - top5-acc: 0.9462 - val_loss: 1.1843 - val_acc: 0.5690 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 59s 674ms/step - loss: 1.2491 - acc: 0.5571 - top5-acc: 0.9465 - val_loss: 1.1716 - val_acc: 0.5846 - val_top5-acc: 0.9552 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 78s 887ms/step - loss: 1.2507 - acc: 0.5561 - top5-acc: 0.9461 - val_loss: 1.1648 - val_acc: 0.5836 - val_top5-acc: 0.9588 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 78s 884ms/step - loss: 1.2533 - acc: 0.5576 - top5-acc: 0.9458 - val_loss: 1.1673 - val_acc: 0.5904 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 78s 888ms/step - loss: 1.2492 - acc: 0.5589 - top5-acc: 0.9459 - val_loss: 1.1740 - val_acc: 0.5798 - val_top5-acc: 0.9558 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 78s 888ms/step - loss: 1.2502 - acc: 0.5578 - top5-acc: 0.9466 - val_loss: 1.1718 - val_acc: 0.5764 - val_top5-acc: 0.9550 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 78s 885ms/step - loss: 1.2467 - acc: 0.5587 - top5-acc: 0.9451 - val_loss: 1.1703 - val_acc: 0.5848 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 77s 873ms/step - loss: 1.2527 - acc: 0.5561 - top5-acc: 0.9455 - val_loss: 1.1731 - val_acc: 0.5826 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 72s 816ms/step - loss: 1.2468 - acc: 0.5609 - top5-acc: 0.9462 - val_loss: 1.1665 - val_acc: 0.5824 - val_top5-acc: 0.9570 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 76s 859ms/step - loss: 1.2435 - acc: 0.5589 - top5-acc: 0.9476 - val_loss: 1.1672 - val_acc: 0.5852 - val_top5-acc: 0.9562 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 73s 827ms/step - loss: 1.2459 - acc: 0.5615 - top5-acc: 0.9450 - val_loss: 1.1766 - val_acc: 0.5826 - val_top5-acc: 0.9560 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 149s 476ms/step - loss: 1.1922 - acc: 0.5746 - top5-acc: 0.9543\n",
      "Test accuracy: 57.46%\n",
      "Test top 5 accuracy: 95.43%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 87s 923ms/step - loss: 1.7207 - acc: 0.3983 - top5-acc: 0.8563 - val_loss: 1.4809 - val_acc: 0.4692 - val_top5-acc: 0.9208 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 79s 900ms/step - loss: 1.4526 - acc: 0.4892 - top5-acc: 0.9209 - val_loss: 1.3725 - val_acc: 0.5094 - val_top5-acc: 0.9336 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 1.3935 - acc: 0.5111 - top5-acc: 0.9274 - val_loss: 1.3131 - val_acc: 0.5328 - val_top5-acc: 0.9440 - lr: 0.0050\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 21s 234ms/step - loss: 1.3681 - acc: 0.5183 - top5-acc: 0.9325 - val_loss: 1.2876 - val_acc: 0.5438 - val_top5-acc: 0.9470 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3417 - acc: 0.5258 - top5-acc: 0.9355 - val_loss: 1.2625 - val_acc: 0.5528 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3291 - acc: 0.5311 - top5-acc: 0.9368 - val_loss: 1.2787 - val_acc: 0.5500 - val_top5-acc: 0.9452 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.3088 - acc: 0.5381 - top5-acc: 0.9381 - val_loss: 1.2608 - val_acc: 0.5516 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3080 - acc: 0.5397 - top5-acc: 0.9377 - val_loss: 1.2348 - val_acc: 0.5662 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 222ms/step - loss: 1.2980 - acc: 0.5395 - top5-acc: 0.9416 - val_loss: 1.2133 - val_acc: 0.5754 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2974 - acc: 0.5408 - top5-acc: 0.9401 - val_loss: 1.2286 - val_acc: 0.5610 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.2894 - acc: 0.5443 - top5-acc: 0.9405 - val_loss: 1.2247 - val_acc: 0.5584 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2877 - acc: 0.5428 - top5-acc: 0.9414 - val_loss: 1.2186 - val_acc: 0.5652 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 19s 215ms/step - loss: 1.2729 - acc: 0.5506 - top5-acc: 0.9433 - val_loss: 1.2201 - val_acc: 0.5714 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.2860 - acc: 0.5448 - top5-acc: 0.9416 - val_loss: 1.2060 - val_acc: 0.5698 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 19s 216ms/step - loss: 1.2755 - acc: 0.5487 - top5-acc: 0.9432 - val_loss: 1.1887 - val_acc: 0.5756 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 19s 214ms/step - loss: 1.2768 - acc: 0.5481 - top5-acc: 0.9414 - val_loss: 1.1940 - val_acc: 0.5726 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 19s 216ms/step - loss: 1.2707 - acc: 0.5517 - top5-acc: 0.9439 - val_loss: 1.1828 - val_acc: 0.5764 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 19s 219ms/step - loss: 1.2681 - acc: 0.5510 - top5-acc: 0.9433 - val_loss: 1.1840 - val_acc: 0.5764 - val_top5-acc: 0.9580 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 19s 213ms/step - loss: 1.2596 - acc: 0.5524 - top5-acc: 0.9444 - val_loss: 1.1917 - val_acc: 0.5746 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 19s 213ms/step - loss: 1.2621 - acc: 0.5535 - top5-acc: 0.9439 - val_loss: 1.1863 - val_acc: 0.5816 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 222ms/step - loss: 1.2658 - acc: 0.5510 - top5-acc: 0.9446 - val_loss: 1.1861 - val_acc: 0.5744 - val_top5-acc: 0.9570 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 19s 222ms/step - loss: 1.2615 - acc: 0.5534 - top5-acc: 0.9441 - val_loss: 1.1811 - val_acc: 0.5788 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 19s 216ms/step - loss: 1.2587 - acc: 0.5546 - top5-acc: 0.9460 - val_loss: 1.1832 - val_acc: 0.5838 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 19s 212ms/step - loss: 1.2616 - acc: 0.5520 - top5-acc: 0.9435 - val_loss: 1.1750 - val_acc: 0.5846 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 19s 214ms/step - loss: 1.2602 - acc: 0.5527 - top5-acc: 0.9458 - val_loss: 1.1821 - val_acc: 0.5766 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 19s 214ms/step - loss: 1.2626 - acc: 0.5531 - top5-acc: 0.9434 - val_loss: 1.1849 - val_acc: 0.5808 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 19s 215ms/step - loss: 1.2579 - acc: 0.5549 - top5-acc: 0.9444 - val_loss: 1.1674 - val_acc: 0.5868 - val_top5-acc: 0.9570 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 19s 213ms/step - loss: 1.2543 - acc: 0.5569 - top5-acc: 0.9448 - val_loss: 1.1656 - val_acc: 0.5864 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 19s 213ms/step - loss: 1.2645 - acc: 0.5504 - top5-acc: 0.9430 - val_loss: 1.1832 - val_acc: 0.5746 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 19s 216ms/step - loss: 1.2678 - acc: 0.5504 - top5-acc: 0.9427 - val_loss: 1.1740 - val_acc: 0.5802 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 19s 217ms/step - loss: 1.2530 - acc: 0.5553 - top5-acc: 0.9461 - val_loss: 1.1700 - val_acc: 0.5814 - val_top5-acc: 0.9576 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2540 - acc: 0.5549 - top5-acc: 0.9449 - val_loss: 1.1987 - val_acc: 0.5672 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 19s 218ms/step - loss: 1.2558 - acc: 0.5538 - top5-acc: 0.9448 - val_loss: 1.1695 - val_acc: 0.5828 - val_top5-acc: 0.9588 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 19s 221ms/step - loss: 1.2358 - acc: 0.5635 - top5-acc: 0.9469 - val_loss: 1.1547 - val_acc: 0.5874 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2391 - acc: 0.5604 - top5-acc: 0.9465 - val_loss: 1.1720 - val_acc: 0.5814 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2437 - acc: 0.5584 - top5-acc: 0.9470 - val_loss: 1.1636 - val_acc: 0.5832 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2444 - acc: 0.5572 - top5-acc: 0.9463 - val_loss: 1.1567 - val_acc: 0.5876 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 1.2418 - acc: 0.5613 - top5-acc: 0.9460 - val_loss: 1.1527 - val_acc: 0.5912 - val_top5-acc: 0.9608 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2389 - acc: 0.5615 - top5-acc: 0.9471 - val_loss: 1.1573 - val_acc: 0.5888 - val_top5-acc: 0.9566 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.2422 - acc: 0.5594 - top5-acc: 0.9464 - val_loss: 1.1662 - val_acc: 0.5812 - val_top5-acc: 0.9596 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2379 - acc: 0.5627 - top5-acc: 0.9475 - val_loss: 1.1529 - val_acc: 0.5908 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 222ms/step - loss: 1.2407 - acc: 0.5609 - top5-acc: 0.9478 - val_loss: 1.1613 - val_acc: 0.5886 - val_top5-acc: 0.9608 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.2387 - acc: 0.5608 - top5-acc: 0.9471 - val_loss: 1.1594 - val_acc: 0.5880 - val_top5-acc: 0.9584 - lr: 0.0025\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.2366 - acc: 0.5636 - top5-acc: 0.9472 - val_loss: 1.1496 - val_acc: 0.5940 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2368 - acc: 0.5616 - top5-acc: 0.9446 - val_loss: 1.1535 - val_acc: 0.5918 - val_top5-acc: 0.9598 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.2397 - acc: 0.5599 - top5-acc: 0.9469 - val_loss: 1.1500 - val_acc: 0.5912 - val_top5-acc: 0.9586 - lr: 0.0012\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 224ms/step - loss: 1.2342 - acc: 0.5658 - top5-acc: 0.9466 - val_loss: 1.1591 - val_acc: 0.5910 - val_top5-acc: 0.9596 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 227ms/step - loss: 1.2332 - acc: 0.5626 - top5-acc: 0.9460 - val_loss: 1.1484 - val_acc: 0.5930 - val_top5-acc: 0.9592 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2334 - acc: 0.5659 - top5-acc: 0.9468 - val_loss: 1.1592 - val_acc: 0.5872 - val_top5-acc: 0.9566 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 228ms/step - loss: 1.2346 - acc: 0.5651 - top5-acc: 0.9463 - val_loss: 1.1507 - val_acc: 0.5910 - val_top5-acc: 0.9604 - lr: 0.0012\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 1.1655 - acc: 0.5846 - top5-acc: 0.9545\n",
      "Test accuracy: 58.46%\n",
      "Test top 5 accuracy: 95.45%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 37s 254ms/step - loss: 1.7343 - acc: 0.3890 - top5-acc: 0.8553 - val_loss: 1.4885 - val_acc: 0.4706 - val_top5-acc: 0.9184 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.4735 - acc: 0.4800 - top5-acc: 0.9172 - val_loss: 1.3908 - val_acc: 0.5066 - val_top5-acc: 0.9338 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.4077 - acc: 0.5047 - top5-acc: 0.9273 - val_loss: 1.3282 - val_acc: 0.5270 - val_top5-acc: 0.9412 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3727 - acc: 0.5188 - top5-acc: 0.9313 - val_loss: 1.3172 - val_acc: 0.5232 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.3536 - acc: 0.5221 - top5-acc: 0.9335 - val_loss: 1.2842 - val_acc: 0.5498 - val_top5-acc: 0.9470 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.3312 - acc: 0.5302 - top5-acc: 0.9362 - val_loss: 1.2634 - val_acc: 0.5526 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3238 - acc: 0.5335 - top5-acc: 0.9376 - val_loss: 1.2895 - val_acc: 0.5348 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.3166 - acc: 0.5350 - top5-acc: 0.9368 - val_loss: 1.2470 - val_acc: 0.5550 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3005 - acc: 0.5414 - top5-acc: 0.9407 - val_loss: 1.2319 - val_acc: 0.5590 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2994 - acc: 0.5403 - top5-acc: 0.9400 - val_loss: 1.2363 - val_acc: 0.5630 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2931 - acc: 0.5442 - top5-acc: 0.9395 - val_loss: 1.2392 - val_acc: 0.5508 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2885 - acc: 0.5432 - top5-acc: 0.9410 - val_loss: 1.2189 - val_acc: 0.5696 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2862 - acc: 0.5419 - top5-acc: 0.9412 - val_loss: 1.2313 - val_acc: 0.5566 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2762 - acc: 0.5493 - top5-acc: 0.9421 - val_loss: 1.2344 - val_acc: 0.5580 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2795 - acc: 0.5475 - top5-acc: 0.9427 - val_loss: 1.2211 - val_acc: 0.5688 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.2763 - acc: 0.5498 - top5-acc: 0.9428 - val_loss: 1.1941 - val_acc: 0.5748 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2680 - acc: 0.5508 - top5-acc: 0.9449 - val_loss: 1.2117 - val_acc: 0.5660 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 22s 247ms/step - loss: 1.2820 - acc: 0.5459 - top5-acc: 0.9426 - val_loss: 1.1991 - val_acc: 0.5702 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2732 - acc: 0.5494 - top5-acc: 0.9435 - val_loss: 1.1883 - val_acc: 0.5746 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2707 - acc: 0.5498 - top5-acc: 0.9441 - val_loss: 1.1971 - val_acc: 0.5728 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2741 - acc: 0.5481 - top5-acc: 0.9422 - val_loss: 1.1906 - val_acc: 0.5702 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2638 - acc: 0.5493 - top5-acc: 0.9443 - val_loss: 1.1945 - val_acc: 0.5726 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2631 - acc: 0.5499 - top5-acc: 0.9445 - val_loss: 1.2006 - val_acc: 0.5650 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2583 - acc: 0.5536 - top5-acc: 0.9447 - val_loss: 1.2060 - val_acc: 0.5648 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.2472 - acc: 0.5589 - top5-acc: 0.9449 - val_loss: 1.1692 - val_acc: 0.5816 - val_top5-acc: 0.9576 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2511 - acc: 0.5575 - top5-acc: 0.9456 - val_loss: 1.1760 - val_acc: 0.5810 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2492 - acc: 0.5587 - top5-acc: 0.9456 - val_loss: 1.1971 - val_acc: 0.5648 - val_top5-acc: 0.9590 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2483 - acc: 0.5561 - top5-acc: 0.9458 - val_loss: 1.1713 - val_acc: 0.5796 - val_top5-acc: 0.9592 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.2479 - acc: 0.5567 - top5-acc: 0.9458 - val_loss: 1.1854 - val_acc: 0.5770 - val_top5-acc: 0.9552 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2475 - acc: 0.5605 - top5-acc: 0.9456 - val_loss: 1.1762 - val_acc: 0.5768 - val_top5-acc: 0.9574 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2401 - acc: 0.5624 - top5-acc: 0.9465 - val_loss: 1.1669 - val_acc: 0.5842 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2363 - acc: 0.5649 - top5-acc: 0.9468 - val_loss: 1.1793 - val_acc: 0.5752 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2427 - acc: 0.5603 - top5-acc: 0.9466 - val_loss: 1.1634 - val_acc: 0.5850 - val_top5-acc: 0.9578 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.2481 - acc: 0.5596 - top5-acc: 0.9451 - val_loss: 1.1701 - val_acc: 0.5780 - val_top5-acc: 0.9582 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2429 - acc: 0.5616 - top5-acc: 0.9454 - val_loss: 1.1689 - val_acc: 0.5848 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2448 - acc: 0.5576 - top5-acc: 0.9462 - val_loss: 1.1706 - val_acc: 0.5822 - val_top5-acc: 0.9584 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2425 - acc: 0.5601 - top5-acc: 0.9469 - val_loss: 1.1638 - val_acc: 0.5858 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2428 - acc: 0.5626 - top5-acc: 0.9460 - val_loss: 1.1721 - val_acc: 0.5836 - val_top5-acc: 0.9566 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2394 - acc: 0.5638 - top5-acc: 0.9471 - val_loss: 1.1724 - val_acc: 0.5788 - val_top5-acc: 0.9606 - lr: 6.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2410 - acc: 0.5600 - top5-acc: 0.9462 - val_loss: 1.1677 - val_acc: 0.5824 - val_top5-acc: 0.9582 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 23s 256ms/step - loss: 1.2396 - acc: 0.5635 - top5-acc: 0.9466 - val_loss: 1.1695 - val_acc: 0.5832 - val_top5-acc: 0.9592 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.2387 - acc: 0.5638 - top5-acc: 0.9467 - val_loss: 1.1734 - val_acc: 0.5838 - val_top5-acc: 0.9578 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2385 - acc: 0.5647 - top5-acc: 0.9466 - val_loss: 1.1683 - val_acc: 0.5834 - val_top5-acc: 0.9584 - lr: 6.2500e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2342 - acc: 0.5639 - top5-acc: 0.9492 - val_loss: 1.1673 - val_acc: 0.5790 - val_top5-acc: 0.9582 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2355 - acc: 0.5667 - top5-acc: 0.9468 - val_loss: 1.1693 - val_acc: 0.5812 - val_top5-acc: 0.9572 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2406 - acc: 0.5656 - top5-acc: 0.9468 - val_loss: 1.1745 - val_acc: 0.5820 - val_top5-acc: 0.9570 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 23s 264ms/step - loss: 1.2391 - acc: 0.5634 - top5-acc: 0.9469 - val_loss: 1.1702 - val_acc: 0.5828 - val_top5-acc: 0.9578 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.2450 - acc: 0.5604 - top5-acc: 0.9461 - val_loss: 1.1733 - val_acc: 0.5838 - val_top5-acc: 0.9576 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2424 - acc: 0.5628 - top5-acc: 0.9482 - val_loss: 1.1704 - val_acc: 0.5812 - val_top5-acc: 0.9580 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2420 - acc: 0.5623 - top5-acc: 0.9468 - val_loss: 1.1727 - val_acc: 0.5834 - val_top5-acc: 0.9582 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 1.1815 - acc: 0.5832 - top5-acc: 0.9553\n",
      "Test accuracy: 58.32%\n",
      "Test top 5 accuracy: 95.53%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 277ms/step - loss: 1.7560 - acc: 0.3832 - top5-acc: 0.8522 - val_loss: 1.5016 - val_acc: 0.4696 - val_top5-acc: 0.9194 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 1.4854 - acc: 0.4783 - top5-acc: 0.9170 - val_loss: 1.4268 - val_acc: 0.4838 - val_top5-acc: 0.9246 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.4281 - acc: 0.4961 - top5-acc: 0.9242 - val_loss: 1.3771 - val_acc: 0.5066 - val_top5-acc: 0.9356 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 22s 253ms/step - loss: 1.3870 - acc: 0.5106 - top5-acc: 0.9301 - val_loss: 1.3451 - val_acc: 0.5248 - val_top5-acc: 0.9416 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.3697 - acc: 0.5148 - top5-acc: 0.9330 - val_loss: 1.2990 - val_acc: 0.5376 - val_top5-acc: 0.9470 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 22s 252ms/step - loss: 1.3488 - acc: 0.5215 - top5-acc: 0.9349 - val_loss: 1.2969 - val_acc: 0.5406 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.3330 - acc: 0.5275 - top5-acc: 0.9385 - val_loss: 1.2635 - val_acc: 0.5472 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3245 - acc: 0.5298 - top5-acc: 0.9383 - val_loss: 1.2594 - val_acc: 0.5524 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.3210 - acc: 0.5292 - top5-acc: 0.9383 - val_loss: 1.2527 - val_acc: 0.5576 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.3114 - acc: 0.5366 - top5-acc: 0.9380 - val_loss: 1.2440 - val_acc: 0.5522 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2975 - acc: 0.5414 - top5-acc: 0.9403 - val_loss: 1.2470 - val_acc: 0.5602 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3074 - acc: 0.5350 - top5-acc: 0.9390 - val_loss: 1.2189 - val_acc: 0.5698 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2913 - acc: 0.5427 - top5-acc: 0.9410 - val_loss: 1.2444 - val_acc: 0.5606 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2873 - acc: 0.5428 - top5-acc: 0.9421 - val_loss: 1.2264 - val_acc: 0.5648 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 1.2876 - acc: 0.5412 - top5-acc: 0.9430 - val_loss: 1.2287 - val_acc: 0.5610 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2813 - acc: 0.5462 - top5-acc: 0.9419 - val_loss: 1.2045 - val_acc: 0.5712 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.2777 - acc: 0.5479 - top5-acc: 0.9433 - val_loss: 1.2169 - val_acc: 0.5660 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2728 - acc: 0.5490 - top5-acc: 0.9432 - val_loss: 1.1954 - val_acc: 0.5770 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2733 - acc: 0.5507 - top5-acc: 0.9432 - val_loss: 1.1978 - val_acc: 0.5744 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 22s 253ms/step - loss: 1.2715 - acc: 0.5520 - top5-acc: 0.9424 - val_loss: 1.2087 - val_acc: 0.5630 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2747 - acc: 0.5486 - top5-acc: 0.9417 - val_loss: 1.1970 - val_acc: 0.5720 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2620 - acc: 0.5537 - top5-acc: 0.9439 - val_loss: 1.2025 - val_acc: 0.5724 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 21s 236ms/step - loss: 1.2656 - acc: 0.5486 - top5-acc: 0.9438 - val_loss: 1.2053 - val_acc: 0.5662 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.2535 - acc: 0.5578 - top5-acc: 0.9445 - val_loss: 1.1804 - val_acc: 0.5860 - val_top5-acc: 0.9568 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2561 - acc: 0.5531 - top5-acc: 0.9455 - val_loss: 1.1828 - val_acc: 0.5800 - val_top5-acc: 0.9570 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 22s 247ms/step - loss: 1.2545 - acc: 0.5548 - top5-acc: 0.9453 - val_loss: 1.1730 - val_acc: 0.5802 - val_top5-acc: 0.9562 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2540 - acc: 0.5559 - top5-acc: 0.9450 - val_loss: 1.1936 - val_acc: 0.5734 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2566 - acc: 0.5552 - top5-acc: 0.9445 - val_loss: 1.1880 - val_acc: 0.5756 - val_top5-acc: 0.9554 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2548 - acc: 0.5549 - top5-acc: 0.9453 - val_loss: 1.1767 - val_acc: 0.5840 - val_top5-acc: 0.9584 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2555 - acc: 0.5554 - top5-acc: 0.9441 - val_loss: 1.1750 - val_acc: 0.5842 - val_top5-acc: 0.9588 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2542 - acc: 0.5600 - top5-acc: 0.9456 - val_loss: 1.1839 - val_acc: 0.5760 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2504 - acc: 0.5581 - top5-acc: 0.9459 - val_loss: 1.1702 - val_acc: 0.5864 - val_top5-acc: 0.9584 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2458 - acc: 0.5612 - top5-acc: 0.9465 - val_loss: 1.1780 - val_acc: 0.5856 - val_top5-acc: 0.9568 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 22s 247ms/step - loss: 1.2499 - acc: 0.5593 - top5-acc: 0.9454 - val_loss: 1.1715 - val_acc: 0.5858 - val_top5-acc: 0.9554 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.2513 - acc: 0.5584 - top5-acc: 0.9464 - val_loss: 1.1741 - val_acc: 0.5868 - val_top5-acc: 0.9566 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2520 - acc: 0.5592 - top5-acc: 0.9449 - val_loss: 1.1749 - val_acc: 0.5886 - val_top5-acc: 0.9556 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 22s 245ms/step - loss: 1.2438 - acc: 0.5594 - top5-acc: 0.9473 - val_loss: 1.1818 - val_acc: 0.5854 - val_top5-acc: 0.9568 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2485 - acc: 0.5587 - top5-acc: 0.9491 - val_loss: 1.1740 - val_acc: 0.5880 - val_top5-acc: 0.9578 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.2496 - acc: 0.5559 - top5-acc: 0.9458 - val_loss: 1.1742 - val_acc: 0.5870 - val_top5-acc: 0.9584 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2446 - acc: 0.5602 - top5-acc: 0.9476 - val_loss: 1.1721 - val_acc: 0.5830 - val_top5-acc: 0.9584 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2489 - acc: 0.5594 - top5-acc: 0.9467 - val_loss: 1.1747 - val_acc: 0.5878 - val_top5-acc: 0.9574 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2489 - acc: 0.5578 - top5-acc: 0.9469 - val_loss: 1.1812 - val_acc: 0.5836 - val_top5-acc: 0.9580 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2450 - acc: 0.5615 - top5-acc: 0.9469 - val_loss: 1.1780 - val_acc: 0.5876 - val_top5-acc: 0.9576 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.2475 - acc: 0.5605 - top5-acc: 0.9457 - val_loss: 1.1751 - val_acc: 0.5876 - val_top5-acc: 0.9580 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 22s 248ms/step - loss: 1.2445 - acc: 0.5624 - top5-acc: 0.9474 - val_loss: 1.1751 - val_acc: 0.5898 - val_top5-acc: 0.9592 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 21s 239ms/step - loss: 1.2445 - acc: 0.5610 - top5-acc: 0.9468 - val_loss: 1.1803 - val_acc: 0.5844 - val_top5-acc: 0.9580 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2450 - acc: 0.5640 - top5-acc: 0.9471 - val_loss: 1.1755 - val_acc: 0.5882 - val_top5-acc: 0.9586 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 1.2510 - acc: 0.5611 - top5-acc: 0.9460 - val_loss: 1.1771 - val_acc: 0.5886 - val_top5-acc: 0.9582 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2460 - acc: 0.5642 - top5-acc: 0.9470 - val_loss: 1.1779 - val_acc: 0.5892 - val_top5-acc: 0.9578 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 22s 245ms/step - loss: 1.2500 - acc: 0.5614 - top5-acc: 0.9480 - val_loss: 1.1788 - val_acc: 0.5876 - val_top5-acc: 0.9576 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.1960 - acc: 0.5794 - top5-acc: 0.9508\n",
      "Test accuracy: 57.94%\n",
      "Test top 5 accuracy: 95.08%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 276ms/step - loss: 1.7632 - acc: 0.3785 - top5-acc: 0.8498 - val_loss: 1.4869 - val_acc: 0.4714 - val_top5-acc: 0.9190 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 22s 247ms/step - loss: 1.4928 - acc: 0.4754 - top5-acc: 0.9148 - val_loss: 1.3908 - val_acc: 0.5058 - val_top5-acc: 0.9276 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.4255 - acc: 0.4984 - top5-acc: 0.9238 - val_loss: 1.3572 - val_acc: 0.5148 - val_top5-acc: 0.9392 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.3880 - acc: 0.5127 - top5-acc: 0.9299 - val_loss: 1.3099 - val_acc: 0.5430 - val_top5-acc: 0.9426 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 23s 264ms/step - loss: 1.3655 - acc: 0.5172 - top5-acc: 0.9336 - val_loss: 1.3044 - val_acc: 0.5316 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.3443 - acc: 0.5236 - top5-acc: 0.9363 - val_loss: 1.2739 - val_acc: 0.5506 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.3308 - acc: 0.5296 - top5-acc: 0.9373 - val_loss: 1.2714 - val_acc: 0.5474 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 22s 254ms/step - loss: 1.3242 - acc: 0.5307 - top5-acc: 0.9368 - val_loss: 1.2652 - val_acc: 0.5542 - val_top5-acc: 0.9470 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.3136 - acc: 0.5349 - top5-acc: 0.9399 - val_loss: 1.2669 - val_acc: 0.5514 - val_top5-acc: 0.9472 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 21s 243ms/step - loss: 1.2947 - acc: 0.5445 - top5-acc: 0.9424 - val_loss: 1.2233 - val_acc: 0.5652 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2994 - acc: 0.5417 - top5-acc: 0.9398 - val_loss: 1.2463 - val_acc: 0.5586 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 21s 244ms/step - loss: 1.2859 - acc: 0.5456 - top5-acc: 0.9428 - val_loss: 1.2265 - val_acc: 0.5666 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 22s 247ms/step - loss: 1.2904 - acc: 0.5426 - top5-acc: 0.9406 - val_loss: 1.2213 - val_acc: 0.5608 - val_top5-acc: 0.9584 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.2902 - acc: 0.5435 - top5-acc: 0.9402 - val_loss: 1.2002 - val_acc: 0.5776 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 22s 254ms/step - loss: 1.2750 - acc: 0.5501 - top5-acc: 0.9428 - val_loss: 1.2283 - val_acc: 0.5710 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2761 - acc: 0.5507 - top5-acc: 0.9424 - val_loss: 1.1895 - val_acc: 0.5790 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2710 - acc: 0.5527 - top5-acc: 0.9423 - val_loss: 1.2021 - val_acc: 0.5808 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2707 - acc: 0.5500 - top5-acc: 0.9433 - val_loss: 1.1912 - val_acc: 0.5784 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2597 - acc: 0.5542 - top5-acc: 0.9450 - val_loss: 1.2068 - val_acc: 0.5712 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2599 - acc: 0.5522 - top5-acc: 0.9442 - val_loss: 1.1764 - val_acc: 0.5820 - val_top5-acc: 0.9554 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2617 - acc: 0.5553 - top5-acc: 0.9440 - val_loss: 1.1953 - val_acc: 0.5786 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2650 - acc: 0.5530 - top5-acc: 0.9439 - val_loss: 1.2014 - val_acc: 0.5740 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.2613 - acc: 0.5544 - top5-acc: 0.9460 - val_loss: 1.1729 - val_acc: 0.5858 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2599 - acc: 0.5533 - top5-acc: 0.9443 - val_loss: 1.1890 - val_acc: 0.5832 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 22s 246ms/step - loss: 1.2568 - acc: 0.5541 - top5-acc: 0.9437 - val_loss: 1.1669 - val_acc: 0.5898 - val_top5-acc: 0.9578 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 22s 246ms/step - loss: 1.2669 - acc: 0.5505 - top5-acc: 0.9447 - val_loss: 1.2113 - val_acc: 0.5700 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 22s 246ms/step - loss: 1.2584 - acc: 0.5545 - top5-acc: 0.9433 - val_loss: 1.1756 - val_acc: 0.5826 - val_top5-acc: 0.9592 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 22s 246ms/step - loss: 1.2606 - acc: 0.5513 - top5-acc: 0.9454 - val_loss: 1.1914 - val_acc: 0.5726 - val_top5-acc: 0.9582 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 22s 246ms/step - loss: 1.2581 - acc: 0.5540 - top5-acc: 0.9459 - val_loss: 1.2066 - val_acc: 0.5744 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 22s 248ms/step - loss: 1.2507 - acc: 0.5581 - top5-acc: 0.9463 - val_loss: 1.2035 - val_acc: 0.5686 - val_top5-acc: 0.9584 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2423 - acc: 0.5611 - top5-acc: 0.9464 - val_loss: 1.1600 - val_acc: 0.5884 - val_top5-acc: 0.9604 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 22s 248ms/step - loss: 1.2374 - acc: 0.5642 - top5-acc: 0.9460 - val_loss: 1.1540 - val_acc: 0.5916 - val_top5-acc: 0.9586 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2435 - acc: 0.5613 - top5-acc: 0.9465 - val_loss: 1.1503 - val_acc: 0.5938 - val_top5-acc: 0.9606 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 22s 248ms/step - loss: 1.2392 - acc: 0.5610 - top5-acc: 0.9462 - val_loss: 1.1586 - val_acc: 0.5922 - val_top5-acc: 0.9578 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2380 - acc: 0.5649 - top5-acc: 0.9460 - val_loss: 1.1679 - val_acc: 0.5868 - val_top5-acc: 0.9590 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2351 - acc: 0.5652 - top5-acc: 0.9467 - val_loss: 1.1578 - val_acc: 0.5868 - val_top5-acc: 0.9598 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 22s 255ms/step - loss: 1.2351 - acc: 0.5650 - top5-acc: 0.9472 - val_loss: 1.1640 - val_acc: 0.5876 - val_top5-acc: 0.9600 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 22s 247ms/step - loss: 1.2380 - acc: 0.5628 - top5-acc: 0.9465 - val_loss: 1.1583 - val_acc: 0.5894 - val_top5-acc: 0.9606 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 22s 249ms/step - loss: 1.2326 - acc: 0.5631 - top5-acc: 0.9462 - val_loss: 1.1535 - val_acc: 0.5922 - val_top5-acc: 0.9620 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 22s 247ms/step - loss: 1.2327 - acc: 0.5652 - top5-acc: 0.9478 - val_loss: 1.1579 - val_acc: 0.5926 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 22s 252ms/step - loss: 1.2334 - acc: 0.5636 - top5-acc: 0.9468 - val_loss: 1.1591 - val_acc: 0.5896 - val_top5-acc: 0.9600 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.2300 - acc: 0.5658 - top5-acc: 0.9481 - val_loss: 1.1523 - val_acc: 0.5930 - val_top5-acc: 0.9586 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 21s 240ms/step - loss: 1.2329 - acc: 0.5664 - top5-acc: 0.9469 - val_loss: 1.1595 - val_acc: 0.5914 - val_top5-acc: 0.9604 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2301 - acc: 0.5670 - top5-acc: 0.9485 - val_loss: 1.1531 - val_acc: 0.5934 - val_top5-acc: 0.9566 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2284 - acc: 0.5686 - top5-acc: 0.9479 - val_loss: 1.1554 - val_acc: 0.5932 - val_top5-acc: 0.9594 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2309 - acc: 0.5649 - top5-acc: 0.9477 - val_loss: 1.1579 - val_acc: 0.5914 - val_top5-acc: 0.9602 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2313 - acc: 0.5670 - top5-acc: 0.9470 - val_loss: 1.1550 - val_acc: 0.5934 - val_top5-acc: 0.9604 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.2306 - acc: 0.5662 - top5-acc: 0.9466 - val_loss: 1.1594 - val_acc: 0.5936 - val_top5-acc: 0.9606 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2332 - acc: 0.5686 - top5-acc: 0.9470 - val_loss: 1.1572 - val_acc: 0.5942 - val_top5-acc: 0.9608 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 21s 241ms/step - loss: 1.2290 - acc: 0.5675 - top5-acc: 0.9472 - val_loss: 1.1578 - val_acc: 0.5918 - val_top5-acc: 0.9600 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 37s 118ms/step - loss: 1.1784 - acc: 0.5835 - top5-acc: 0.9518\n",
      "Test accuracy: 58.35%\n",
      "Test top 5 accuracy: 95.18%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 277ms/step - loss: 1.7182 - acc: 0.3934 - top5-acc: 0.8600 - val_loss: 1.4652 - val_acc: 0.4838 - val_top5-acc: 0.9264 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 1.4580 - acc: 0.4880 - top5-acc: 0.9197 - val_loss: 1.3643 - val_acc: 0.5166 - val_top5-acc: 0.9336 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.3913 - acc: 0.5085 - top5-acc: 0.9306 - val_loss: 1.3272 - val_acc: 0.5258 - val_top5-acc: 0.9370 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.3524 - acc: 0.5238 - top5-acc: 0.9341 - val_loss: 1.3083 - val_acc: 0.5366 - val_top5-acc: 0.9420 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 22s 250ms/step - loss: 1.3296 - acc: 0.5314 - top5-acc: 0.9358 - val_loss: 1.2715 - val_acc: 0.5490 - val_top5-acc: 0.9438 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 22s 253ms/step - loss: 1.3188 - acc: 0.5348 - top5-acc: 0.9374 - val_loss: 1.2717 - val_acc: 0.5516 - val_top5-acc: 0.9454 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 1.2996 - acc: 0.5416 - top5-acc: 0.9405 - val_loss: 1.2460 - val_acc: 0.5580 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 1.2914 - acc: 0.5409 - top5-acc: 0.9409 - val_loss: 1.2367 - val_acc: 0.5600 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 23s 261ms/step - loss: 1.2850 - acc: 0.5461 - top5-acc: 0.9421 - val_loss: 1.2080 - val_acc: 0.5776 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.2761 - acc: 0.5515 - top5-acc: 0.9413 - val_loss: 1.2254 - val_acc: 0.5716 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.2706 - acc: 0.5514 - top5-acc: 0.9430 - val_loss: 1.2153 - val_acc: 0.5704 - val_top5-acc: 0.9494 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 1.2632 - acc: 0.5532 - top5-acc: 0.9435 - val_loss: 1.1888 - val_acc: 0.5790 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 1.2592 - acc: 0.5542 - top5-acc: 0.9442 - val_loss: 1.1983 - val_acc: 0.5788 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.2535 - acc: 0.5576 - top5-acc: 0.9438 - val_loss: 1.1796 - val_acc: 0.5838 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.2499 - acc: 0.5583 - top5-acc: 0.9446 - val_loss: 1.2075 - val_acc: 0.5762 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 23s 260ms/step - loss: 1.2513 - acc: 0.5568 - top5-acc: 0.9447 - val_loss: 1.1770 - val_acc: 0.5856 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 23s 261ms/step - loss: 1.2459 - acc: 0.5594 - top5-acc: 0.9454 - val_loss: 1.1866 - val_acc: 0.5816 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 1.2470 - acc: 0.5567 - top5-acc: 0.9438 - val_loss: 1.1878 - val_acc: 0.5810 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 1.2460 - acc: 0.5571 - top5-acc: 0.9464 - val_loss: 1.1741 - val_acc: 0.5842 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 23s 262ms/step - loss: 1.2437 - acc: 0.5595 - top5-acc: 0.9472 - val_loss: 1.1808 - val_acc: 0.5740 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 1.2451 - acc: 0.5606 - top5-acc: 0.9458 - val_loss: 1.1859 - val_acc: 0.5804 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.2417 - acc: 0.5606 - top5-acc: 0.9455 - val_loss: 1.1886 - val_acc: 0.5806 - val_top5-acc: 0.9550 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 23s 265ms/step - loss: 1.2382 - acc: 0.5612 - top5-acc: 0.9452 - val_loss: 1.1544 - val_acc: 0.5912 - val_top5-acc: 0.9570 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.2397 - acc: 0.5625 - top5-acc: 0.9458 - val_loss: 1.1697 - val_acc: 0.5888 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.2361 - acc: 0.5621 - top5-acc: 0.9475 - val_loss: 1.1505 - val_acc: 0.5928 - val_top5-acc: 0.9582 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.2348 - acc: 0.5631 - top5-acc: 0.9470 - val_loss: 1.1765 - val_acc: 0.5758 - val_top5-acc: 0.9562 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 23s 262ms/step - loss: 1.2398 - acc: 0.5610 - top5-acc: 0.9473 - val_loss: 1.1595 - val_acc: 0.5896 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.2343 - acc: 0.5644 - top5-acc: 0.9462 - val_loss: 1.1821 - val_acc: 0.5884 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.2363 - acc: 0.5632 - top5-acc: 0.9468 - val_loss: 1.1792 - val_acc: 0.5838 - val_top5-acc: 0.9546 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 23s 257ms/step - loss: 1.2272 - acc: 0.5670 - top5-acc: 0.9481 - val_loss: 1.1507 - val_acc: 0.5938 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.2162 - acc: 0.5695 - top5-acc: 0.9498 - val_loss: 1.1589 - val_acc: 0.5882 - val_top5-acc: 0.9600 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 1.2219 - acc: 0.5684 - top5-acc: 0.9480 - val_loss: 1.1613 - val_acc: 0.5880 - val_top5-acc: 0.9556 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.2173 - acc: 0.5728 - top5-acc: 0.9487 - val_loss: 1.1405 - val_acc: 0.5924 - val_top5-acc: 0.9584 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.2225 - acc: 0.5704 - top5-acc: 0.9492 - val_loss: 1.1746 - val_acc: 0.5816 - val_top5-acc: 0.9544 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 1.2192 - acc: 0.5708 - top5-acc: 0.9490 - val_loss: 1.1354 - val_acc: 0.5972 - val_top5-acc: 0.9610 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.2204 - acc: 0.5690 - top5-acc: 0.9497 - val_loss: 1.1520 - val_acc: 0.5908 - val_top5-acc: 0.9596 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 1.2160 - acc: 0.5715 - top5-acc: 0.9491 - val_loss: 1.1427 - val_acc: 0.5938 - val_top5-acc: 0.9602 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 23s 264ms/step - loss: 1.2187 - acc: 0.5704 - top5-acc: 0.9486 - val_loss: 1.1598 - val_acc: 0.5910 - val_top5-acc: 0.9570 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 22s 253ms/step - loss: 1.2189 - acc: 0.5714 - top5-acc: 0.9486 - val_loss: 1.1408 - val_acc: 0.5982 - val_top5-acc: 0.9588 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 23s 262ms/step - loss: 1.2217 - acc: 0.5692 - top5-acc: 0.9458 - val_loss: 1.1453 - val_acc: 0.5962 - val_top5-acc: 0.9590 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 23s 260ms/step - loss: 1.2104 - acc: 0.5732 - top5-acc: 0.9491 - val_loss: 1.1446 - val_acc: 0.5960 - val_top5-acc: 0.9570 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 23s 264ms/step - loss: 1.2129 - acc: 0.5725 - top5-acc: 0.9495 - val_loss: 1.1484 - val_acc: 0.5972 - val_top5-acc: 0.9562 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 23s 263ms/step - loss: 1.2151 - acc: 0.5694 - top5-acc: 0.9493 - val_loss: 1.1433 - val_acc: 0.5950 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 23s 261ms/step - loss: 1.2109 - acc: 0.5723 - top5-acc: 0.9498 - val_loss: 1.1461 - val_acc: 0.5926 - val_top5-acc: 0.9574 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 23s 262ms/step - loss: 1.2104 - acc: 0.5751 - top5-acc: 0.9496 - val_loss: 1.1477 - val_acc: 0.5966 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 1.2026 - acc: 0.5750 - top5-acc: 0.9507 - val_loss: 1.1405 - val_acc: 0.5946 - val_top5-acc: 0.9598 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 23s 261ms/step - loss: 1.2109 - acc: 0.5736 - top5-acc: 0.9495 - val_loss: 1.1421 - val_acc: 0.5970 - val_top5-acc: 0.9592 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 23s 266ms/step - loss: 1.2097 - acc: 0.5748 - top5-acc: 0.9494 - val_loss: 1.1452 - val_acc: 0.5960 - val_top5-acc: 0.9590 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 23s 262ms/step - loss: 1.2073 - acc: 0.5777 - top5-acc: 0.9494 - val_loss: 1.1428 - val_acc: 0.5982 - val_top5-acc: 0.9596 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 23s 256ms/step - loss: 1.2066 - acc: 0.5765 - top5-acc: 0.9491 - val_loss: 1.1454 - val_acc: 0.5958 - val_top5-acc: 0.9582 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 38s 121ms/step - loss: 1.1615 - acc: 0.5902 - top5-acc: 0.9542\n",
      "Test accuracy: 59.02%\n",
      "Test top 5 accuracy: 95.42%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 33s 302ms/step - loss: 1.7031 - acc: 0.3961 - top5-acc: 0.8635 - val_loss: 1.4444 - val_acc: 0.4932 - val_top5-acc: 0.9230 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 25s 280ms/step - loss: 1.4481 - acc: 0.4902 - top5-acc: 0.9223 - val_loss: 1.3452 - val_acc: 0.5284 - val_top5-acc: 0.9366 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 24s 273ms/step - loss: 1.3871 - acc: 0.5086 - top5-acc: 0.9306 - val_loss: 1.3196 - val_acc: 0.5354 - val_top5-acc: 0.9380 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 25s 281ms/step - loss: 1.3529 - acc: 0.5220 - top5-acc: 0.9336 - val_loss: 1.2851 - val_acc: 0.5472 - val_top5-acc: 0.9412 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 1.3335 - acc: 0.5282 - top5-acc: 0.9359 - val_loss: 1.2607 - val_acc: 0.5530 - val_top5-acc: 0.9448 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 24s 276ms/step - loss: 1.3134 - acc: 0.5352 - top5-acc: 0.9381 - val_loss: 1.2372 - val_acc: 0.5640 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2995 - acc: 0.5416 - top5-acc: 0.9400 - val_loss: 1.2291 - val_acc: 0.5652 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2932 - acc: 0.5422 - top5-acc: 0.9406 - val_loss: 1.2159 - val_acc: 0.5734 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2907 - acc: 0.5425 - top5-acc: 0.9420 - val_loss: 1.2563 - val_acc: 0.5484 - val_top5-acc: 0.9436 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 1.2834 - acc: 0.5476 - top5-acc: 0.9418 - val_loss: 1.2295 - val_acc: 0.5598 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2841 - acc: 0.5445 - top5-acc: 0.9419 - val_loss: 1.2272 - val_acc: 0.5600 - val_top5-acc: 0.9508 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 1.2628 - acc: 0.5517 - top5-acc: 0.9430 - val_loss: 1.2133 - val_acc: 0.5606 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2607 - acc: 0.5538 - top5-acc: 0.9461 - val_loss: 1.2275 - val_acc: 0.5626 - val_top5-acc: 0.9526 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2664 - acc: 0.5519 - top5-acc: 0.9434 - val_loss: 1.2019 - val_acc: 0.5672 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 1.2677 - acc: 0.5522 - top5-acc: 0.9429 - val_loss: 1.1914 - val_acc: 0.5744 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 1.2503 - acc: 0.5540 - top5-acc: 0.9447 - val_loss: 1.2002 - val_acc: 0.5748 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 1.2545 - acc: 0.5534 - top5-acc: 0.9435 - val_loss: 1.2137 - val_acc: 0.5686 - val_top5-acc: 0.9570 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 1.2463 - acc: 0.5582 - top5-acc: 0.9447 - val_loss: 1.1837 - val_acc: 0.5786 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 1.2496 - acc: 0.5592 - top5-acc: 0.9453 - val_loss: 1.1858 - val_acc: 0.5690 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2424 - acc: 0.5595 - top5-acc: 0.9470 - val_loss: 1.1933 - val_acc: 0.5716 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2465 - acc: 0.5584 - top5-acc: 0.9454 - val_loss: 1.1655 - val_acc: 0.5782 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2415 - acc: 0.5598 - top5-acc: 0.9454 - val_loss: 1.1937 - val_acc: 0.5730 - val_top5-acc: 0.9582 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2461 - acc: 0.5585 - top5-acc: 0.9476 - val_loss: 1.1807 - val_acc: 0.5788 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2415 - acc: 0.5605 - top5-acc: 0.9455 - val_loss: 1.1832 - val_acc: 0.5754 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2403 - acc: 0.5592 - top5-acc: 0.9452 - val_loss: 1.1722 - val_acc: 0.5802 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2410 - acc: 0.5606 - top5-acc: 0.9473 - val_loss: 1.1752 - val_acc: 0.5800 - val_top5-acc: 0.9594 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2283 - acc: 0.5656 - top5-acc: 0.9478 - val_loss: 1.1967 - val_acc: 0.5668 - val_top5-acc: 0.9558 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2240 - acc: 0.5666 - top5-acc: 0.9461 - val_loss: 1.1577 - val_acc: 0.5864 - val_top5-acc: 0.9590 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2287 - acc: 0.5640 - top5-acc: 0.9480 - val_loss: 1.1534 - val_acc: 0.5922 - val_top5-acc: 0.9604 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 1.2305 - acc: 0.5661 - top5-acc: 0.9480 - val_loss: 1.1605 - val_acc: 0.5844 - val_top5-acc: 0.9604 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 1.2260 - acc: 0.5648 - top5-acc: 0.9472 - val_loss: 1.1654 - val_acc: 0.5816 - val_top5-acc: 0.9594 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 24s 273ms/step - loss: 1.2229 - acc: 0.5668 - top5-acc: 0.9486 - val_loss: 1.1522 - val_acc: 0.5892 - val_top5-acc: 0.9590 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 1.2254 - acc: 0.5675 - top5-acc: 0.9476 - val_loss: 1.1710 - val_acc: 0.5784 - val_top5-acc: 0.9600 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 1.2263 - acc: 0.5663 - top5-acc: 0.9484 - val_loss: 1.1527 - val_acc: 0.5834 - val_top5-acc: 0.9624 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 1.2319 - acc: 0.5645 - top5-acc: 0.9464 - val_loss: 1.1579 - val_acc: 0.5844 - val_top5-acc: 0.9580 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 1.2233 - acc: 0.5645 - top5-acc: 0.9486 - val_loss: 1.1623 - val_acc: 0.5858 - val_top5-acc: 0.9588 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 25s 287ms/step - loss: 1.2223 - acc: 0.5681 - top5-acc: 0.9480 - val_loss: 1.1612 - val_acc: 0.5820 - val_top5-acc: 0.9560 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 25s 280ms/step - loss: 1.2237 - acc: 0.5690 - top5-acc: 0.9481 - val_loss: 1.1563 - val_acc: 0.5858 - val_top5-acc: 0.9584 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 1.2176 - acc: 0.5695 - top5-acc: 0.9482 - val_loss: 1.1540 - val_acc: 0.5848 - val_top5-acc: 0.9604 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2206 - acc: 0.5690 - top5-acc: 0.9476 - val_loss: 1.1534 - val_acc: 0.5858 - val_top5-acc: 0.9588 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2213 - acc: 0.5696 - top5-acc: 0.9472 - val_loss: 1.1519 - val_acc: 0.5860 - val_top5-acc: 0.9578 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 1.2220 - acc: 0.5697 - top5-acc: 0.9484 - val_loss: 1.1528 - val_acc: 0.5926 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2171 - acc: 0.5698 - top5-acc: 0.9488 - val_loss: 1.1613 - val_acc: 0.5860 - val_top5-acc: 0.9584 - lr: 0.0012\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2159 - acc: 0.5713 - top5-acc: 0.9479 - val_loss: 1.1639 - val_acc: 0.5856 - val_top5-acc: 0.9596 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 1.2181 - acc: 0.5708 - top5-acc: 0.9482 - val_loss: 1.1506 - val_acc: 0.5896 - val_top5-acc: 0.9596 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.2159 - acc: 0.5742 - top5-acc: 0.9483 - val_loss: 1.1580 - val_acc: 0.5862 - val_top5-acc: 0.9588 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2219 - acc: 0.5676 - top5-acc: 0.9485 - val_loss: 1.1546 - val_acc: 0.5892 - val_top5-acc: 0.9594 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 1.2180 - acc: 0.5690 - top5-acc: 0.9482 - val_loss: 1.1518 - val_acc: 0.5886 - val_top5-acc: 0.9580 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 24s 274ms/step - loss: 1.2188 - acc: 0.5714 - top5-acc: 0.9479 - val_loss: 1.1534 - val_acc: 0.5886 - val_top5-acc: 0.9576 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 24s 272ms/step - loss: 1.2168 - acc: 0.5714 - top5-acc: 0.9480 - val_loss: 1.1578 - val_acc: 0.5836 - val_top5-acc: 0.9596 - lr: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 49s 158ms/step - loss: 1.1638 - acc: 0.5879 - top5-acc: 0.9554\n",
      "Test accuracy: 58.79%\n",
      "Test top 5 accuracy: 95.54%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 34s 311ms/step - loss: 1.6603 - acc: 0.4148 - top5-acc: 0.8740 - val_loss: 1.3519 - val_acc: 0.5266 - val_top5-acc: 0.9378 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 1.3679 - acc: 0.5153 - top5-acc: 0.9338 - val_loss: 1.2570 - val_acc: 0.5610 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 1.3146 - acc: 0.5348 - top5-acc: 0.9385 - val_loss: 1.2419 - val_acc: 0.5592 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 1.2862 - acc: 0.5438 - top5-acc: 0.9413 - val_loss: 1.1965 - val_acc: 0.5806 - val_top5-acc: 0.9492 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 1.2659 - acc: 0.5516 - top5-acc: 0.9448 - val_loss: 1.1915 - val_acc: 0.5730 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 1.2525 - acc: 0.5557 - top5-acc: 0.9444 - val_loss: 1.1846 - val_acc: 0.5806 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 1.2376 - acc: 0.5624 - top5-acc: 0.9453 - val_loss: 1.1682 - val_acc: 0.5882 - val_top5-acc: 0.9590 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 1.2319 - acc: 0.5619 - top5-acc: 0.9467 - val_loss: 1.1622 - val_acc: 0.5892 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 1.2275 - acc: 0.5682 - top5-acc: 0.9472 - val_loss: 1.1500 - val_acc: 0.5946 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 1.2278 - acc: 0.5639 - top5-acc: 0.9466 - val_loss: 1.1450 - val_acc: 0.5970 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 25s 280ms/step - loss: 1.2136 - acc: 0.5734 - top5-acc: 0.9491 - val_loss: 1.1430 - val_acc: 0.5972 - val_top5-acc: 0.9600 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 25s 280ms/step - loss: 1.2140 - acc: 0.5666 - top5-acc: 0.9493 - val_loss: 1.1362 - val_acc: 0.5968 - val_top5-acc: 0.9600 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 25s 280ms/step - loss: 1.2140 - acc: 0.5682 - top5-acc: 0.9487 - val_loss: 1.1324 - val_acc: 0.6038 - val_top5-acc: 0.9594 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 25s 287ms/step - loss: 1.2125 - acc: 0.5678 - top5-acc: 0.9494 - val_loss: 1.1351 - val_acc: 0.5962 - val_top5-acc: 0.9592 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 25s 285ms/step - loss: 1.2053 - acc: 0.5737 - top5-acc: 0.9486 - val_loss: 1.1247 - val_acc: 0.5990 - val_top5-acc: 0.9592 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 25s 284ms/step - loss: 1.2023 - acc: 0.5730 - top5-acc: 0.9493 - val_loss: 1.1057 - val_acc: 0.6134 - val_top5-acc: 0.9602 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 25s 284ms/step - loss: 1.2009 - acc: 0.5709 - top5-acc: 0.9503 - val_loss: 1.1193 - val_acc: 0.6020 - val_top5-acc: 0.9600 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 25s 283ms/step - loss: 1.2065 - acc: 0.5721 - top5-acc: 0.9498 - val_loss: 1.1191 - val_acc: 0.6092 - val_top5-acc: 0.9586 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 25s 283ms/step - loss: 1.1958 - acc: 0.5749 - top5-acc: 0.9503 - val_loss: 1.1125 - val_acc: 0.6092 - val_top5-acc: 0.9610 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 25s 289ms/step - loss: 1.2024 - acc: 0.5761 - top5-acc: 0.9488 - val_loss: 1.1395 - val_acc: 0.5984 - val_top5-acc: 0.9594 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 25s 286ms/step - loss: 1.1916 - acc: 0.5772 - top5-acc: 0.9500 - val_loss: 1.1083 - val_acc: 0.6044 - val_top5-acc: 0.9634 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 25s 284ms/step - loss: 1.1806 - acc: 0.5835 - top5-acc: 0.9526 - val_loss: 1.0969 - val_acc: 0.6166 - val_top5-acc: 0.9600 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 25s 283ms/step - loss: 1.1807 - acc: 0.5825 - top5-acc: 0.9512 - val_loss: 1.1001 - val_acc: 0.6120 - val_top5-acc: 0.9632 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 25s 285ms/step - loss: 1.1819 - acc: 0.5806 - top5-acc: 0.9525 - val_loss: 1.1048 - val_acc: 0.6144 - val_top5-acc: 0.9612 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 25s 286ms/step - loss: 1.1858 - acc: 0.5778 - top5-acc: 0.9520 - val_loss: 1.1142 - val_acc: 0.6072 - val_top5-acc: 0.9608 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 1.1759 - acc: 0.5839 - top5-acc: 0.9528 - val_loss: 1.1088 - val_acc: 0.6104 - val_top5-acc: 0.9602 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 24s 269ms/step - loss: 1.1762 - acc: 0.5826 - top5-acc: 0.9535 - val_loss: 1.1007 - val_acc: 0.6082 - val_top5-acc: 0.9648 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 1.1752 - acc: 0.5827 - top5-acc: 0.9526 - val_loss: 1.0941 - val_acc: 0.6146 - val_top5-acc: 0.9646 - lr: 0.0012\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 25s 286ms/step - loss: 1.1740 - acc: 0.5856 - top5-acc: 0.9528 - val_loss: 1.0888 - val_acc: 0.6182 - val_top5-acc: 0.9616 - lr: 0.0012\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.1762 - acc: 0.5835 - top5-acc: 0.9528 - val_loss: 1.1031 - val_acc: 0.6100 - val_top5-acc: 0.9616 - lr: 0.0012\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 25s 279ms/step - loss: 1.1721 - acc: 0.5834 - top5-acc: 0.9533 - val_loss: 1.1011 - val_acc: 0.6134 - val_top5-acc: 0.9636 - lr: 0.0012\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 24s 273ms/step - loss: 1.1754 - acc: 0.5861 - top5-acc: 0.9532 - val_loss: 1.0932 - val_acc: 0.6138 - val_top5-acc: 0.9640 - lr: 0.0012\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 1.1711 - acc: 0.5848 - top5-acc: 0.9538 - val_loss: 1.1002 - val_acc: 0.6108 - val_top5-acc: 0.9608 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 23s 262ms/step - loss: 1.1786 - acc: 0.5816 - top5-acc: 0.9511 - val_loss: 1.0981 - val_acc: 0.6118 - val_top5-acc: 0.9630 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 23s 261ms/step - loss: 1.1732 - acc: 0.5846 - top5-acc: 0.9542 - val_loss: 1.0997 - val_acc: 0.6120 - val_top5-acc: 0.9634 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1754 - acc: 0.5828 - top5-acc: 0.9531 - val_loss: 1.0943 - val_acc: 0.6134 - val_top5-acc: 0.9630 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.1744 - acc: 0.5824 - top5-acc: 0.9543 - val_loss: 1.0917 - val_acc: 0.6172 - val_top5-acc: 0.9640 - lr: 6.2500e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.1747 - acc: 0.5840 - top5-acc: 0.9531 - val_loss: 1.0946 - val_acc: 0.6128 - val_top5-acc: 0.9624 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1714 - acc: 0.5872 - top5-acc: 0.9524 - val_loss: 1.0961 - val_acc: 0.6148 - val_top5-acc: 0.9616 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1714 - acc: 0.5864 - top5-acc: 0.9537 - val_loss: 1.0980 - val_acc: 0.6134 - val_top5-acc: 0.9618 - lr: 3.1250e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1665 - acc: 0.5908 - top5-acc: 0.9551 - val_loss: 1.0945 - val_acc: 0.6162 - val_top5-acc: 0.9620 - lr: 3.1250e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.1696 - acc: 0.5899 - top5-acc: 0.9526 - val_loss: 1.0945 - val_acc: 0.6158 - val_top5-acc: 0.9626 - lr: 3.1250e-04\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1704 - acc: 0.5873 - top5-acc: 0.9537 - val_loss: 1.0945 - val_acc: 0.6154 - val_top5-acc: 0.9628 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1724 - acc: 0.5882 - top5-acc: 0.9533 - val_loss: 1.0979 - val_acc: 0.6166 - val_top5-acc: 0.9626 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1710 - acc: 0.5867 - top5-acc: 0.9544 - val_loss: 1.0965 - val_acc: 0.6158 - val_top5-acc: 0.9616 - lr: 1.5625e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 1.1724 - acc: 0.5889 - top5-acc: 0.9525 - val_loss: 1.0992 - val_acc: 0.6152 - val_top5-acc: 0.9624 - lr: 1.5625e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1741 - acc: 0.5877 - top5-acc: 0.9542 - val_loss: 1.0969 - val_acc: 0.6140 - val_top5-acc: 0.9622 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1741 - acc: 0.5845 - top5-acc: 0.9533 - val_loss: 1.0984 - val_acc: 0.6150 - val_top5-acc: 0.9612 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1746 - acc: 0.5875 - top5-acc: 0.9529 - val_loss: 1.0998 - val_acc: 0.6154 - val_top5-acc: 0.9616 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 23s 259ms/step - loss: 1.1759 - acc: 0.5855 - top5-acc: 0.9522 - val_loss: 1.0998 - val_acc: 0.6158 - val_top5-acc: 0.9624 - lr: 7.8125e-05\n",
      "313/313 [==============================] - 38s 122ms/step - loss: 1.1163 - acc: 0.6025 - top5-acc: 0.9597\n",
      "Test accuracy: 60.25%\n",
      "Test top 5 accuracy: 95.97%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 34s 296ms/step - loss: 1.2036 - acc: 0.5761 - top5-acc: 0.9370 - val_loss: 1.0166 - val_acc: 0.6446 - val_top5-acc: 0.9674 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.9708 - acc: 0.6563 - top5-acc: 0.9697 - val_loss: 0.9723 - val_acc: 0.6624 - val_top5-acc: 0.9694 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.9470 - acc: 0.6646 - top5-acc: 0.9716 - val_loss: 0.9618 - val_acc: 0.6642 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.9323 - acc: 0.6720 - top5-acc: 0.9722 - val_loss: 0.9764 - val_acc: 0.6548 - val_top5-acc: 0.9712 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.9231 - acc: 0.6725 - top5-acc: 0.9732 - val_loss: 0.9634 - val_acc: 0.6612 - val_top5-acc: 0.9744 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 23s 267ms/step - loss: 0.9179 - acc: 0.6736 - top5-acc: 0.9736 - val_loss: 0.9683 - val_acc: 0.6688 - val_top5-acc: 0.9722 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.9103 - acc: 0.6784 - top5-acc: 0.9750 - val_loss: 0.9686 - val_acc: 0.6642 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.9093 - acc: 0.6786 - top5-acc: 0.9740 - val_loss: 0.9690 - val_acc: 0.6668 - val_top5-acc: 0.9732 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8973 - acc: 0.6826 - top5-acc: 0.9741 - val_loss: 0.9452 - val_acc: 0.6750 - val_top5-acc: 0.9740 - lr: 0.0025\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 24s 267ms/step - loss: 0.8944 - acc: 0.6849 - top5-acc: 0.9742 - val_loss: 0.9335 - val_acc: 0.6760 - val_top5-acc: 0.9742 - lr: 0.0025\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8949 - acc: 0.6818 - top5-acc: 0.9753 - val_loss: 0.9435 - val_acc: 0.6752 - val_top5-acc: 0.9748 - lr: 0.0025\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 25s 288ms/step - loss: 0.8953 - acc: 0.6841 - top5-acc: 0.9751 - val_loss: 0.9367 - val_acc: 0.6764 - val_top5-acc: 0.9738 - lr: 0.0025\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 25s 283ms/step - loss: 0.8923 - acc: 0.6854 - top5-acc: 0.9751 - val_loss: 0.9335 - val_acc: 0.6752 - val_top5-acc: 0.9740 - lr: 0.0025\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 25s 279ms/step - loss: 0.8894 - acc: 0.6857 - top5-acc: 0.9759 - val_loss: 0.9387 - val_acc: 0.6716 - val_top5-acc: 0.9768 - lr: 0.0025\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 25s 281ms/step - loss: 0.8868 - acc: 0.6880 - top5-acc: 0.9764 - val_loss: 0.9512 - val_acc: 0.6750 - val_top5-acc: 0.9736 - lr: 0.0025\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8802 - acc: 0.6868 - top5-acc: 0.9761 - val_loss: 0.9305 - val_acc: 0.6744 - val_top5-acc: 0.9748 - lr: 0.0012\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8824 - acc: 0.6871 - top5-acc: 0.9756 - val_loss: 0.9321 - val_acc: 0.6760 - val_top5-acc: 0.9750 - lr: 0.0012\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8863 - acc: 0.6874 - top5-acc: 0.9754 - val_loss: 0.9299 - val_acc: 0.6792 - val_top5-acc: 0.9738 - lr: 0.0012\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.8814 - acc: 0.6882 - top5-acc: 0.9770 - val_loss: 0.9295 - val_acc: 0.6778 - val_top5-acc: 0.9738 - lr: 0.0012\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8814 - acc: 0.6878 - top5-acc: 0.9758 - val_loss: 0.9229 - val_acc: 0.6794 - val_top5-acc: 0.9752 - lr: 0.0012\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8814 - acc: 0.6868 - top5-acc: 0.9757 - val_loss: 0.9221 - val_acc: 0.6788 - val_top5-acc: 0.9756 - lr: 0.0012\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8845 - acc: 0.6878 - top5-acc: 0.9754 - val_loss: 0.9422 - val_acc: 0.6738 - val_top5-acc: 0.9748 - lr: 0.0012\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8835 - acc: 0.6868 - top5-acc: 0.9757 - val_loss: 0.9279 - val_acc: 0.6750 - val_top5-acc: 0.9760 - lr: 0.0012\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8785 - acc: 0.6861 - top5-acc: 0.9764 - val_loss: 0.9288 - val_acc: 0.6786 - val_top5-acc: 0.9752 - lr: 0.0012\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8848 - acc: 0.6877 - top5-acc: 0.9754 - val_loss: 0.9266 - val_acc: 0.6788 - val_top5-acc: 0.9748 - lr: 0.0012\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.8846 - acc: 0.6869 - top5-acc: 0.9754 - val_loss: 0.9300 - val_acc: 0.6768 - val_top5-acc: 0.9754 - lr: 0.0012\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8796 - acc: 0.6918 - top5-acc: 0.9758 - val_loss: 0.9210 - val_acc: 0.6802 - val_top5-acc: 0.9738 - lr: 6.2500e-04\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8779 - acc: 0.6905 - top5-acc: 0.9756 - val_loss: 0.9207 - val_acc: 0.6778 - val_top5-acc: 0.9752 - lr: 6.2500e-04\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8736 - acc: 0.6917 - top5-acc: 0.9756 - val_loss: 0.9199 - val_acc: 0.6798 - val_top5-acc: 0.9748 - lr: 6.2500e-04\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.8783 - acc: 0.6886 - top5-acc: 0.9756 - val_loss: 0.9186 - val_acc: 0.6746 - val_top5-acc: 0.9756 - lr: 6.2500e-04\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8774 - acc: 0.6920 - top5-acc: 0.9748 - val_loss: 0.9231 - val_acc: 0.6774 - val_top5-acc: 0.9756 - lr: 6.2500e-04\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8798 - acc: 0.6914 - top5-acc: 0.9756 - val_loss: 0.9252 - val_acc: 0.6812 - val_top5-acc: 0.9734 - lr: 6.2500e-04\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8803 - acc: 0.6881 - top5-acc: 0.9758 - val_loss: 0.9199 - val_acc: 0.6800 - val_top5-acc: 0.9750 - lr: 6.2500e-04\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8811 - acc: 0.6898 - top5-acc: 0.9759 - val_loss: 0.9231 - val_acc: 0.6794 - val_top5-acc: 0.9756 - lr: 6.2500e-04\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 24s 269ms/step - loss: 0.8801 - acc: 0.6904 - top5-acc: 0.9759 - val_loss: 0.9228 - val_acc: 0.6816 - val_top5-acc: 0.9750 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8811 - acc: 0.6892 - top5-acc: 0.9757 - val_loss: 0.9227 - val_acc: 0.6782 - val_top5-acc: 0.9740 - lr: 3.1250e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8773 - acc: 0.6921 - top5-acc: 0.9763 - val_loss: 0.9208 - val_acc: 0.6800 - val_top5-acc: 0.9748 - lr: 3.1250e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8744 - acc: 0.6918 - top5-acc: 0.9765 - val_loss: 0.9239 - val_acc: 0.6760 - val_top5-acc: 0.9744 - lr: 3.1250e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8752 - acc: 0.6908 - top5-acc: 0.9760 - val_loss: 0.9202 - val_acc: 0.6798 - val_top5-acc: 0.9740 - lr: 3.1250e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8786 - acc: 0.6889 - top5-acc: 0.9756 - val_loss: 0.9200 - val_acc: 0.6788 - val_top5-acc: 0.9752 - lr: 3.1250e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8738 - acc: 0.6922 - top5-acc: 0.9758 - val_loss: 0.9171 - val_acc: 0.6784 - val_top5-acc: 0.9750 - lr: 1.5625e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.8752 - acc: 0.6898 - top5-acc: 0.9768 - val_loss: 0.9167 - val_acc: 0.6794 - val_top5-acc: 0.9752 - lr: 1.5625e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8759 - acc: 0.6921 - top5-acc: 0.9759 - val_loss: 0.9193 - val_acc: 0.6820 - val_top5-acc: 0.9744 - lr: 1.5625e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 24s 268ms/step - loss: 0.8761 - acc: 0.6909 - top5-acc: 0.9770 - val_loss: 0.9192 - val_acc: 0.6796 - val_top5-acc: 0.9744 - lr: 1.5625e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 24s 267ms/step - loss: 0.8784 - acc: 0.6940 - top5-acc: 0.9763 - val_loss: 0.9167 - val_acc: 0.6788 - val_top5-acc: 0.9750 - lr: 1.5625e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8730 - acc: 0.6917 - top5-acc: 0.9763 - val_loss: 0.9158 - val_acc: 0.6800 - val_top5-acc: 0.9742 - lr: 1.5625e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8770 - acc: 0.6916 - top5-acc: 0.9760 - val_loss: 0.9186 - val_acc: 0.6778 - val_top5-acc: 0.9744 - lr: 1.5625e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8814 - acc: 0.6880 - top5-acc: 0.9761 - val_loss: 0.9183 - val_acc: 0.6788 - val_top5-acc: 0.9750 - lr: 1.5625e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 24s 270ms/step - loss: 0.8771 - acc: 0.6902 - top5-acc: 0.9760 - val_loss: 0.9197 - val_acc: 0.6776 - val_top5-acc: 0.9738 - lr: 1.5625e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 24s 271ms/step - loss: 0.8790 - acc: 0.6884 - top5-acc: 0.9764 - val_loss: 0.9173 - val_acc: 0.6790 - val_top5-acc: 0.9746 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 40s 127ms/step - loss: 0.9291 - acc: 0.6754 - top5-acc: 0.9735\n",
      "Test accuracy: 67.54%\n",
      "Test top 5 accuracy: 97.35%\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 35s 305ms/step - loss: 0.8330 - acc: 0.7190 - top5-acc: 0.9739 - val_loss: 0.8971 - val_acc: 0.7196 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 24s 274ms/step - loss: 0.6664 - acc: 0.7670 - top5-acc: 0.9881 - val_loss: 0.8635 - val_acc: 0.7242 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6412 - acc: 0.7748 - top5-acc: 0.9890 - val_loss: 0.8772 - val_acc: 0.7210 - val_top5-acc: 0.9832 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6402 - acc: 0.7704 - top5-acc: 0.9890 - val_loss: 0.8633 - val_acc: 0.7242 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6406 - acc: 0.7746 - top5-acc: 0.9895 - val_loss: 0.8649 - val_acc: 0.7190 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6337 - acc: 0.7751 - top5-acc: 0.9898 - val_loss: 0.8685 - val_acc: 0.7250 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6357 - acc: 0.7750 - top5-acc: 0.9892 - val_loss: 0.8716 - val_acc: 0.7136 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6364 - acc: 0.7736 - top5-acc: 0.9893 - val_loss: 0.8711 - val_acc: 0.7250 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6382 - acc: 0.7747 - top5-acc: 0.9894 - val_loss: 0.8677 - val_acc: 0.7228 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 24s 276ms/step - loss: 0.6188 - acc: 0.7807 - top5-acc: 0.9892 - val_loss: 0.8455 - val_acc: 0.7268 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6158 - acc: 0.7806 - top5-acc: 0.9897 - val_loss: 0.8472 - val_acc: 0.7260 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6208 - acc: 0.7796 - top5-acc: 0.9891 - val_loss: 0.8362 - val_acc: 0.7278 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.6196 - acc: 0.7818 - top5-acc: 0.9896 - val_loss: 0.8427 - val_acc: 0.7278 - val_top5-acc: 0.9826 - lr: 0.0025\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.6204 - acc: 0.7790 - top5-acc: 0.9902 - val_loss: 0.8420 - val_acc: 0.7288 - val_top5-acc: 0.9824 - lr: 0.0025\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.6155 - acc: 0.7815 - top5-acc: 0.9898 - val_loss: 0.8411 - val_acc: 0.7288 - val_top5-acc: 0.9838 - lr: 0.0025\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 24s 274ms/step - loss: 0.6122 - acc: 0.7820 - top5-acc: 0.9905 - val_loss: 0.8493 - val_acc: 0.7312 - val_top5-acc: 0.9824 - lr: 0.0025\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 24s 274ms/step - loss: 0.6178 - acc: 0.7802 - top5-acc: 0.9898 - val_loss: 0.8491 - val_acc: 0.7248 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6096 - acc: 0.7822 - top5-acc: 0.9899 - val_loss: 0.8339 - val_acc: 0.7282 - val_top5-acc: 0.9832 - lr: 0.0012\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6101 - acc: 0.7826 - top5-acc: 0.9899 - val_loss: 0.8282 - val_acc: 0.7266 - val_top5-acc: 0.9840 - lr: 0.0012\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6048 - acc: 0.7844 - top5-acc: 0.9897 - val_loss: 0.8298 - val_acc: 0.7284 - val_top5-acc: 0.9834 - lr: 0.0012\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6059 - acc: 0.7858 - top5-acc: 0.9900 - val_loss: 0.8349 - val_acc: 0.7274 - val_top5-acc: 0.9828 - lr: 0.0012\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6087 - acc: 0.7838 - top5-acc: 0.9897 - val_loss: 0.8354 - val_acc: 0.7262 - val_top5-acc: 0.9820 - lr: 0.0012\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6060 - acc: 0.7860 - top5-acc: 0.9908 - val_loss: 0.8333 - val_acc: 0.7308 - val_top5-acc: 0.9834 - lr: 0.0012\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6043 - acc: 0.7862 - top5-acc: 0.9904 - val_loss: 0.8313 - val_acc: 0.7306 - val_top5-acc: 0.9826 - lr: 0.0012\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.6077 - acc: 0.7831 - top5-acc: 0.9900 - val_loss: 0.8251 - val_acc: 0.7300 - val_top5-acc: 0.9822 - lr: 6.2500e-04\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.6048 - acc: 0.7872 - top5-acc: 0.9902 - val_loss: 0.8243 - val_acc: 0.7274 - val_top5-acc: 0.9832 - lr: 6.2500e-04\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 24s 278ms/step - loss: 0.5999 - acc: 0.7878 - top5-acc: 0.9897 - val_loss: 0.8232 - val_acc: 0.7294 - val_top5-acc: 0.9834 - lr: 6.2500e-04\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6020 - acc: 0.7876 - top5-acc: 0.9900 - val_loss: 0.8241 - val_acc: 0.7278 - val_top5-acc: 0.9832 - lr: 6.2500e-04\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6051 - acc: 0.7851 - top5-acc: 0.9898 - val_loss: 0.8272 - val_acc: 0.7270 - val_top5-acc: 0.9832 - lr: 6.2500e-04\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.5980 - acc: 0.7864 - top5-acc: 0.9905 - val_loss: 0.8231 - val_acc: 0.7290 - val_top5-acc: 0.9832 - lr: 6.2500e-04\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.5977 - acc: 0.7883 - top5-acc: 0.9899 - val_loss: 0.8213 - val_acc: 0.7296 - val_top5-acc: 0.9824 - lr: 6.2500e-04\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6002 - acc: 0.7862 - top5-acc: 0.9902 - val_loss: 0.8246 - val_acc: 0.7288 - val_top5-acc: 0.9830 - lr: 6.2500e-04\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.5963 - acc: 0.7883 - top5-acc: 0.9907 - val_loss: 0.8256 - val_acc: 0.7322 - val_top5-acc: 0.9834 - lr: 6.2500e-04\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.5968 - acc: 0.7886 - top5-acc: 0.9914 - val_loss: 0.8295 - val_acc: 0.7302 - val_top5-acc: 0.9832 - lr: 6.2500e-04\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6042 - acc: 0.7855 - top5-acc: 0.9897 - val_loss: 0.8219 - val_acc: 0.7290 - val_top5-acc: 0.9828 - lr: 6.2500e-04\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6004 - acc: 0.7886 - top5-acc: 0.9899 - val_loss: 0.8224 - val_acc: 0.7296 - val_top5-acc: 0.9838 - lr: 6.2500e-04\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6010 - acc: 0.7872 - top5-acc: 0.9903 - val_loss: 0.8209 - val_acc: 0.7274 - val_top5-acc: 0.9832 - lr: 3.1250e-04\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.5963 - acc: 0.7871 - top5-acc: 0.9904 - val_loss: 0.8211 - val_acc: 0.7296 - val_top5-acc: 0.9832 - lr: 3.1250e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.5953 - acc: 0.7894 - top5-acc: 0.9905 - val_loss: 0.8196 - val_acc: 0.7286 - val_top5-acc: 0.9832 - lr: 3.1250e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.5981 - acc: 0.7885 - top5-acc: 0.9900 - val_loss: 0.8199 - val_acc: 0.7294 - val_top5-acc: 0.9832 - lr: 3.1250e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 24s 274ms/step - loss: 0.5978 - acc: 0.7868 - top5-acc: 0.9900 - val_loss: 0.8204 - val_acc: 0.7300 - val_top5-acc: 0.9836 - lr: 3.1250e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.5976 - acc: 0.7899 - top5-acc: 0.9900 - val_loss: 0.8187 - val_acc: 0.7286 - val_top5-acc: 0.9832 - lr: 3.1250e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.6004 - acc: 0.7890 - top5-acc: 0.9905 - val_loss: 0.8190 - val_acc: 0.7298 - val_top5-acc: 0.9822 - lr: 3.1250e-04\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.5992 - acc: 0.7858 - top5-acc: 0.9903 - val_loss: 0.8168 - val_acc: 0.7276 - val_top5-acc: 0.9828 - lr: 3.1250e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 24s 278ms/step - loss: 0.5995 - acc: 0.7894 - top5-acc: 0.9899 - val_loss: 0.8168 - val_acc: 0.7304 - val_top5-acc: 0.9828 - lr: 3.1250e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.5963 - acc: 0.7885 - top5-acc: 0.9905 - val_loss: 0.8179 - val_acc: 0.7296 - val_top5-acc: 0.9832 - lr: 3.1250e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 24s 274ms/step - loss: 0.5977 - acc: 0.7870 - top5-acc: 0.9901 - val_loss: 0.8193 - val_acc: 0.7302 - val_top5-acc: 0.9830 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.5961 - acc: 0.7877 - top5-acc: 0.9900 - val_loss: 0.8183 - val_acc: 0.7286 - val_top5-acc: 0.9828 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 24s 274ms/step - loss: 0.6008 - acc: 0.7862 - top5-acc: 0.9906 - val_loss: 0.8174 - val_acc: 0.7296 - val_top5-acc: 0.9832 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 24s 275ms/step - loss: 0.5935 - acc: 0.7892 - top5-acc: 0.9899 - val_loss: 0.8160 - val_acc: 0.7300 - val_top5-acc: 0.9830 - lr: 1.5625e-04\n",
      "313/313 [==============================] - 41s 130ms/step - loss: 0.8250 - acc: 0.7286 - top5-acc: 0.9818\n",
      "Test accuracy: 72.86%\n",
      "Test top 5 accuracy: 98.18%\n"
     ]
    }
   ],
   "source": [
    "tested_acc_evolution = evol_accuracy(all_models,listnumblocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 1: VERIFICATIONS (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change paths in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'Results_Article/1A/1/mlpmixer_32ly_384Dc_2022-02-25'\n",
    "path = 'Results_Article/1A/mlpmixer_32ly_384Dc'\n",
    "#Call the file\n",
    "#tested_history=np.load( path + '/history_2022-02-25.npy',allow_pickle='TRUE').item()\n",
    "tested_history=np.load( path + '/history.npy',allow_pickle='TRUE').item()\n",
    "with open(path + '/accuracy.pkl','rb') as file:\n",
    "    tested_accuracy = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "curves(tested_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/heatmap_kernel_Sg1_32ly_384Dc.pkl','rb') as file:\n",
    "    tested_heatmap = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAN0AAADjCAYAAAABtBHHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgXUlEQVR4nO2deZwlZXnvv7/ep2eFGfZxQRSRKI4KgiYhiBgH9xtzEwGRaLxKLki8aNTE3KC5IR+3RI1I0CghxgUXEEY+qKAGiBsgiIiASABhWIIjzMBsvZzz3D+qGk89VX3qnO4+dU6feb7zOZ9Pv1VvvfV2Tz/9vvWrZ5GZEQRBdQx0ewJBsKsRRhcEFRNGFwQVE0YXBBUTRhcEFRNGFwQVE0bX40g6T9Lf9cA87pJ0TLfn0Q+E0c0RScdL+pGkrZLul/R1Sb8j6T2SPtvQbz9Jt0r6J0lKj0nSHZJu7t53EHSLMLo5IOl04CPA3wN7AY8HzgZe6fo9AbgK2GBmp9lvPBGOBPYEniTpsIrmPFjFfYJywujaRNJK4G+BU8zsQjPbZmZTZvY1M/uLhn4HkBjc583sHW6Yk4CLgUvTr1u993JJ/zGzako6SNLlkh6S9HNJf9TQ9zxJ/yzpUknbgBekW8S3S7pR0hZJX5Q01nDNyyTdIGmzpO9LOmROP6SgKWF07fM8YAz4apM+TyIxuE+Y2f9tPCFpHPhD4HPp5zWSRspuKmk18G3ge2Z2GjAOXA58nmTVPA44W9JvNVx2PHAmsBz4bnrsj4D1wP7AIcCfpOM/GzgXeDOwGvgEsEHSaNncgvYIo2uf1cAmM5tu0ufpwFLgiwXn/gCYAC4DLgGGgJeW3HNf4Ergy2b21+mxlwF3mdm/mtm0mV0PXEBi0DNcbGbfM7O6me1Mj/2Tmd1nZg8BXwPWpcf/F8kfiavNrGZm/5bO84iSuQVtEkbXPr8G1kgaatJnA8mq8Z30ua6Rk4AvpYYyAVyYHkPSX6XCzFZJ5zRc81JgCdB47AnA4elWcLOkzcAJwN4Nfe4pmNsDDV9vB5Y1jPc2N97jSAw+WECa/eIExfwA2Am8CvjKbJ3M7PR0a/YdSUea2b2S1gJHA8+V9Oq06zgwJmmNmf09iTjj+RdgN+BSSevNbBuJQV1pZi9qMtd2QkjuAc40szPbuCaYA7HStYmZbQH+Bvi4pFdJGpc0LOlYSR9w3U8FvgN8W9JewInAbcBTSbZ164ADgY0kz2TNOBX4OXCJpCUkW9MDJZ2Y3n9Y0mGSnjbHb+1fgJMlHZ6KNEslvVTS8jmOF8xCGN0cMLN/BE4H/hr4FckqcSpwketnJMLENcC3gD8FzjazBxo/JNvGpipmOtab0ntdDEwBvw+8BriPZNv4fmBOwoeZ/Yjkue4s4GHgdlKRJVhYFEGsQVAtsdIFQcWE0QVBEySdK+lBSTfNcl6ps8LtqdPBs8vGDKMLguacR+JMMBvHAk9JP28C/rlswI4ZnaQxSddI+omkn0l6b3r8g6kD8I2SvippVafmEATzxcyuAh5q0uWVwGcs4YfAKkn7NBuzkyvdBHC0mT2TRBpfL+kIEtelp5vZISTy+V92cA5B0Gn2I+uEsDE9NisdezmeStxb0+Zw+jEzu6yh2w/Jui0VIikk1iCHmanZ+alNdzT9vRnZ44A3k2wJZ/ikmX2yzWkUzaHpfTvqkZKGk1wHPBn4uJld7bq8gWL/xCCYP7WppqdTA2vXyDwbSdzlZlhL8t50VjpqdGZWA9alz21flfR0M7sJQNK7gWkST/uWGB1NvrfHL98zc3zF0JJc3/2GV2baewyMZdoHWra9n/v/ecbIlkx797225e6xZM9apj20ZjjTHly9NNPWsvFse8Uycoxm321rPHsNS901Y+PN2yPZ7xNAw+79+bALchjMfh8MZc/Lny/okxtjIPskI7knm8HyX0UtXQXAyB4HlPYFoF5vrd/82ACcKul84HBgi5nd3+yCSnwvzWyzpCtIVKCbJJ1E4iX/Qpvl7bykN5Fd+oOgLazWLBCkNSR9ATiKxMl9I3AGyaMSZnYOSUzkS0g8eLYDry8bs2NGJ2kPYCo1uCXAMcD7Ja0H3gn8npltn+36xqU/numCOVGyvWwFM2vqE5suGqe0M2YnV7p9gH9Ln+sGSMJZLpF0O4l/4OVKUob80MxObmVAS59PzT2n1gueW2vumNzz7qC7ZMgtuMPD2a3j8JL8VmVwPLtFGljqtmHj2W2vlmW3myzLby815raDY27rvHyVO++2rCOuv986Arg+Gmi+FfT9i7aC8sf89tJtJ3P9/T0LxrBtm/N9mmGVbC/bppPq5Y3AswqOP7lT9wyCRhZie9kJIp4u6F+qEVLaJowu6F8W4JmuE4TRBf3LrvZM1wlmi/0bKHAKGHTH/GO6/8Z9UsgBp7QMDOfvrZFB13aj+vZwtq3hgvddXvgoa3tBwp8fyse0yh8rEUGUewdX8GvjrykTSvx7uoGCtJzzXanimS4IqsViexkEFRPbyyComNheBkHF1GvlfbrAojS6QfcQPlRQG2PY9Rl2Uor3SBku8UgZKHDs0JgTUpY4Z2XvXeKcmRkpSNyV6+PG8M7H3nnZiSQ5D5WCMUpFEH8PL4IUXVPmwOyFkyID8e/Z2l25YqULgoqJZ7ogqJjpWOmCoFLilUEQVE1sLztHGiKUPeZDeVzbO5gMu/8g75FSVEFOgz762YkDQ05oGfLnC378XqRwfXJR22UCRoHo4UWMdr1Hcv2LrinyMGnECycFBuKjBKxdIwohJQgqJqIMgqBiYqULgoqJlS4IKiZWuoUj55FSkKh6KCek+PPN24PDTlgZKRAkykJ5vLCSC8spEBvKQne8+OI9UMqElqJjXgQpSZ9XmM+kSLBph6JVyQsn7a5cYXRBUDHxyiAIKiY8UoKgYnq0ynAYXdC/xEoXBBUTz3TzZyYxUb2FbcMgXuFs7gY2gs/onG372DkALXGxbS5+Llf8w2drXlJUQMSNMeqyQvvszD7JUCvZmNst9uH7l7l4Qbmbl1MiCxPDeofldh2Ya70ZxBrlj4P+ZXq6+acFJK2X9PO0pvi7Cs6vlPS1horDpQVEwuiC/sXqzT8lpHU4Pk5SV/xg4DhJB7tupwA3pxWHjwL+QSpyj/8Ni2p7GQTtYNPz3l4+F7jdzO4ASGvQvRK4ufE2wHIloS7LSOqTN11Gw+iC/mX+QkpRPfHDXZ+zSApD3gcsB/7YSmKQ+sLoioo++2P5eLqskjLoSuANjrp4urECQcIlImJJSSIiL6SM5quk5hIReWFl2I3h+reUjdklGspVRW03Nq6IdoWTApEk12d6sr05lKx0BYVHfc3xVuqJvxi4ATgaOICkBNx/mtkjs923L4wuCAop8dVsoeZ4K/XEXw+8Ly0OebukO4GDgGtmGzSElKB/qdWaf8q5FniKpP1TceQ1JFvJRu4GXgggaS/gqcAdzQbtmNFJepyk/5B0Syql/rk7/3ZJJmlNp+YQ7OLM0+jMbBo4FfgmcAtJNeGfSTpZ0kz14P8HPF/ST4FvA+80s03Nxu3k9nIaeJuZXS9pOXCdpMvN7GZJjwNeRPJXIgg6Q33+vpdmdilwqTt2TsPX9wG/386YnSx/fD9wf/r1o5JuIVGDbgY+DLwDuLitMdvoO1CQrChz3g3mhRT5/D7DRbFvwyVtX7aqpE15xuZ8vF1ZoqJ8Oa6ccJKLr5uDcOI8UHIiiH++8kJLoUeKS0zUpkfKArwy6AiVCCmSnkhSf/xqSa8A7jWznxRl8QqCBWNX9b2UtAy4AHgryZbz3bSwHBfIuUHQHj260nVUvZQ0TGJwnzOzC0neY+wP/ETSXSQS7PWS9vbXmtknzexQMzu0k3MM+pi6Nf90iY6tdKlbzKeBW8zsHwHM7KfAng197gIOLVN7gmBO9GiUQSe3l78NnAj8VNIN6bG/StWgBaVINPFLeK6muD/vhBRfGitXT7zgWK6GuC+F5UQQjZaXscrXBy8pa1V2HuYvnLTiPVLmceI9VIq8TSZ3ZNtteqTsckKKmX2XYjeaxj5P7NT9g6CbW8hmhBtY0L/sgtvLIOgqFitdEFTMrvZM10m8cOLzoUC+VNZASWjP8JCrMT7mxhzNe3Yw4tQWH7rjw3RK6ocnfdwYXnzx1/gwnbLszdC+cOLznRR57+fqg5cIJy2E9pSOWUasdEFQLVbbRT1SgqBrTIfRBUGlhJASBFUzHUZXKV4+GPKhPC5QaMDF+siVxtJogejhc6CUhPbkwnZ8mwIPlJxHSkmN8bKyV63ghZNWan87kaPUQ6XkHgBWd8LJVJseKbHSBUHFxEoXBNUSK10QVIzFShcE1WK9WSmrP4yulbQPvof3SBkazIoDuSo9BUKKvJBS5oGS8y7JCyleGCkN3cnlO5mDcLIA3iO50JypCXeNG8OJMebDeACms2OYH7OM3nxN1x9GFwRFxEoXBBXTo3mJwuiC/qUeK10QVIz1ZorHRWl0PmynKLQnV6Un13ahPSOu3PGoEzR8RR7I50DJVdgp8S4pCu3x15QKK3P4L5yvcNJCYlifzySXKDbXv0Ak8R4okzvzfZpQnw6jC4JKqdd60+iiak/Qt8yz+jFQXnM87XOUpBvSQjlXlo0ZK13Qt8x3pWuoOf4iklp110raYGY3N/RZBZwNrDezuyXtWThYA6UrnaRBSZ+d88yDoEtYXU0/LfBYzXEzmwRmao43cjxwoZndDWBmD5YNWrrSmVlN0h6SRtIbd51B54UxWOCR4oUUV82YMWWFk+FhJ6QsWZq9oCi0Z4kvZzyebfvSxLn8J0WhPc1zoJQmim0lZKasYo7z/Ggpn4kTOcwLI77tw4WKvE38fXduz/dpwgI807VSc/xAYFjSFSQ1xz9qZp9pNmir28u7gO9J2gBsmzk4ky49CHqRstVsgWqODwHPIanGugT4gaQfmtlts923VaO7L/0MkFhzEPQ8ZSvdAtUc3whsMrNtwDZJVwHPBOZndGb2XgBJS9PBg6Dnqc//5fhjNceBe0lqjh/v+lwMnCVpCBgh2X5+uNmgLb0ykPQ8STeT1F1G0jMlnd3e/IOgWuq1gaafMlqpOW5mtwDfAG4ErgE+ZWY3NRu31e3lR4AXAxvSG/1E0pEtXrvg+MSxRR4pw+6YF1JGBrOCw9CoF1JcElefSBZywonGsuKLRpe5m2bHyAkrFAgp8xROciIIlHqY5MJ0fP+CXCU25UJzchV3SkoZF1Xk8fepXkgprTmetj8IfLDVMVt+T2dm97i4td7MWR0EKQuwvewIrRrdPZKeD5ikEeA00q1mEPQq9dbexVVOq25gJwOnkLy32AisS9uzIulcSQ9Kuskdf0vqVvMzSR+Yw5yDoCXqpqafbtHqSlc3sxPaHPs84CzgsReFkl5A8kb/EDObaMVlJgjmymJf6a6W9GVJx6qVhCSAmV0FPOQO/xnwPjObSPuUuswEwVxZ7CvdgcAxwBtI3kl8ETiv2Vv3JuP8rqQzgZ3A283s2lYvnvkxebsf9cl5gDGncC5xnk+jw1k1bWTc6UJOvcyVwQIYc25fTr30bflERXOpB+7UyrYTAlGgaHoXLacalrp0AUw4N7AJ9zo3d0/XLoqV8+rltq35Pk2o1XsziKalWVnC5WZ2HPBG4CTgGklXSnpeG/cbAnYDjgD+AvhSqytnELRLzdT00y1aWukkrQZeC5wI/DfwFpJ3duuALwP7t3i/jSQe2UZitHVgDfCrgnt6v7ggaAtb5K8MfgD8O/AqM9vYcPxHks6Z5ZoiLgKOBq6QdCCJ28ymoo6NfnGSejNVb9DT1Ar9lbtPq0b31HR1ymFm7y86LukLwFHAGkkbgTOAc4Fz09cIk8BJs40bBPOlR0sZtGx0ayS9A/gt4DE1wMyOnu2C9PmviNe2Pr1icvXDC93AfI1xd97Fzw06P7FcaSyfzRlKMzbnXLr8+SIhpd34OB/7ViasFIyRE0pyLlxOWCka0wsh3mXLiyI+pm+iPDGRbW/PDazWo9lIWp3V54BbSZ7d3ksSX9ey6hgE3aBe8ukWrRrdajP7NDBlZlea2RtIFMgg6FlqqOmnW7S6vZxxCb9f0ktJAvnWdmZKQbAwLHYh5e8krQTeBnwMWAG8tVOTCoKFoEe9wFqOHL8k/XIL8AIASW/t0JxKGVJWbBhzbYCllt05L3NZjEfHsmLC0Ar3P7TMxcL5pEPk4+cYdW0fPzfsY+UKylq1Wbu7VDgpiFPLCScl19hEc2EFyHmk5ISUmhOEch4peSHFplzMXZHY0oReXenmI++cvmCzCIIO0KtCynySzfbmn5EgSKn1qIfhfIyuR189BkFCj5ana250kh6l2LhEkuMvCHqW6cW40plZT+W4HBhIHkGXDGQFiBXkBYmV7s/cClcLd3y3rBgwuMZ5i6xclR1g6YrcPTSWPeZDd0qFk6La3d5zw9fd9p4fznskV7u7hRJUOQ8Tf4/cnAqElJ07mrdz4UJOWCn0SHGi0db2sj/2aNGeKCAS9C+LcnsZBIuZWOmCoGJipQuCiomVbgHxpbJGC14Z+pwoSwezD+XDy7Oi7MAK53GybGWmqaXZNpDPiZIra9VcOMmJJpAXRrxosTMrJtiEyxviS1a1EoZTVtvbe4sUzdsJIbkwnJxHisv1srMgR8qkE1K278j3aUKsdEFQMb2agrw3o/yCYAGYVvNPK7RSczztd5ikmqQ/LBszjC7oW+bre9lQc/xY4GDgOEkHz9Lv/STVfUoJowv6lpqaf1qglZrjkGTHuwBoKXnyonqmG0gFlKXKChQrLf+3Y7da9m/ZyhXZB/XhvbPeIgNrVmXaWrVHtr18Te4eGndiiwvlyXmH+HwmRQlWXZLWXBiOF1JcmwknYPiQGygQUpxnjPMmyYXY+DbADifgeGFk0ocoOSFlR0Foj7umvq29kvcLIKSU1hyXtB/wP0iy3B3WyqCx0gV9Sw1r+pH0Jkk/avj4PKut1Bz/CPBOM2tZt1lUK10QtEPZSrdANccPBc5PE5WvAV4iadrMLppt0DC6oG9ZgFcGpTXHzeyx7OaSzgMuaWZwEEYX9DHT80wMbmbTkmZqjg8C587UHE/Pt5Pd/DEWldGNpMlZR1xOlGUFOeuXu1CepSuzD+oDa5wIsmdz4UTLds/dw+dI8aKGbd+SvSAngjySGzMnjJTV3fbCyQ5f+7tAoPBhNL496cJwvAhSIKR4IcR2eC8XX3M8u/mznfl1ySazfWrb25NGFiLKupWa4w3H/6SVMReV0QVBO0z3aHKDMLqgb+lNkwujC/qYWOmCoGJ60+QWmdHNCCm7K+tNsudU/se7eiwrKCzdN/sgP7BPVjhh72yWeK10wooP46FAONmaLbFuj2ZL79k2J6xsywspeY8S75HivEV8LpIJJ4LsKPJ68clknegx4UKQvAgyVSB67HR9dmZFj7oTRczdsl7gbFKfygpkUzvySYWbUetRs1tURhcE7RDbyyComN40uS75Xkr6P5J+JukmSV+QNFZ+VRC0xzTW9NMtKje61Cv7NOBQM3s6yZv+11Q9j6D/sZJ/3aJb28shYImkKWCcvBNpISMDqZDikss+vp73ulizdzZ3yMhTVmXa2ne/bHufJ2XbS7P9bdvm3D1sy6+ybS+cbM6eZ2tWSLEt+THZmp13TuTIeX6UtCecNwl57w/v+VHPiSDu+vyQ1Caaix61qeyvWt0FtE1N5UWSVvo0o1eFlMpXOjO7F/gQcDdwP7DFzC6reh5B/9OrVXu6sb3cjST6dn9gX2CppNdWPY+g/6mZNf10i24IKccAd5rZr8xsCrgQeL7v1BhgWPkMg76gRr3pp1t045nubuAISePADuCFQM6wGgMMpXnGaAS7JJH3MsXMrpb0FeB6YBr4Mc2jdx9j1UhSkvhpU9kF+skHbcr1HT8kW7548DnrMm099dDs+f0OyrRrG2/Ozvv+O/ITemBjts+DWeGkvmlztv1I1tuktikvAE0/kv374gWKye1ZMWF6Itv2YsPUpKscBEzXsj+/uivOPVVzIogLnfLtomOTuHm6/lMuYfBUQVkr7/fSbumrbq5mzeiKemlmZwBndOPewa5Db5pceKQEfUzNetPswuiCvqU3TS6MLuhj4pluAVg6mLhornWVbMb2z38bA/uszrT1tGwe0Jxwcu+tmbbddn22fd+9uXvU7/fCyaOZ9tQD2XlOPZoVArY/nBc5JnZmvxcvjEw4z45JJ3rsNCekFKRu9KJFzfXx5/2vbr3gRZO/xkXlFAgp2XZRxmV/Tbulr7r5Lq4Zi8rogqAduulf2YwwuqBvCSElCCqmHitdEFRLrHRBUDG9uc4tMqNbNZiUotp9OJtsZ2jf3XJ9Bx6XTTQ0uDZbyy/n5nXLtdnz192QaU/+YnPuHtvuy/74tm3J1hzfsnV59nwt2/8R5X/8Wwe8e1T2/A6nHDovMJzXGFMFv3r+iK9K6rdlrdQE8PK8j8z2Y/hYt6kCed8faVcYmY5XBkFQLbG9DIKK6dVXBlEUMuhbalZv+mkFSesl/VzS7ZLeVXD+BEk3pp/vS3pm2Zix0gV9i83TI0XSIPBx4EUkBSKvlbTBzBoFgTuB3zOzhyUdSxKmdnh+tN+wqIxOqbvS2oOyCX4Gjvyfub5Dz3lJpj39jU9n2vV7srFw9ft/nWlvuTob+7bpgWxpLYBf78zWGH/UlfB6eDC7kdjhvL62FOwztin7F9gLITtLBAgvULSSnKdMsMi5gRX8MvsjZcKKv8dUwcrj5z5YWI14dhbA9/K5wO1mdgeApPNJUo08ZnRm9v2G/j8kqdbalNheBn1L3azpp4Wa4/sB9zS0N6bHZuNPga+XzWtRrXRB0A5lz20t1BwvWloLtw6SXkBidL9TNq8wuqBvWYBXBhuBxzW011KQo1XSIcCngGPN7Nf+vCe2l0HfsgAZnq8FniJpf0kjJJnINzR2kPR4kox2J5rZba0MuqhWuoMGEzFj/NXZWDgvmgBMX5cpE039luzPY/q+hzPtnXdmo7d++ctszfG7B7LeJgAPjmZ3H1td0rItyvphTLj/6EfI1+7eadlrvFAyYV44cW3zQkr+r32ZquevKRJOcmOWnPfeIX4O05b3e/GeMQPtCinzXOnMbFrSqcA3SdL/n2tmP5N0cnr+HOBvgNXA2UpiCqfN7NDZxoRFZnRB0A62AB4pZnYpcKk7dk7D128E3tjOmGF0Qd8SbmBBUDERTxcEFVOrx0o3b46bTBL9DP/BWzLHpy78WK7v9guyiYU23pr1KHlo5+7Z80NZd5FbxrL/YQ+RL4r9kCucPenEgG31rFDitzs76nkhxQsK/q+1FyByYTjuHnNx+m1FOCljwCc/KtnqtbIVVJtCSj22l0FQLbG9DIKKie1lEFTMfKMMOkUYXdC39OorA/XqX4NGZurTTf7qvwC47pC3Z85/YSSfKfnWmqvv7fb3m2s7Mu1ttWzelc2T2drfk/V8oe3J2nTTdtmDfH0O259e+N9qRc5Qm2WtWhljRkiZmEgc/80KanY1sGx8/6Y/rq3b75z/JOdArHRB39KrK10YXdC39OoublEZ3cgeB3R7CsEiIt7TBUHFLMRL/k6wKISUbiLpTWmE8S4/j16YQy/NY65EEGs5Pm9Gt+iFefTCHKB35jEnwuiCoGLC6IKgYsLoyumVZ4demEcvzAF6Zx5zIoSUIKiYWOmCoGLC6IKgYsLogqBiwugakDQi6XWSjknbx0s6S9Ipkoa7Pb9uI2nPbs+hHwghpQFJnyNxjRsHNgPLSLL3vpDkZ3VSF+e2upWU3Qt4v939IeA64FkkP4uHKprHCuAvSVKaf93MPt9w7mwz+99VzGMhCaNrQNKNZnaIpCHgXmBfM6spCez6iZkdUtE83gd8yMw2SToU+BJJxaph4HVmdmUFc6gDv3SH15Lk9zcze1Kn55DO4wLgFyRlqN4ATAHHm9mEpOvN7NlVzGMhie1lloE0Z/1yktVuJoXYKMkvfFW81Mw2pV9/EPhjM3sySXHCf6hoDu8Afg68wsz2N7P9gY3p15UYXMoBZvYuM7vIzF4BXA98R9LqCuewoESUQZZPA7eS5K1/N/BlSXcARwDnVziPYUlDZjYNLDGzawHM7DZJ+aIKHcDMPpQWQfywpHuAM+hO4PqopAFLc6Sb2ZmSNgJXkWz/Fx2xvXRI2hfAzO6TtAo4BrjbzK6pcA5vAV4OvA84EljFb54tn2RmJ1Y1l3Q+Lyf5I/REM9u74nt/ALjMzL7ljq8HPmZmT6lyPgtBGF2PIuko4M+AA0l2JPcAF5FUjsknbOnMHA4iqTx6NVAj2erdJGm9mX2jijmk83guyXPktZIOBtYDt6bFPRYdYXSLDEmvN7N/reA+pwGnALcA64A/N7OL03OVCRiSzgCOJfnDczlwOHAFyQ7km2Z2ZhXzWEjC6BYZku42s8dXcJ+fAs8zs62Sngh8Bfh3M/uopB+b2bM6PYeGeawjEbMeANaa2SOSlgBXV6UoLyQhpPQgkm6c7RSwV0XTGDSzrQBmdle63f2KpCfQWha+hWLazGrAdkn/ZWaPpHPakb7WWHSE0fUmewEvBh52xwV8v6I5PCBpnZndAJCueC8DzgWeUdEcACYljZvZduA5MwclrYSCMrOLgDC63uQSYNnML3wjkq6oaA6vAzKCTSrgvE7SJyqaA8CRZjaR3r/RyIaBrnkIzYd4pguCigmPlCComDC6IKiYMLouImlrea+g3wij2wVIoyaCHiGMrseQ9HJJV0v6saRvSdpL0oCkX0jaI+0zIOl2SWsk7SHpAknXpp/fTvu8R9InJV0GfKar31SQIYyu9/gucETq8XE+8I5UKv8scELa5xiS+L5NwEeBD5vZYcCrgU81jPUc4JVmdnxlsw9KiW1H77EW+KKkfYAR4M70+LnAxcBHSII5Z/wvjwEObiiguELS8vTrDWaWrX4ZdJ1Y6XqPjwFnmdkzgDcDYwBmdg/w35KOJnH6/Xraf4DER3Jd+tnPzB5Nz22reO5BC4TR9R4rSVJFQN7j4lMk28wvpf6IAJcBp850kLSu0xMM5kcYXXcZl7Sx4XM68B6SiPX/BDa5/htIoqUbQ3tOAw6VdKOkm4GTq5h4MHfCDWwRkSYp+rCZ/W635xLMnRBSFgmS3kUSSX5CWd+gt4mVLggqJp7pgqBiwuiCoGLC6IKgYsLogqBiwuiCoGLC6IKgYv4/Tpg0iId337EAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_Heatmap(tested_heatmap,'kernel',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Results_Article/1B'\n",
    "total_acc = list()\n",
    "num_blocks = 8\n",
    "for i in range(0,num_blocks):\n",
    "    with open(path + '/accuracy_Blocks_'+ str(num_blocks) + '_L' + str(i+1) +'.pkl','rb') as file:\n",
    "        partial_acc = pickle.load(file)\n",
    "        total_acc.append(partial_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADQCAYAAAA53LuNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmRElEQVR4nO3deZhU1ZnH8e+vF+hmbRBQ2QQVcBcEcTdGjRgT0aiToDHRbI6ZaKIxGjVPEjWThMQ4mUxMJuM4JsZdERHjrmjEBRVoWVzYFIQGZG3WbujlnT/ubSy6a7lVXdXVdL2f56mHqruc+9al+q1T5557jswM55xzba8o3wE451yh8gTsnHN54gnYOefyxBOwc87liSdg55zLE0/AzjmXJ56AXZuSZJIOzHDfkyQtyHZMEY47QtI7krZI+n5bH991XJ6AXVySlkqqkbQ15nF7G8ewW7I2s+lmNqItYwhdB7xkZt3N7L8SbSTpb5LqJe3bhrG5PZgnYJfM2WbWLeZxRb4DypP9gHeTbSCpK3A+sAm4uC2Cijl2SVsez2WPJ2CXFkmdJVVLOixmWd+wttwvfP0dSYslbZA0VVL/BGW9LOnbMa8vlfRq+PyVcPGcsPb9FUmnSFoRs/3BYRnVkt6VND5m3d8k/UnSk2HTwZuSDkjyvsaHZVSHZR4cLp8GfBa4PYxjeIIizgeqgVuAS5qV3VvSXyWtlLRR0pSYdeeEzRubJS2RdGa4fKmk02O2u0nSveHzIeGvg29J+hiYFi5/RNJqSZskvSLp0Jj9yyXdJmlZuP7VcNmTkq5sFu9cSV9KdK5c9ngCdmkxsx3AZODCmMVfBv5pZmsknQr8Oly2L7AMeDCD45wcPj0yrH0/FLteUinwBPAc0A+4ErhPUmwTxQTgZqAXsBj4ZbxjhUn1AeAqoC/wFPCEpE5mdiowHbgijGNhgpAvCct4EDhI0uiYdfcAXYBDw1h/Hx53LPB34FqgAjgZWJrwpLT0GeBgYFz4+mlgWHiM2cB9Mdv+DhgNHA/0JmhWaQTuJqbGLulIYADwZBpxuAx5AnbJTAlrhE2P74TL7ydIbk0uCpcBfBW4y8xmh8n6BuA4SUOyHNuxQDdgopntNLNpwD/Y/YvhMTN7y8zqCZLRyARlfQV40syeN7M6gmRVTpCsUpI0mKCWfL+ZfQK8CHw9XLcv8HngcjPbaGZ1ZvbPcNdvEZyr582s0cyqzOyDyGcAbjKzbWZWA2Bmd5nZlvC83wQcKamnpCLgm8APwmM0mNnr4XZTgeGShoVlfg14yMx2phGHy5AnYJfMuWZWEfP433D5S0AXSceEiXUk8Fi4rj9BrRcAM9sKrCeoVWVTf2C5mTXGLFvW7DirY55vJ0jYicqKjbkRWE70mL8GvG9m74Sv7wMuCmvpg4ANZrYxzn6DgCURjxHP8qYnkoolTQybMTbzaU26T/goi3csM6sFHgIuDhP1hQQ1dtcGvPHepc3MGiQ9TPDH+gnwDzPbEq5eSXDRCth1cWovoCpOUdsIfpo32SeNMFYCgyQVxSThwUCiJoJUZR3e9EKSCJJjvJjj+TowWFJTwi8heM9nAW8BvSVVmFl1s/2WA4napaOcm9ihDC8CzgFOJ0i+PYGNgIB1QG14rDlxyrmbIOm+Cmw3szcSxOSyzGvALlP3E/x0/yqfNj9A0A76DUkjJXUGfgW8aWZL45TxDnCepC5hd7NvNVv/CbB/guO/SVCrvU5SqaRTgLPJoL0ZeBj4gqTTwlrrNcAO4PVUO0o6jiCxjSX4JTASOIzgnHzdzFYRtM3+WVKvMNam9u3/IzhXp0kqkjRA0kHhuneACeH2Y4ALUoTSPYx5PUHi/lXTivAL6i7gPyT1D2vLx4X/P4QJtxG4Da/9tilPwC6ZJ7R7P+CmZgbM7E2CWlp/ggTTtPwF4KfAo8AqguQ0gfh+D+wkSLR3s/tFIwjaMe8O25+/HLsibKM8m6B9dR3wZ4KEl04balNZCwguRP0xLOtsgi54UdpBLwEeN7N5Zra66QH8AfiipN4ETRR1wAfAGoKLfZjZW8A3CM7DJuCffPrr4acE524jwYXE2C+5eP5O0IxSBbwHzGi2/kfAPOBtYAPwG3b/+/87wa+AeyO8Z5cl8gHZnXOSvg5cZmYn5juWQuI1YOcKnKQuwL8Bd+Q7lkLjCdi5AiZpHLCWoBkoVTOHyzJvgnDOuTzxGrBzzuVJh+kH3KdPHxsyZEi+w3DOuRZmzZq1zsz6Nl/eYRLwkCFDmDlzZr7DcM65FiQti7fcmyCccy5PPAE751yedJgmCOecy5UplVXc+uwCVlbX0L+inGvHjeDcUa0fX8oTsHPOJTGlsoobJs+jpq4BgKrqGm6YPA+g1UnYmyCccy6JW59dsCv5Nqmpa+DWZ1s/P6wnYOecS2JldU1ay9PhCdg555LoX1Ge1vJ0eAJ2zrkkrh03gpIi7basvLSYa8eNSLBHdJ6AnXMuibOP7E/XzsV0LilCwICKcn593uHeC8I553LtjSXr2VRTz58uOoovHLFvVsv2GrBzziUxadZyepSVcNrB/bJetidg55xLYHNtHc+8u5rxI/tTVlqc9fI9ATvnXAJPzV1FbV0jF4welJPyPQE751wCk2at4MB+3ThyYM+clJ/TBCzpTEkLJC2WdH2c9b+X9E74WCipOmbdJZIWhY9Lchmnc84199G6bcxctpELRg9EUuodMpCzXhCSioE/AZ8DVgBvS5pqZu81bWNmV8dsfyUwKnzeG/g5MAYwYFa478Zcxeucc7Emz15BkeBLWehulkgua8BjgcVm9qGZ7QQeBM5Jsv2FwAPh83HA82a2IUy6zwNn5jBW55zbpbHReHTWCk4e3pe9e5Tl7Di5TMADgOUxr1eEy1qQtB8wFJiW7r7OOZdtb3y4npWbarlg9MCcHqe9XISbAEwys4aUW8aQdJmkmZJmrl27NkehOecKzaRZK+hRVsLpB++d0+PkMgFXAbF9NwaGy+KZwKfND5H3NbM7zGyMmY3p27fFfHfOOZe2LbV1PD1/FWcfmZu+v7FymYDfBoZJGiqpE0GSndp8I0kHAb2AN2IWPwucIamXpF7AGeEy55zLqafmNfX9zW3zA+SwF4SZ1Uu6giBxFgN3mdm7km4BZppZUzKeADxoZhaz7wZJvyBI4gC3mNmGXMXqnHNNJs1awQF9uzJyUEXOj5XTwXjM7CngqWbLftbs9U0J9r0LuCtnwTnnXDNL123j7aUb+fGZB+Ws72+s9nIRzjnn8q4t+v7G8gTsnHOEfX9nV3HSsL7s0zN3fX9jeQJ2zhW8KZVVjP3VC1RV1zB3RTVTKhN12MouH5DdOVfQmk87v3F7XdamnU/Fa8DOuYKWy2nnU/EE7JwraLmcdj4VT8DOuYKWy2nnU/EE7JwraD86YzjNe/xma9r5VDwBO+cK2hGDKjCgorw069POp+K9IJxzBW36wmAkxSeuPJFBvbu06bG9BuycK2jTF61jyF5d2jz5gidg51wB21nfyBsfruekYfkZztYTsHOuYM3+eCPbdzZw0rA+eTl+XmdFDrf5sqT3JL0r6f6Y5Q0xMya3GEfYOeda69VF6yguEscdsFdejp/XWZElDQNuAE4ws42S+sUUUWNmI3MVn3POTV+0llGDKuheVpqX4+d7VuTvAH9qmm7ezNbkMB7nnNtl47adzK3alLf2X8j/rMjDgeGSXpM0Q1Ls1PNl4YSbMySdG+8APimncy5Try1ZhxmcNDw/7b+Q/37AJcAw4BSCiTdfkXS4mVUD+5lZlaT9gWmS5pnZktidzewO4A6AMWPGGM45F9H0hevoUVbCEQN65i2GfM+KvAKYamZ1ZvYRsJAgIWNmVeG/HwIvA6NyGKtzroCYGdMXreWEA/tQUpy/zmD5nhV5CkHtF0l9CJokPgxnQ+4cs/wE4D2ccy4LlqzdxspNtXlt/4X8z4rcNP38e0ADcK2ZrZd0PPA/khoJviQmxvaecM651pi+KLhmlK/+v03yOityOBX9D8NH7DavA4fnMjbnXOGavmgdQ/t0zcvtx7H8TjjnXEHZUd/AG0vW5732C56AnXMFZvayamrqGvLe/guegJ1zBWb6orWUFIlj9++d71A8ATvnCsv0Res4anCvvN1+HMsTsHOuYKzfuoP5Kze1i/Zf8ATsnCsgry1ZH95+nP/2X/AE7JwrINMXrqVneSmH5/H241gpE7CksyV5onbO7dGC24/XceKBfSguaj4Pcn5ESaxfARZJ+q2kg3IdkHPO5cLiNVtZvbm23bT/QoQEbGYXEwyEswT4m6Q3wmEgu+c8Ouecy5JXFq0D4MQ9KQEDmNlmYBLBoOr7Al8CZku6MoexOedc1kxftJb9+3ZlYK/83n4cK0ob8HhJjxEMCVkKjDWzzwNHAtfkNjznnGu9HfUNzPhwPSe3g7vfYkUZjOd84Pdm9krsQjPbLulbuQnLOeeyY0plFf/+5HvU1jXyxJyVjBxUwbmjmk/Okx9RmiBuAt5qeiGpXNIQADN7MdmOrZwV+RJJi8LHJVHejHPOxZpSWcUNk+exbutOANZv28kNk+cxpbL53BD5ESUBPwI0xrxuCJclFTMr8ueBQ4ALJR3SbJvYWZEPBa4Kl/cGfg4cQzC5588l9YoQq3PO7XLrswuoqWvYbVlNXQO3PrsgTxHtLkoCLglnNQYgfN4pwn6tmRV5HPC8mW0I1z0PnIlzzqWhqrom7vKVCZa3tSgJeK2k8U0vJJ0DrIuwX2tmRY6yr8+K7JyLq7augZ88Ni/h+v4V5W0YTWJRLsJdDtwn6XZABInx61k8fotZkaPu7LMiO9exTams4tZnF7Cyuob+FeVcO25Eygtoiz7ZwhX3V7Lgky2cOqIvr3+4ntq6T1tRy0uLuXbciFyHHknKBBxOBX+spG7h660Ry446K/KbZlYHfCSpaVbkKsLJOmP2fTnicZ3LmkwSQHs+zp6k6QJaUxtuVXUNN0wOarVN52b381bGiQf24fE5K+naqYS7vzmWzwzv267PrYJp2VJsJH0BOBQoa1pmZrek2KeEYJr50wgS6tvARWb2bsw2ZwIXmtkl4ezHlcBIwIBZwFHhprOB0Wa2IdHxxowZYzNnzkz5XpyLqnkCgKD29OvzDt/tDzjKH3iybaIep9AcP/FFVlbXtli+T88y3rj+VB5/Z2WL8wYwvF837v32MfTrUdZi33yRNMvMxjRfnrIGLOkvQBfgs8CdwAXEdEtLpDWzIofH/QVB0ga4JVnydS4XEl1B/+WT73PQvt0pKynm5YVrmPj0B7t+4iaqpbWsyc1lc20dhw3oyc1PvJvwSn0+EnBrv1Bac5xzRvbnneXVPDl3VdzkC7B6Uy0jfvoMDY1GQ2PLCuTWnfXtKvkmk7IGLGmumR0R82834GkzO6ltQozGa8AuE4kSyfIN2znpty9lXG5JkTikfw+6diqh8uON1NY3pt4pjge+cyzHDO1NUZHa5Kd0lNp4Nmrs8cooKRLdOpdQXVNHabEoluKet57lJUw4ejD/88qHccsW8NHEL0SKo61kXAMGmr6GtkvqD6wnGA/CuXYtVcKKVzP90SNz+N2zH7AiQe0LYK+unfjFuYdRW9fADx+eE3eb+kajd9dObNtRnzT53nXpGK5/dB5rtuxosU7Ahf87g/49yzi4fw9eXbSOHfWJa9pR3nMqv3nmg7i18WsemcNfX19KRXkpb320IWWNPVkcjY3Gr556v0UZ9Y1GTV0Dv/uXI/ncIXvz0gdr4ib6m8cfxrmjBvCPuavidjNrLz0cooiSgJ+QVAHcStAWa8D/5jIo51orXnK9btJcKj/eyGEDelIbJox4SeCTLTu44fMHUVwkbntuYYsE8NMvHsJZhwd1kNueWxg3CQyoKOdv3xgLwAkTpyXc5tSD9ubGs+oTJJpD6FxazGOVVbz4/poW+9fUNXDzE++yV7dOVJR34q2l67n12QVJm0MS2bqjnr+/sZRVm+J/8TQ0Gj3LS6nevrPFOWtSVV3Dzx6fz476BqZUrtzty+K6SXN4Ys5KauoamFe1iS219XHL2FnfyAWjB+4Wc6JEfu24EXHPW3vp4RBF0iaIcCD2Y83s9fB1Z6DMzDa1UXyReROEi3X8r19kZYJkkkrsT9h0a9GQ2U/2VMcZev2TZNrPsn/PMl6/4bS4x/n+qQeyYXsdd7yyhI3b6+hcUrQrccYaUFHOa9efCiT+QulUXERpsdi2M36CBjh8QE+OGNiTJ+euorqmLulxomjPPRxiJWqCiNIGXGlmo3IWWZZ4AnYQ/Lx9fE4VVz8Uv2lAwCvXfZay0mLG3/5q3BpfLpJAaxNFoqTXr3tnbr/oKKq37+Sye2Yl3H/s0N707lLKtAVr2RknwZ4yoi8/OG0Yy9Zvb9UXyvgj+3PAjU/F/bJo/sVWSD0/WtMG/KKk84HJFqXPmnNtIF5C69OtM7966n3eW7WZ0mJR19Dy49q/opxBvYPxYH985kFZ+Ql77qgBKZNGlG2SSfRz+8azDmbs0N5A8MURL0l361xCzc4Gnvkofkeivt0672ouGTU4GHIl2ZdFqqaB/gniiG2bTVVGoYhSA94CdAXqCS7ICTAz65H78KLzGvCeIxf9ZosEjQYDewXbNjYYN06Zn5U+vO1Fa5tDEjVjZLvXQKHVbqPIuAZsZj71kMuaqHc33TB5LjUxF5N+/OhcFq/ZwughveP2m220oHvSi9d8hs4lxQCoSCmTa2trpm0pVazZqJlmK85kcbhPRakBnxxvefMB2vPNa8B7hkRtmaXFYr+9urK1tp5PNtdmdMGpPfb/bE+8Zpo/rWkDvjbmeRnBMJOzgOhXKVzBMzNmLduYcHjAugZj+N7d6Na5hIdnrkhYzuR/O57L75kVt9/sntT/Mx+8Ztr+RGmCODv2taRBwH/mKiC3Z2veTvlvpxxAbX0jD771MYvWbA0uIMTZb0BFOX/+6mgAXlu8PmG/2aMG9+LGsw7e4/t/5sue1ORSCKLUgJtbARyc7UDcni9e++5PpswHYOSgCn5z/uFgcNMT7yVNnqk62HtNznUUUQbj+SOfVlqKCEYrm53DmFwOtMXV/ni3l0LQV3XK907Y9bpzaXGrujk1beMJ1+3polyEi50Qsx5YamavRSo8GG7yDwSjod1pZhObrb+U4BbnpnGCbzezO8N1DUDTkPYfm9l4kvCLcIll8+JL80T+nZOHsrO+kccqV/L+qs1x9/GLY67QteYi3CSg1swawoKKJXUxs+0pDtg0KefnCJot3pY01czea7bpQ2Z2RZwiasxsZIT4XArJJiZMJwHHa2K4aWrw33nkoAp6lpewqablPf5+ccy5+KLMCfciEPsXVA68EGG/KJNyujaQaALCquqa3dZNqazihInTGHr9k5wwcdquqbvNjHdXbuJnj8+P28Swd4/OPP69E7h5/GGUlxbvts4vjjmXWJQacFnsNERmtlVSlwj7xZtY85g4250f9jVeCFxtZk37lEmaSdDsMdHMpkQ4poujokspG7e3HPgE4PiJ0zhsQA8G9erCtA/W7DaC1bWT5nDfjGUs3bCdtXG6fTVZszlY5xfHnEtPlAS8TdJRZjYbQNJoIFtzOj8BPGBmOyT9K3A3n/Yv3s/MqiTtD0yTNC+cn24XSZcBlwEMHjw4SyF1HFt31PPTKfPZuL1u1626TcpLi7nqc8Mwg+feXc3T81e32L+uwZj58Ua+eER/Th7Wh9ueW8jqzS0Hr2l+j78nXOeiiZKArwIekbSS4HrKPsBXIuyXclLOpumHQncCv41ZVxX++6Gkl4FRwJJm+/usyAnMXVHNlQ9UsnzDdn74ueEMrCjntucXxq2ZXv6ZAxIPd2jwxwuDwfBKi4u8/61zWRTlRoy3JR0ENP2VLQhnMU7lbWCYpKEEiXcCcFHsBpL2NbNV4cvxwPvh8l7A9rBm3Ac4gZjk7FqK7Z3QvayELbX17NuzjIf+9TiOHhKMlnVeONB1PD6ClXNtL0o/4O8B95nZ/PB1L0kXmtmfk+0XcVLO70saT9DOuwG4NNz9YOB/JDUSXCicGKf3hAs1752wubaeIsEVpx64K/mmEnV2AW9icC57ovQDfqd5d7D2OEh7R+4HnOwmipXVNZz1h+kFNbuAc3ua1vQDLpakpsHYw/69nbIdoIsv7txmj87libkrqdpYwwertyTcN1H3s0S8dutc24rSD/gZ4CFJp0k6DXgAeDq3Ybkm8W6i2FnfyIvvr6GiSyk3nnUQ/bp3jruv3wDhXPsWpQb8Y4KuXpeHr+cS9IRwOVbX0Jhw+EYBD152HAD9upd57wTn9kBRekE0SnoTOAD4MtAHeDTXgRWKeO2uZxy6Nw+9vZw7p3+UcD/vneDcni/hRThJw4ELw8c64CHgR2a2X9uFF92eeBEu3iA5JUWiU7HYXtfI0UN6MXJQBffOWLZreh7wWQyc29NkchHuA2A68EUzWxwWcnWO4itI8dp36xuNkiIx6fLjGBN2ITu0f0+v3TrXASVLwOcR3DzxkqRnCAbTUZtEVSAS9VLYUd+4K/mC905wrqNK2AvCzKaY2QTgIOAlgluS+0n6b0lntFF8HVq/Ht57wblClrIbmpltM7P7w7nhBgKVBD0jXCssXrOF2jhDO3rvBecKR5R+wLuY2UYzu8PMTstVQIWg8uONXPCXN+hUEiTbARXliODONb+45lzhyGRSTtcKryxcy+X3zqJPt87c+61jGLxXF7732QPzHZZzLg88AedYbD/fii6lbKqpY8Q+Pbj7m0fTr3tZvsNzzuWRJ+Acat7Pt2lg9K8dN9iTr3MuvTbgdEk6U9ICSYslXR9n/aWS1kp6J3x8O2bdJZIWhY9Lmu+7J4jXz7fR4E/TliTYwzlXSHJWA27NrMiSegM/B8YABswK992Yq3izzcwSjuOQ7ihlzrmOKZc14NbMijwOeN7MNoRJ93ngzBzFmXWzlm3gS39+PeF67+frnIPctgG3ZlbkePu26JuV70k5mw+k840ThjD74408NW81/bp3ZsLYQTxeWdViHAfv5+ucg/xfhEs2K3JK+ZyUM95A6f/+5PuUFomrTx/Od04eSpdOJRw7dC8fx8E5F1cuE3BrZkWuAk5ptu/LWY+wFeJdYAPYq1tnfnD6sF2vfRwH51wiuWwD3jUrsqROBAP7TI3dQNK+MS93zYpMMJHnGeEEoL2AM8Jl7UaiC2mfbK5t40icc3uqnNWAWzMrspltkPQLgiQOcIuZbchVrJmIMo27c84lk3JW5D1FWw/IPqWyimsenkNDzPnzgdKdc/G0ZlZkF8cJB/bBMLp2Lmb7jga/wOacS5sn4Aw9PHM5jQZTrziRA/p2y3c4zrk9UE5vRe6oGhqN+2Ys48QD+3jydc5lzBNwBqZ9sIaVm2q5+Nh2OT+pc24P4Qk4A/fMWMY+Pco4/eB++Q7FObcH8wScpqXrtvHKwrVcOHYwJcV++pxzmfMMkqb73lxGSZGYMHZQ6o2dcy4JT8BpqK1r4JFZKxh36D7s3cMHVHfOtY4n4DT8Y+4qqrfX+cU351xWeAJOwz0zlnFgv24cu3/vfIfinOsAPAFHNHdFNXOWV3PxMYORlO9wnHMdgCfgiO6dsYzy0mLOGz0w36E45zoIT8ARbNpex9Q5Kzl31AB6lJXmOxznXAeR11mRY7Y7X5JJGhO+HiKpJma25L/kMs5UJs1eQW1dIxcf2/bTHjnnOq68z4osqTvwA+DNZkUsMbORuYovqsZG494Zyxi9Xy8O7d8z3+E45zqQ9jAr8i+A3wDtbiqJKZVVHP3LF/ho3TY+XLuVKZVVqXdyzrmIcpmAU85sLOkoYJCZPRln/6GSKiX9U9JJ8Q4g6TJJMyXNXLt2bdYCh08n3Vy/bScAG7fXccPkeZ6EnXNZk7eLcJKKgP8AromzehUw2MxGAT8E7pfUo/lGZnaHmY0xszF9+/bNanzxJt2sqWvg1mcXZPU4zrnClcsEnGpW5O7AYcDLkpYCxwJTJY0xsx1NMyab2SxgCTA8h7G2kGjSzUTLnXMuXXmbFdnMNplZHzMbYmZDgBnAeDObKalveBEPSfsDw4APcxjrbsyM8k7Fcdf5pJvOuWzJ96zIiZwM3CKpDmgELm/LWZEffHs523c2UFIk6ht3n3Tz2nEj2ioM51wH57MiNzN3RTUX/PcbHLN/b740cgC3Pb+QldU1Pummcy5jPityBBu27eS7986mb/fO/NeEUfTq2slvPXbO5Ywn4FBDo/GDBytZu2UHj1x+HL26dsp3SM65Ds4TcOgPLyxk+qJ1/Pq8wzlyUEW+w3HOFQAfjAd48f1P+K9pi/mX0QOZcLRPNeScaxsFWwOeUlnFrc8u2NWvd0BFGb849zAf69c512YKsgbcdJtxVXUNBhiwbutOnpm/Ot+hOecKSEEm4Hi3Ge+ob/TbjJ1zbaogE7DfZuycaw8KMgEnup3YbzN2zrWlgkzA144bQXnp7mM9+G3Gzrm2VpC9IJpuJ27qBeG3GTvn8qEgEzAESdgTrnMunwqyCcI559oDT8DOOZcnHWY4SklrgWXNFvcB1mWh+GyU05HKaE+x+Ptp37H4+wnsZ2Yt5k3rMAk4Hkkz443BmY9yOlIZ7SkWfz/tOxZ/P8l5E4RzzuWJJ2DnnMuTjp6A72hH5XSkMrJVTnspI1vltJcyslVOeykjW+W0lzJ26dBtwM4515519Bqwc861W56AnXMuTzpsApZ0pqQFkhZLuj6D/QdJeknSe5LelfSDVsRSLKlS0j9aUUaFpEmSPpD0vqTjMijj6vC9zJf0gKSyiPvdJWmNpPkxy3pLel7SovDfXhmUcWv4fuZKekxSRbplxKy7RpJJ6pNJGZKuDGN5V9Jvk5WR5P2MlDRD0juSZkoam6KMuJ+xdM5tkjIin9tUn/U0zm3CcqKe3yTvJ/K5lVQm6S1Jc8Iybg6XD5X0ZpgTHpKUdObdJOXcF+aW+eHnoDRZOUmZWYd7AMXAEmB/oBMwBzgkzTL2BY4Kn3cHFqZbRkxZPwTuB/7Rivd0N/Dt8HknoCLN/QcAHwHl4euHgUsj7nsycBQwP2bZb4Hrw+fXA7/JoIwzgJLw+W8yKSNcPgh4luBGnD4ZxPFZ4AWgc/i6X4bn5Dng8+Hzs4CXM/mMpXNuk5QR+dwm+6yneW4TxRL5/CYpI/K5BQR0C5+XAm8Cx4af+Qnh8r8A303xfhKVc1a4TsADqcpJ9uioNeCxwGIz+9DMdgIPAuekU4CZrTKz2eHzLcD7BEksLZIGAl8A7kx335gyehL8wf9fGM9OM6vOoKgSoFxSCdAFWBllJzN7BdjQbPE5BF8KhP+em24ZZvacmdWHL2cAAzOIA+D3wHUEs0sllaCM7wITzWxHuM2aDMsxoEf4vCcpzm+Sz1jkc5uojHTObYrPejrnNlE5kc9vkjIin1sLbA1floYPA04FJoXLo3xm45ZjZk+F6wx4ixSf22Q6agIeACyPeb2CDJJnE0lDgFEE34Dp+k+CD3BjpscHhgJrgb+GTRl3SuqaTgFmVgX8DvgYWAVsMrPnWhHT3ma2Kny+Gti7FWUBfBN4Ot2dJJ0DVJnZnFYcezhwUvjz9J+Sjs6wnKuAWyUtJzjXN0TdsdlnLKNzm+RzGvncxpbRmnPbLJaMzm+zMq4ijXOroNnvHWAN8DzBL+LqmC+lSDmheTlm9mbMulLga8AzUd5PPB01AWeNpG7Ao8BVZrY5zX2/CKwxs1mtDKOE4Ofuf5vZKGAbwU/TdGLpRVCzGgr0B7pKuriVcQFBlYAINaQksf0EqAfuS3O/LsCNwM8yPXaoBOhN8PPyWuBhKaPpsb8LXG1mg4CrCX+xpJLsMxb13CYqI51zG1tGuE9G5zZOLGmf3zhlpHVuzazBzEYS1E7HAgel+z7ilSPpsJjVfwZeMbPpmZQNHTcBVxG0XTUZGC5LS/gN9yhwn5lNziCOE4DxkpYSNIOcKuneDMpZAayI+fadRJCQ03E68JGZrTWzOmAycHwGsTT5RNK+AOG/KX+2xyPpUuCLwFfDZJOOAwi+UOaE53ggMFvSPmmWswKYHP6qfIvg10rSC04JXEJwXgEeIfjDTyrBZyytc5voc5rOuY1TRkbnNkEsaZ3fBGWkfW4Bwqa6l4DjgIqw+Q3SzAkx5ZwZxvhzoC/B9Z2MddQE/DYwLLzq2QmYAExNp4DwG/r/gPfN7D8yCcLMbjCzgWY2JIxhmpmlXes0s9XAcklNcyadBryXZjEfA8dK6hK+t9MI2tcyNZXgj4Lw38fTLUDSmQTNM+PNbHu6+5vZPDPrZ2ZDwnO8guACzuo0i5pCcKEIScMJLnJmMuLVSuAz4fNTgUXJNk7yGYt8bhOVkc65jVdGJuc2yfuZQsTzm6SMyOdWUl+FvT4klQOfI/isvwRcEG6W8jOboJwPJH0bGAdcaGataVrsmL0g7NMrpQsJ2n5+ksH+JxL89JsLvBM+zmpFPKfQul4QI4GZYTxTgF4ZlHEz8AEwH7iH8Kp0hP0eIGg3riP4Q/wWsBfwIsEfwgtA7wzKWEzQVt90fv+SbhnN1i8l9ZX6eHF0Au4Nz8ts4NQMz8mJwCyCXjdvAqMz+Yylc26TlBH53Eb5rEc8t4liiXx+k5QR+dwCRwCVYRnzgZ+Fy/cnuGi2mKAWnfTzn6SceoK80hTfzzL9u/ZbkZ1zLk86ahOEc861e56AnXMuTzwBO+dcnngCds65PPEE7JxzeeIJ2HV4kram3sq5tucJ2LksibnLyrlIPAG7giTp7HBwmEpJL0jaW1KRgjF4+4bbFIVjx/YNH49Kejt8nBBuc5OkeyS9RnBzi3OReQJ2hepV4FgLBjd6ELjOgttK7wW+Gm5zOjDHzNYCfwB+b2ZHA+ez+/CihwCnm9mFbRa96xD8J5MrVAOBh8LBbjoRDFYPcBfBGAH/STCM41/D5acDh8QM4tUjHLELYKqZ1bRF0K5j8RqwK1R/BG43s8OBfwXKAMxsOcFoZKcSjLjVNI5uEUGNeWT4GGCfDta9rY1jdx2EJ2BXqHry6XCElzRbdydBU8QjZtYQLnsOuLJpA0kjcx2g6/g8AbtC0EXSipjHD4GbgEckzaLl0IhTgW582vwA8H1gjIJJLt8DLm+LwF3H5qOhOdeMpDEEF9xOyncsrmPzi3DOxZB0PcH0N19Nta1zreU1YOecyxNvA3bOuTzxBOycc3niCdg55/LEE7BzzuWJJ2DnnMuT/wc0H+mmmJWiaAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Change this parameters\n",
    "accuracy_evolution_tested = total_acc # the index represents the position of num_blocks\n",
    "name = '_' + str(num_blocks) +'L' # num_block[1] = 12 (layers)\n",
    "######################################################\n",
    "block = len(accuracy_evolution_tested)\n",
    "x = list(range(1,block+1))\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.plot(x,accuracy_evolution_tested,marker='o')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.locator_params(axis='x', nbins=block)\n",
    "plt.title('Evolution of Accuracy')\n",
    "plt.tight_layout()\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.savefig('Results_Article/1B/EvolutionAcc_' + name +'.png') \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_image_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
