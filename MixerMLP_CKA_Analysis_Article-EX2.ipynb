{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSzgroG_Suq"
   },
   "source": [
    "# Study of Image classification with modern MLP Mixer model and CKA\n",
    "\n",
    "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
    "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U087DdDw_Suy"
   },
   "source": [
    "# Setup for the MLP-Mixer Architecture\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AnTyoluw_Suz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import pickle\n",
    "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
    "from CKA_Google import *\n",
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 : Explore the importance of different widths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DAOrIHu_Su2"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1iMiVS7o_Su3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224 = 50176\n",
      "Patch size: 32 X 32 = 1024 \n",
      "Patches per image: 49\n",
      "Elements per patch (3 channels): 3072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2022-03-23'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 512 \n",
    "num_epochs = 50\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Selected Architecture: B/32\n",
    "\n",
    "image_size = 224  # We'll resize input images to this size. Square\n",
    "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
    "embedding_dim = [256,384,512,640]  \n",
    "num_blocks = [12] # Fixed number of layers Dimension\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "now = datetime.datetime.today()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n",
    "str(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuu2OAS_Su0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Dataset for training \n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "#plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mpnEZH_Su5"
   },
   "source": [
    "## Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ra-bXojQ_Su6"
   },
   "outputs": [],
   "source": [
    "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data. \n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches. \n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks. ## (sequential_82)\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation) \n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpQFU8H__Su8"
   },
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9E1zD2BY_Su8"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        #Negative Log Likelihood = Categorical Cross Entropy\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping regularization callback. \n",
    "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
    "    #early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    #    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    #)\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history, accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxeE0cmM_Su9"
   },
   "source": [
    "## Use data augmentation\n",
    "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zh6hFPWX_Su-"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G77cUgiu_Su-"
   },
   "source": [
    "## Implement patch extraction as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYKNktXe_Su_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        #Extract the shape dimension in the position 0 = columns\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Without overlapping, stride horizontally and vertically\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
    "            rates=[1, 1, 1, 1],\n",
    "            #Patches contained in the images are considered, no zero padding\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        #shape[-1], number of colummns, as well as shape[0]\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Patches, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'patch_size' : self.patch_size ,\n",
    "            'num_patches' : self.num_patches\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yehcSS_Su_"
   },
   "source": [
    "## The MLP-Mixer model\n",
    "\n",
    "The MLP-Mixer is an architecture based exclusively on\n",
    "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "1. One applied independently to image patches, which mixes the per-location features.\n",
    "2. The other applied across patches (along channels), which mixes spatial information.\n",
    "\n",
    "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
    "instead of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwg4e2n_SvA"
   },
   "source": [
    "### Implement the MLP-Mixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dT6wVEki_SvA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp2 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super(MLPMixerLayer, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'num_patches' : num_patches,\n",
    "            'embedding_dim' : embedding_dim,\n",
    "            'dropout_rate' : dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUiufq92_SvA"
   },
   "source": [
    "## Build, train, and evaluate the MLP-Mixer model\n",
    "\n",
    "Note that training the model with the current settings on a V100 GPUs\n",
    "takes around 8 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report: Learning Curve\n",
    "def curves(history):\n",
    "    ymax1 = min(history[\"loss\"])\n",
    "    xmax1 = history[\"loss\"].index(ymax1)\n",
    "    ymax2 = min(history[\"val_loss\"])\n",
    "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
    "    plt.title(\"Cross Entropy Loss\")\n",
    "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
    "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Graph accuracy\n",
    "    ymax3 = max(history[\"acc\"])\n",
    "    xmax3 = history[\"acc\"].index(ymax3)\n",
    "    ymax4 = max(history[\"val_acc\"])\n",
    "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
    "    ymax5 = max(history[\"top5-acc\"])\n",
    "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
    "    ymax6 = max(history[\"val_top5-acc\"])\n",
    "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Classification accuracy')\n",
    "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Classification top5-acc')\n",
    "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.suptitle(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain activations + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers + Patches + One dense layer\n",
    "def Preprocessing(num_example):\n",
    "    augmented = data_augmentation(x_train[num_example])\n",
    "    b = Patches(patch_size, num_patches)(augmented)\n",
    "    a = layers.Dense(units=embedding_dim)(b)\n",
    "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
    "def Batch_Preprocessing(batch_size):\n",
    "    #Vector with the number of Sample of the Xtrain\n",
    "    a  = list(range(0,x_train.shape[0]))\n",
    "    b = random.sample(a,batch_size)\n",
    "    batch_regularization = list()\n",
    "    for i in range(0,batch_size):\n",
    "        inter_result = Preprocessing(b[i])\n",
    "        batch_regularization.append(inter_result)\n",
    "    return batch_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_out(result,layer_number,example):\n",
    "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "    ax1.imshow(x_train[example])\n",
    "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
    "    ax2.imshow(result[layer_number])\n",
    "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
    "    #The input is fixed to the beginning of the mlp blocks\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[0]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of layer's activation\n",
    "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
    "def Heatmap(result,type,sigma):\n",
    "    dim = len(result)\n",
    "    k = (dim - 1)\n",
    "    heatmap_CKA = np.zeros((dim,dim))\n",
    "    for i in range(0,dim):\n",
    "        tr = (dim - 1)\n",
    "        for j in range(0,dim):\n",
    "            if type == 'kernel':\n",
    "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
    "            elif type == 'linear':\n",
    "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
    "            else:\n",
    "                print('There is no such category, try again')\n",
    "                break\n",
    "\n",
    "            tr -= 1\n",
    "        k -= 1\n",
    "    #print('CKA' + type + 'calculated')\n",
    "    return heatmap_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of heatmaps (obsolet)\n",
    "def Prom_Mixer_Heatmaps(batch_result,type):\n",
    "    mat_heatmaps = list()\n",
    "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
    "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
    "        mat_heatmaps.append(heatmap_raw)\n",
    "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
    "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
    "    return prom_mixer_heatmap,mat_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 4\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i'%(i+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(3,3))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 : Explore the importance of different widths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A: Different widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create different mlpmixers according to an array of widths or depths\n",
    "def mlpmixer_iterations(num_patches,experiment,embedding_dim,num_blocks):\n",
    "    it_widths = len(embedding_dim)\n",
    "    it_blocks = len(num_blocks)\n",
    "    for j in range(it_widths):\n",
    "        for i in range(it_blocks):\n",
    "            mlpmixer_blocks = keras.Sequential(\n",
    "            [MLPMixerLayer(num_patches, embedding_dim[j], dropout_rate) for _ in range(num_blocks[i])] # creates the number of block without a \n",
    "            )\n",
    "            mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim[j]) # Returns the model\n",
    "            history,accuracy, top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "            #Saving Results\n",
    "            pwd = 'Results_Article/'+ str(experiment) +'/mlpmixer_'+ str(num_blocks[i]) + 'ly_' + str(embedding_dim[j]) + 'Dc'\n",
    "            mlpmixer_classifier.save(pwd)\n",
    "            np.save( pwd + '/history.npy',history.history)\n",
    "            with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "                pickle.dump(accuracy,file)\n",
    "            with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "                pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 25s 194ms/step - loss: 3.7234 - acc: 0.2418 - top5-acc: 0.7346 - val_loss: 1.6849 - val_acc: 0.3872 - val_top5-acc: 0.8752 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.5863 - acc: 0.4222 - top5-acc: 0.9003 - val_loss: 1.4300 - val_acc: 0.4874 - val_top5-acc: 0.9184 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.4588 - acc: 0.4729 - top5-acc: 0.9192 - val_loss: 1.3658 - val_acc: 0.5148 - val_top5-acc: 0.9324 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.3656 - acc: 0.5084 - top5-acc: 0.9334 - val_loss: 1.2603 - val_acc: 0.5376 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.3025 - acc: 0.5341 - top5-acc: 0.9404 - val_loss: 1.1890 - val_acc: 0.5744 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 1.2556 - acc: 0.5481 - top5-acc: 0.9462 - val_loss: 1.1632 - val_acc: 0.5880 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.2215 - acc: 0.5628 - top5-acc: 0.9491 - val_loss: 1.1970 - val_acc: 0.5792 - val_top5-acc: 0.9520 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.1905 - acc: 0.5729 - top5-acc: 0.9532 - val_loss: 1.1352 - val_acc: 0.5888 - val_top5-acc: 0.9626 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.1596 - acc: 0.5850 - top5-acc: 0.9540 - val_loss: 1.0844 - val_acc: 0.6180 - val_top5-acc: 0.9620 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.1528 - acc: 0.5898 - top5-acc: 0.9563 - val_loss: 1.1099 - val_acc: 0.6026 - val_top5-acc: 0.9616 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.1101 - acc: 0.6053 - top5-acc: 0.9603 - val_loss: 1.0710 - val_acc: 0.6190 - val_top5-acc: 0.9638 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 1.1002 - acc: 0.6114 - top5-acc: 0.9597 - val_loss: 1.0314 - val_acc: 0.6358 - val_top5-acc: 0.9626 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.0867 - acc: 0.6132 - top5-acc: 0.9610 - val_loss: 0.9876 - val_acc: 0.6526 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.0583 - acc: 0.6242 - top5-acc: 0.9645 - val_loss: 0.9728 - val_acc: 0.6562 - val_top5-acc: 0.9704 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.0344 - acc: 0.6330 - top5-acc: 0.9657 - val_loss: 1.0218 - val_acc: 0.6430 - val_top5-acc: 0.9672 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.0252 - acc: 0.6365 - top5-acc: 0.9664 - val_loss: 0.9505 - val_acc: 0.6632 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.0153 - acc: 0.6398 - top5-acc: 0.9666 - val_loss: 0.9368 - val_acc: 0.6730 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 1.0027 - acc: 0.6439 - top5-acc: 0.9688 - val_loss: 0.9595 - val_acc: 0.6636 - val_top5-acc: 0.9714 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.9908 - acc: 0.6485 - top5-acc: 0.9690 - val_loss: 0.9539 - val_acc: 0.6646 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.9741 - acc: 0.6555 - top5-acc: 0.9712 - val_loss: 0.9098 - val_acc: 0.6790 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.9777 - acc: 0.6511 - top5-acc: 0.9697 - val_loss: 0.8847 - val_acc: 0.6872 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.9445 - acc: 0.6666 - top5-acc: 0.9707 - val_loss: 0.9034 - val_acc: 0.6890 - val_top5-acc: 0.9746 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.9272 - acc: 0.6736 - top5-acc: 0.9733 - val_loss: 0.8628 - val_acc: 0.6976 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.9109 - acc: 0.6788 - top5-acc: 0.9744 - val_loss: 0.8770 - val_acc: 0.6946 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 0.9158 - acc: 0.6774 - top5-acc: 0.9745 - val_loss: 0.8894 - val_acc: 0.6922 - val_top5-acc: 0.9762 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.9084 - acc: 0.6798 - top5-acc: 0.9749 - val_loss: 0.8481 - val_acc: 0.7058 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8889 - acc: 0.6846 - top5-acc: 0.9759 - val_loss: 0.8356 - val_acc: 0.7070 - val_top5-acc: 0.9816 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8893 - acc: 0.6846 - top5-acc: 0.9754 - val_loss: 0.8235 - val_acc: 0.7060 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8920 - acc: 0.6878 - top5-acc: 0.9762 - val_loss: 0.9016 - val_acc: 0.6874 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8671 - acc: 0.6932 - top5-acc: 0.9775 - val_loss: 0.8347 - val_acc: 0.7082 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8368 - acc: 0.7061 - top5-acc: 0.9789 - val_loss: 0.8336 - val_acc: 0.7088 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8437 - acc: 0.7016 - top5-acc: 0.9783 - val_loss: 0.8044 - val_acc: 0.7178 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8284 - acc: 0.7077 - top5-acc: 0.9788 - val_loss: 0.8105 - val_acc: 0.7220 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8231 - acc: 0.7102 - top5-acc: 0.9803 - val_loss: 0.8069 - val_acc: 0.7190 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 16s 184ms/step - loss: 0.8128 - acc: 0.7132 - top5-acc: 0.9802 - val_loss: 0.8127 - val_acc: 0.7196 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8235 - acc: 0.7091 - top5-acc: 0.9794 - val_loss: 0.8766 - val_acc: 0.7058 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8097 - acc: 0.7157 - top5-acc: 0.9808 - val_loss: 0.7751 - val_acc: 0.7284 - val_top5-acc: 0.9858 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.7984 - acc: 0.7180 - top5-acc: 0.9812 - val_loss: 0.8095 - val_acc: 0.7216 - val_top5-acc: 0.9826 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.8036 - acc: 0.7157 - top5-acc: 0.9806 - val_loss: 0.8227 - val_acc: 0.7168 - val_top5-acc: 0.9858 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 0.7961 - acc: 0.7183 - top5-acc: 0.9807 - val_loss: 0.8160 - val_acc: 0.7228 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 0.7979 - acc: 0.7192 - top5-acc: 0.9805 - val_loss: 0.8270 - val_acc: 0.7188 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.7781 - acc: 0.7274 - top5-acc: 0.9818 - val_loss: 0.7990 - val_acc: 0.7224 - val_top5-acc: 0.9834 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 16s 183ms/step - loss: 0.7001 - acc: 0.7510 - top5-acc: 0.9859 - val_loss: 0.7204 - val_acc: 0.7500 - val_top5-acc: 0.9866 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 16s 181ms/step - loss: 0.6713 - acc: 0.7634 - top5-acc: 0.9871 - val_loss: 0.7163 - val_acc: 0.7536 - val_top5-acc: 0.9870 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 0.6654 - acc: 0.7637 - top5-acc: 0.9878 - val_loss: 0.7324 - val_acc: 0.7544 - val_top5-acc: 0.9846 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 0.6650 - acc: 0.7661 - top5-acc: 0.9875 - val_loss: 0.7100 - val_acc: 0.7584 - val_top5-acc: 0.9856 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 16s 181ms/step - loss: 0.6588 - acc: 0.7663 - top5-acc: 0.9883 - val_loss: 0.7270 - val_acc: 0.7492 - val_top5-acc: 0.9866 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 0.6571 - acc: 0.7659 - top5-acc: 0.9879 - val_loss: 0.7224 - val_acc: 0.7510 - val_top5-acc: 0.9870 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 0.6614 - acc: 0.7646 - top5-acc: 0.9885 - val_loss: 0.7213 - val_acc: 0.7542 - val_top5-acc: 0.9858 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 16s 182ms/step - loss: 0.6531 - acc: 0.7699 - top5-acc: 0.9881 - val_loss: 0.7296 - val_acc: 0.7504 - val_top5-acc: 0.9866 - lr: 0.0025\n",
      "313/313 [==============================] - 16s 50ms/step - loss: 0.7444 - acc: 0.7508 - top5-acc: 0.9829\n",
      "Test accuracy: 75.08%\n",
      "Test top 5 accuracy: 98.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn, layer_normalization_1_layer_call_and_return_conditional_losses, layer_normalization_2_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/2A/mlpmixer_12ly_256Dc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/2A/mlpmixer_12ly_256Dc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 245ms/step - loss: 4.6180 - acc: 0.2330 - top5-acc: 0.7296 - val_loss: 1.7044 - val_acc: 0.4016 - val_top5-acc: 0.8834 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.6088 - acc: 0.4162 - top5-acc: 0.8964 - val_loss: 1.4618 - val_acc: 0.4834 - val_top5-acc: 0.9198 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.4576 - acc: 0.4724 - top5-acc: 0.9220 - val_loss: 1.3313 - val_acc: 0.5290 - val_top5-acc: 0.9406 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.3850 - acc: 0.5011 - top5-acc: 0.9303 - val_loss: 1.2552 - val_acc: 0.5588 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.3146 - acc: 0.5279 - top5-acc: 0.9406 - val_loss: 1.2158 - val_acc: 0.5640 - val_top5-acc: 0.9538 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2681 - acc: 0.5450 - top5-acc: 0.9455 - val_loss: 1.1533 - val_acc: 0.5924 - val_top5-acc: 0.9570 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2371 - acc: 0.5585 - top5-acc: 0.9480 - val_loss: 1.1263 - val_acc: 0.6052 - val_top5-acc: 0.9608 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1979 - acc: 0.5725 - top5-acc: 0.9504 - val_loss: 1.1330 - val_acc: 0.6008 - val_top5-acc: 0.9614 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.1671 - acc: 0.5847 - top5-acc: 0.9549 - val_loss: 1.0898 - val_acc: 0.6122 - val_top5-acc: 0.9642 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1355 - acc: 0.5963 - top5-acc: 0.9562 - val_loss: 1.0805 - val_acc: 0.6200 - val_top5-acc: 0.9686 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.1204 - acc: 0.6033 - top5-acc: 0.9575 - val_loss: 1.0443 - val_acc: 0.6222 - val_top5-acc: 0.9702 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0946 - acc: 0.6096 - top5-acc: 0.9606 - val_loss: 1.0596 - val_acc: 0.6228 - val_top5-acc: 0.9638 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0773 - acc: 0.6180 - top5-acc: 0.9627 - val_loss: 1.0020 - val_acc: 0.6466 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0555 - acc: 0.6224 - top5-acc: 0.9637 - val_loss: 0.9976 - val_acc: 0.6426 - val_top5-acc: 0.9690 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0281 - acc: 0.6352 - top5-acc: 0.9654 - val_loss: 0.9809 - val_acc: 0.6538 - val_top5-acc: 0.9706 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0151 - acc: 0.6438 - top5-acc: 0.9668 - val_loss: 0.9622 - val_acc: 0.6578 - val_top5-acc: 0.9714 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.9938 - acc: 0.6488 - top5-acc: 0.9680 - val_loss: 0.9218 - val_acc: 0.6690 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.9772 - acc: 0.6535 - top5-acc: 0.9697 - val_loss: 0.9431 - val_acc: 0.6718 - val_top5-acc: 0.9730 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.9861 - acc: 0.6517 - top5-acc: 0.9687 - val_loss: 0.9491 - val_acc: 0.6686 - val_top5-acc: 0.9740 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.9437 - acc: 0.6644 - top5-acc: 0.9719 - val_loss: 0.9029 - val_acc: 0.6850 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.9341 - acc: 0.6690 - top5-acc: 0.9728 - val_loss: 0.9238 - val_acc: 0.6814 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9206 - acc: 0.6738 - top5-acc: 0.9728 - val_loss: 0.8972 - val_acc: 0.6852 - val_top5-acc: 0.9742 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9146 - acc: 0.6772 - top5-acc: 0.9746 - val_loss: 0.8993 - val_acc: 0.6870 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.9129 - acc: 0.6814 - top5-acc: 0.9736 - val_loss: 0.8658 - val_acc: 0.7034 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8988 - acc: 0.6837 - top5-acc: 0.9752 - val_loss: 0.8467 - val_acc: 0.7070 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8854 - acc: 0.6893 - top5-acc: 0.9760 - val_loss: 0.8483 - val_acc: 0.7074 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8950 - acc: 0.6851 - top5-acc: 0.9753 - val_loss: 0.8849 - val_acc: 0.6922 - val_top5-acc: 0.9768 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8634 - acc: 0.6973 - top5-acc: 0.9765 - val_loss: 0.8283 - val_acc: 0.7172 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8566 - acc: 0.6970 - top5-acc: 0.9779 - val_loss: 0.8398 - val_acc: 0.7138 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8338 - acc: 0.7058 - top5-acc: 0.9789 - val_loss: 0.8478 - val_acc: 0.7066 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8495 - acc: 0.7019 - top5-acc: 0.9786 - val_loss: 0.8277 - val_acc: 0.7172 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8424 - acc: 0.7046 - top5-acc: 0.9786 - val_loss: 0.8565 - val_acc: 0.7072 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8198 - acc: 0.7114 - top5-acc: 0.9785 - val_loss: 0.8023 - val_acc: 0.7180 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8211 - acc: 0.7102 - top5-acc: 0.9802 - val_loss: 0.8136 - val_acc: 0.7188 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8088 - acc: 0.7148 - top5-acc: 0.9804 - val_loss: 0.8518 - val_acc: 0.7076 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.8015 - acc: 0.7185 - top5-acc: 0.9801 - val_loss: 0.8178 - val_acc: 0.7228 - val_top5-acc: 0.9796 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.7959 - acc: 0.7184 - top5-acc: 0.9810 - val_loss: 0.8570 - val_acc: 0.7124 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 0.7839 - acc: 0.7228 - top5-acc: 0.9825 - val_loss: 0.8184 - val_acc: 0.7212 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.6958 - acc: 0.7528 - top5-acc: 0.9856 - val_loss: 0.7496 - val_acc: 0.7438 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.6724 - acc: 0.7604 - top5-acc: 0.9872 - val_loss: 0.7538 - val_acc: 0.7434 - val_top5-acc: 0.9848 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.6608 - acc: 0.7673 - top5-acc: 0.9878 - val_loss: 0.7449 - val_acc: 0.7540 - val_top5-acc: 0.9856 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.6594 - acc: 0.7666 - top5-acc: 0.9883 - val_loss: 0.7447 - val_acc: 0.7436 - val_top5-acc: 0.9838 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.6466 - acc: 0.7708 - top5-acc: 0.9882 - val_loss: 0.7351 - val_acc: 0.7518 - val_top5-acc: 0.9842 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 21s 233ms/step - loss: 0.6425 - acc: 0.7717 - top5-acc: 0.9882 - val_loss: 0.7315 - val_acc: 0.7518 - val_top5-acc: 0.9858 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.6448 - acc: 0.7703 - top5-acc: 0.9886 - val_loss: 0.7440 - val_acc: 0.7490 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.6394 - acc: 0.7713 - top5-acc: 0.9886 - val_loss: 0.7422 - val_acc: 0.7590 - val_top5-acc: 0.9842 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.6467 - acc: 0.7701 - top5-acc: 0.9886 - val_loss: 0.7471 - val_acc: 0.7458 - val_top5-acc: 0.9850 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.6377 - acc: 0.7736 - top5-acc: 0.9888 - val_loss: 0.7433 - val_acc: 0.7480 - val_top5-acc: 0.9840 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.6460 - acc: 0.7709 - top5-acc: 0.9882 - val_loss: 0.7411 - val_acc: 0.7492 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.5744 - acc: 0.7961 - top5-acc: 0.9907 - val_loss: 0.7165 - val_acc: 0.7644 - val_top5-acc: 0.9846 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.7381 - acc: 0.7573 - top5-acc: 0.9854\n",
      "Test accuracy: 75.73%\n",
      "Test top 5 accuracy: 98.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_12_layer_call_fn, layer_normalization_12_layer_call_and_return_conditional_losses, layer_normalization_13_layer_call_fn, layer_normalization_13_layer_call_and_return_conditional_losses, layer_normalization_14_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/2A/mlpmixer_12ly_384Dc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/2A/mlpmixer_12ly_384Dc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 35s 304ms/step - loss: 4.3594 - acc: 0.2653 - top5-acc: 0.7480 - val_loss: 1.6683 - val_acc: 0.3988 - val_top5-acc: 0.8914 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 1.5429 - acc: 0.4458 - top5-acc: 0.9050 - val_loss: 1.4154 - val_acc: 0.5006 - val_top5-acc: 0.9310 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.4063 - acc: 0.4941 - top5-acc: 0.9267 - val_loss: 1.2971 - val_acc: 0.5348 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.3196 - acc: 0.5284 - top5-acc: 0.9372 - val_loss: 1.2448 - val_acc: 0.5566 - val_top5-acc: 0.9558 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 1.2662 - acc: 0.5442 - top5-acc: 0.9441 - val_loss: 1.2595 - val_acc: 0.5682 - val_top5-acc: 0.9500 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.2197 - acc: 0.5644 - top5-acc: 0.9478 - val_loss: 1.1981 - val_acc: 0.5718 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 1.1845 - acc: 0.5780 - top5-acc: 0.9511 - val_loss: 1.1470 - val_acc: 0.5966 - val_top5-acc: 0.9626 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 1.1481 - acc: 0.5909 - top5-acc: 0.9556 - val_loss: 1.1227 - val_acc: 0.6044 - val_top5-acc: 0.9604 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 1.1224 - acc: 0.6015 - top5-acc: 0.9586 - val_loss: 1.0568 - val_acc: 0.6326 - val_top5-acc: 0.9664 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 1.0933 - acc: 0.6130 - top5-acc: 0.9596 - val_loss: 1.0412 - val_acc: 0.6294 - val_top5-acc: 0.9706 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.0707 - acc: 0.6210 - top5-acc: 0.9632 - val_loss: 1.0081 - val_acc: 0.6426 - val_top5-acc: 0.9684 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.0551 - acc: 0.6264 - top5-acc: 0.9628 - val_loss: 1.0053 - val_acc: 0.6486 - val_top5-acc: 0.9686 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.0516 - acc: 0.6270 - top5-acc: 0.9645 - val_loss: 0.9624 - val_acc: 0.6566 - val_top5-acc: 0.9714 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.0144 - acc: 0.6410 - top5-acc: 0.9675 - val_loss: 0.9408 - val_acc: 0.6706 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 0.9830 - acc: 0.6534 - top5-acc: 0.9684 - val_loss: 0.9554 - val_acc: 0.6682 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 0.9741 - acc: 0.6584 - top5-acc: 0.9689 - val_loss: 0.9401 - val_acc: 0.6722 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.9624 - acc: 0.6609 - top5-acc: 0.9694 - val_loss: 0.9647 - val_acc: 0.6700 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.9407 - acc: 0.6671 - top5-acc: 0.9713 - val_loss: 0.9542 - val_acc: 0.6768 - val_top5-acc: 0.9742 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 0.9343 - acc: 0.6715 - top5-acc: 0.9719 - val_loss: 0.9160 - val_acc: 0.6814 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.9156 - acc: 0.6756 - top5-acc: 0.9738 - val_loss: 0.8619 - val_acc: 0.6986 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 0.9013 - acc: 0.6848 - top5-acc: 0.9750 - val_loss: 0.8992 - val_acc: 0.6910 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8918 - acc: 0.6852 - top5-acc: 0.9763 - val_loss: 0.8383 - val_acc: 0.7048 - val_top5-acc: 0.9758 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8864 - acc: 0.6881 - top5-acc: 0.9753 - val_loss: 0.8760 - val_acc: 0.6954 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 10.4652 - acc: 0.3753 - top5-acc: 0.7952 - val_loss: 1.8018 - val_acc: 0.4032 - val_top5-acc: 0.9092 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.4666 - acc: 0.4862 - top5-acc: 0.9216 - val_loss: 1.2123 - val_acc: 0.5758 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.2408 - acc: 0.5620 - top5-acc: 0.9462 - val_loss: 1.0697 - val_acc: 0.6220 - val_top5-acc: 0.9658 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.1380 - acc: 0.5939 - top5-acc: 0.9572 - val_loss: 1.0149 - val_acc: 0.6412 - val_top5-acc: 0.9674 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 1.0601 - acc: 0.6228 - top5-acc: 0.9633 - val_loss: 0.9732 - val_acc: 0.6576 - val_top5-acc: 0.9702 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.0263 - acc: 0.6386 - top5-acc: 0.9660 - val_loss: 0.9497 - val_acc: 0.6646 - val_top5-acc: 0.9724 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 1.0045 - acc: 0.6447 - top5-acc: 0.9681 - val_loss: 0.9279 - val_acc: 0.6712 - val_top5-acc: 0.9734 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 26s 293ms/step - loss: 0.9770 - acc: 0.6553 - top5-acc: 0.9689 - val_loss: 0.9244 - val_acc: 0.6712 - val_top5-acc: 0.9740 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.9562 - acc: 0.6632 - top5-acc: 0.9703 - val_loss: 0.9188 - val_acc: 0.6766 - val_top5-acc: 0.9752 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.9278 - acc: 0.6715 - top5-acc: 0.9723 - val_loss: 0.8887 - val_acc: 0.6894 - val_top5-acc: 0.9760 - lr: 0.0012\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.9101 - acc: 0.6798 - top5-acc: 0.9740 - val_loss: 0.8765 - val_acc: 0.6912 - val_top5-acc: 0.9778 - lr: 0.0012\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8991 - acc: 0.6817 - top5-acc: 0.9752 - val_loss: 0.8719 - val_acc: 0.6928 - val_top5-acc: 0.9784 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8897 - acc: 0.6868 - top5-acc: 0.9750 - val_loss: 0.8599 - val_acc: 0.6966 - val_top5-acc: 0.9778 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8804 - acc: 0.6902 - top5-acc: 0.9762 - val_loss: 0.8447 - val_acc: 0.7022 - val_top5-acc: 0.9778 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8648 - acc: 0.6954 - top5-acc: 0.9764 - val_loss: 0.8468 - val_acc: 0.7048 - val_top5-acc: 0.9800 - lr: 6.2500e-04\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8592 - acc: 0.6974 - top5-acc: 0.9778 - val_loss: 0.8384 - val_acc: 0.7052 - val_top5-acc: 0.9800 - lr: 6.2500e-04\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 26s 293ms/step - loss: 0.8543 - acc: 0.7003 - top5-acc: 0.9782 - val_loss: 0.8330 - val_acc: 0.7086 - val_top5-acc: 0.9794 - lr: 6.2500e-04\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 26s 293ms/step - loss: 0.8484 - acc: 0.7012 - top5-acc: 0.9779 - val_loss: 0.8300 - val_acc: 0.7080 - val_top5-acc: 0.9806 - lr: 6.2500e-04\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8402 - acc: 0.7046 - top5-acc: 0.9786 - val_loss: 0.8231 - val_acc: 0.7106 - val_top5-acc: 0.9804 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8385 - acc: 0.7054 - top5-acc: 0.9789 - val_loss: 0.8167 - val_acc: 0.7128 - val_top5-acc: 0.9796 - lr: 6.2500e-04\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8303 - acc: 0.7088 - top5-acc: 0.9787 - val_loss: 0.8187 - val_acc: 0.7148 - val_top5-acc: 0.9798 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8291 - acc: 0.7087 - top5-acc: 0.9784 - val_loss: 0.8257 - val_acc: 0.7112 - val_top5-acc: 0.9796 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 0.8236 - acc: 0.7122 - top5-acc: 0.9799 - val_loss: 0.8214 - val_acc: 0.7144 - val_top5-acc: 0.9800 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 0.8141 - acc: 0.7147 - top5-acc: 0.9794 - val_loss: 0.8221 - val_acc: 0.7126 - val_top5-acc: 0.9806 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 26s 291ms/step - loss: 0.8147 - acc: 0.7133 - top5-acc: 0.9802 - val_loss: 0.8042 - val_acc: 0.7182 - val_top5-acc: 0.9822 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8079 - acc: 0.7177 - top5-acc: 0.9796 - val_loss: 0.7994 - val_acc: 0.7188 - val_top5-acc: 0.9820 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.8043 - acc: 0.7162 - top5-acc: 0.9805 - val_loss: 0.7917 - val_acc: 0.7178 - val_top5-acc: 0.9838 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.8250 - acc: 0.7115 - top5-acc: 0.9788\n",
      "Test accuracy: 71.15%\n",
      "Test top 5 accuracy: 97.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_24_layer_call_fn, layer_normalization_24_layer_call_and_return_conditional_losses, layer_normalization_25_layer_call_fn, layer_normalization_25_layer_call_and_return_conditional_losses, layer_normalization_26_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/2A/mlpmixer_12ly_512Dc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/2A/mlpmixer_12ly_512Dc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 39s 353ms/step - loss: 5.2224 - acc: 0.2304 - top5-acc: 0.7273 - val_loss: 1.7511 - val_acc: 0.3632 - val_top5-acc: 0.8704 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.6214 - acc: 0.4148 - top5-acc: 0.8914 - val_loss: 1.4406 - val_acc: 0.4830 - val_top5-acc: 0.9304 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 1.4463 - acc: 0.4795 - top5-acc: 0.9216 - val_loss: 1.4384 - val_acc: 0.4988 - val_top5-acc: 0.9380 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.3630 - acc: 0.5088 - top5-acc: 0.9329 - val_loss: 1.2872 - val_acc: 0.5416 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.2881 - acc: 0.5385 - top5-acc: 0.9419 - val_loss: 1.2086 - val_acc: 0.5762 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.2325 - acc: 0.5600 - top5-acc: 0.9481 - val_loss: 1.1261 - val_acc: 0.6008 - val_top5-acc: 0.9630 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.1995 - acc: 0.5732 - top5-acc: 0.9512 - val_loss: 1.1165 - val_acc: 0.6074 - val_top5-acc: 0.9632 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.1512 - acc: 0.5913 - top5-acc: 0.9555 - val_loss: 1.1514 - val_acc: 0.6066 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.1420 - acc: 0.5935 - top5-acc: 0.9559 - val_loss: 1.1075 - val_acc: 0.6134 - val_top5-acc: 0.9618 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.1092 - acc: 0.6045 - top5-acc: 0.9586 - val_loss: 1.0554 - val_acc: 0.6292 - val_top5-acc: 0.9662 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 30s 342ms/step - loss: 1.0781 - acc: 0.6184 - top5-acc: 0.9612 - val_loss: 1.0341 - val_acc: 0.6444 - val_top5-acc: 0.9668 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.0651 - acc: 0.6228 - top5-acc: 0.9643 - val_loss: 1.0753 - val_acc: 0.6298 - val_top5-acc: 0.9642 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 1.0541 - acc: 0.6259 - top5-acc: 0.9638 - val_loss: 0.9887 - val_acc: 0.6496 - val_top5-acc: 0.9730 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 1.0300 - acc: 0.6352 - top5-acc: 0.9662 - val_loss: 1.0035 - val_acc: 0.6520 - val_top5-acc: 0.9688 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 1.0023 - acc: 0.6437 - top5-acc: 0.9691 - val_loss: 0.9629 - val_acc: 0.6602 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.9693 - acc: 0.6584 - top5-acc: 0.9702 - val_loss: 0.9731 - val_acc: 0.6652 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 0.9570 - acc: 0.6608 - top5-acc: 0.9714 - val_loss: 0.9402 - val_acc: 0.6688 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.9540 - acc: 0.6636 - top5-acc: 0.9712 - val_loss: 0.9458 - val_acc: 0.6782 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.9242 - acc: 0.6731 - top5-acc: 0.9736 - val_loss: 0.9170 - val_acc: 0.6892 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.9345 - acc: 0.6672 - top5-acc: 0.9731 - val_loss: 0.8728 - val_acc: 0.6922 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 0.8948 - acc: 0.6830 - top5-acc: 0.9751 - val_loss: 0.8985 - val_acc: 0.6900 - val_top5-acc: 0.9740 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 0.8957 - acc: 0.6840 - top5-acc: 0.9762 - val_loss: 0.8599 - val_acc: 0.7022 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.8862 - acc: 0.6868 - top5-acc: 0.9759 - val_loss: 0.8782 - val_acc: 0.6946 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 30s 339ms/step - loss: 0.8762 - acc: 0.6895 - top5-acc: 0.9778 - val_loss: 0.9059 - val_acc: 0.6918 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 30s 339ms/step - loss: 0.8963 - acc: 0.6842 - top5-acc: 0.9756 - val_loss: 0.8937 - val_acc: 0.7028 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.8500 - acc: 0.6994 - top5-acc: 0.9780 - val_loss: 0.8736 - val_acc: 0.7030 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.8344 - acc: 0.7057 - top5-acc: 0.9788 - val_loss: 0.9300 - val_acc: 0.6932 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 30s 339ms/step - loss: 0.7514 - acc: 0.7331 - top5-acc: 0.9833 - val_loss: 0.7922 - val_acc: 0.7296 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.7040 - acc: 0.7525 - top5-acc: 0.9858 - val_loss: 0.7864 - val_acc: 0.7304 - val_top5-acc: 0.9812 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.6907 - acc: 0.7569 - top5-acc: 0.9859 - val_loss: 0.7664 - val_acc: 0.7414 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.6915 - acc: 0.7554 - top5-acc: 0.9863 - val_loss: 0.7765 - val_acc: 0.7400 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 0.6823 - acc: 0.7587 - top5-acc: 0.9874 - val_loss: 0.8336 - val_acc: 0.7210 - val_top5-acc: 0.9820 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.6796 - acc: 0.7601 - top5-acc: 0.9866 - val_loss: 0.8243 - val_acc: 0.7254 - val_top5-acc: 0.9824 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.6694 - acc: 0.7626 - top5-acc: 0.9880 - val_loss: 0.8037 - val_acc: 0.7304 - val_top5-acc: 0.9826 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.6734 - acc: 0.7598 - top5-acc: 0.9877 - val_loss: 0.7811 - val_acc: 0.7354 - val_top5-acc: 0.9822 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 30s 342ms/step - loss: 0.6048 - acc: 0.7848 - top5-acc: 0.9896 - val_loss: 0.7375 - val_acc: 0.7540 - val_top5-acc: 0.9856 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 30s 342ms/step - loss: 0.5742 - acc: 0.7972 - top5-acc: 0.9915 - val_loss: 0.7422 - val_acc: 0.7628 - val_top5-acc: 0.9848 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 30s 342ms/step - loss: 0.5700 - acc: 0.7987 - top5-acc: 0.9907 - val_loss: 0.7501 - val_acc: 0.7566 - val_top5-acc: 0.9858 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 30s 342ms/step - loss: 0.5653 - acc: 0.7981 - top5-acc: 0.9916 - val_loss: 0.7589 - val_acc: 0.7552 - val_top5-acc: 0.9850 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 30s 342ms/step - loss: 0.5619 - acc: 0.8019 - top5-acc: 0.9910 - val_loss: 0.7550 - val_acc: 0.7554 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.5562 - acc: 0.8020 - top5-acc: 0.9919 - val_loss: 0.7750 - val_acc: 0.7424 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.5113 - acc: 0.8172 - top5-acc: 0.9929 - val_loss: 0.7484 - val_acc: 0.7564 - val_top5-acc: 0.9856 - lr: 6.2500e-04\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 30s 342ms/step - loss: 0.4987 - acc: 0.8237 - top5-acc: 0.9936 - val_loss: 0.7437 - val_acc: 0.7576 - val_top5-acc: 0.9844 - lr: 6.2500e-04\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 30s 341ms/step - loss: 0.4909 - acc: 0.8242 - top5-acc: 0.9945 - val_loss: 0.7543 - val_acc: 0.7588 - val_top5-acc: 0.9856 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 0.4870 - acc: 0.8257 - top5-acc: 0.9945 - val_loss: 0.7527 - val_acc: 0.7590 - val_top5-acc: 0.9856 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.4779 - acc: 0.8318 - top5-acc: 0.9941 - val_loss: 0.7497 - val_acc: 0.7656 - val_top5-acc: 0.9858 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.4524 - acc: 0.8372 - top5-acc: 0.9951 - val_loss: 0.7498 - val_acc: 0.7624 - val_top5-acc: 0.9866 - lr: 3.1250e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 0.4494 - acc: 0.8386 - top5-acc: 0.9953 - val_loss: 0.7451 - val_acc: 0.7676 - val_top5-acc: 0.9844 - lr: 3.1250e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.4390 - acc: 0.8436 - top5-acc: 0.9952 - val_loss: 0.7402 - val_acc: 0.7652 - val_top5-acc: 0.9874 - lr: 3.1250e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 30s 341ms/step - loss: 0.4379 - acc: 0.8445 - top5-acc: 0.9955 - val_loss: 0.7426 - val_acc: 0.7660 - val_top5-acc: 0.9862 - lr: 3.1250e-04\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.7737 - acc: 0.7551 - top5-acc: 0.9848\n",
      "Test accuracy: 75.51%\n",
      "Test top 5 accuracy: 98.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_36_layer_call_fn, layer_normalization_36_layer_call_and_return_conditional_losses, layer_normalization_37_layer_call_fn, layer_normalization_37_layer_call_and_return_conditional_losses, layer_normalization_38_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/2A/mlpmixer_12ly_640Dc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/2A/mlpmixer_12ly_640Dc\\assets\n"
     ]
    }
   ],
   "source": [
    "mlpmixer_iterations(num_patches,'2A', embedding_dim,num_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Average of layer's activation\n",
    "sigma = 1\n",
    "type = 'kernel'\n",
    "num_blocks = 12\n",
    "embedding_dimension = embedding_dim\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000204ABD07CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000204ABD07CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000204ABD07670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000204ABD07670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-2._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.layer-3._random_generator._generator._state_var\n"
     ]
    }
   ],
   "source": [
    "for item in embedding_dimension:\n",
    "    embedding_dim = item\n",
    "    batch_prepro = Batch_Preprocessing(batch_size)\n",
    "    path = 'Results_Article/2A/mlpmixer_12ly_'+ str(item) + 'Dc'\n",
    "    #Call the folder\n",
    "    tested_model = tf.keras.models.load_model(path)\n",
    "    A1_ave_mixer_activations = Prom_Mixer_Activations_Blocks(tested_model,batch_prepro)\n",
    "    A1_global_heatmap = Heatmap(A1_ave_mixer_activations,type,sigma)\n",
    "    with open(path + '/heatmap_'+ type + '_Sg'+ str(sigma) +'_12ly_'+ str(item) +'Dc.pkl','wb') as file:\n",
    "                pickle.dump(A1_global_heatmap,file)\n",
    "    with open(path + '/activations_'+ type + '_Sg'+ str(sigma) +'_12ly_'+ str(item) +'Dc.pkl','wb') as file:\n",
    "                pickle.dump(A1_ave_mixer_activations,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 2: VERIFICATIONS (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change paths in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'Results_Article/2A/1/mlpmixer_12ly_640Dc_2022-02-26'\n",
    "path = 'Results_Article/2A/mlpmixer_12ly_256Dc'\n",
    "#Call the file\n",
    "#tested_history=np.load( path + '/history_2022-02-26.npy',allow_pickle='TRUE').item()\n",
    "tested_history=np.load( path + '/history.npy',allow_pickle='TRUE').item()\n",
    "with open(path + '/accuracy.pkl','rb') as file:\n",
    "    tested_accuracy = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "curves(tested_history)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_image_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
