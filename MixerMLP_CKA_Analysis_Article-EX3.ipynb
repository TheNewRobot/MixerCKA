{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zSzgroG_Suq"
      },
      "source": [
        "# Study of Image classification with modern MLP Mixer model and CKA\n",
        "\n",
        "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
        "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U087DdDw_Suy"
      },
      "source": [
        "# Setup for the MLP-Mixer Architecture\n",
        "\n",
        "################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnTyoluw_Suz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt \n",
        "from scipy.stats import norm\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "import datetime\n",
        "import pickle\n",
        "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
        "from CKA_Google import *\n",
        "import seaborn as sns \n",
        "import random\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 3 : Across Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DAOrIHu_Su2"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iMiVS7o_Su3"
      },
      "outputs": [],
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 512 \n",
        "num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "learning_rate = 0.005\n",
        "\n",
        "## Selected Architecture: B/32\n",
        "\n",
        "image_size = 224  # We'll resize input images to this size. Square\n",
        "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
        "embedding_dim = 384  # Fixed Embedding Dimension\n",
        "num_blocks = 12\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "date = now.strftime(\"%Y-%m-%d_%H-%M\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUuu2OAS_Su0"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Dataset for training \n",
        "\n",
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
        "#plt.imshow(x_train[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08mpnEZH_Su5"
      },
      "source": [
        "## Build a classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra-bXojQ_Su6"
      },
      "outputs": [],
      "source": [
        "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data. \n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches. \n",
        "    patches = Patches(patch_size, num_patches)(augmented)\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
        "    x = layers.Dense(units=embedding_dim)(patches)\n",
        "    if positional_encoding:\n",
        "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "        position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=embedding_dim\n",
        "        )(positions)\n",
        "        x = x + position_embedding\n",
        "    # Process x using the module blocks. ## (sequential_82)\n",
        "    x = blocks(x)\n",
        "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
        "    representation = layers.GlobalAveragePooling1D()(x)\n",
        "    # Apply dropout.\n",
        "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
        "    # Compute logits outputs.\n",
        "    logits = layers.Dense(num_classes)(representation) \n",
        "    # Create the Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpQFU8H__Su8"
      },
      "source": [
        "## Define an experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E1zD2BY_Su8"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
        "    )\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        #Negative Log Likelihood = Categorical Cross Entropy\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5\n",
        "    )\n",
        "    # Create an early stopping regularization callback. \n",
        "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history, accuracy, top_5_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxeE0cmM_Su9"
      },
      "source": [
        "## Use data augmentation\n",
        "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh6hFPWX_Su-"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G77cUgiu_Su-"
      },
      "source": [
        "## Implement patch extraction as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYKNktXe_Su_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, num_patches):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def call(self, images):\n",
        "        #Extract the shape dimension in the position 0 = columns\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            #Without overlapping, stride horizontally and vertically\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
        "            rates=[1, 1, 1, 1],\n",
        "            #Patches contained in the images are considered, no zero padding\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        #shape[-1], number of colummns, as well as shape[0]\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Patches, self).get_config().copy()\n",
        "        config.update ({\n",
        "            'patch_size' : self.patch_size ,\n",
        "            'num_patches' : self.num_patches\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7yehcSS_Su_"
      },
      "source": [
        "## The MLP-Mixer model\n",
        "\n",
        "The MLP-Mixer is an architecture based exclusively on\n",
        "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
        "\n",
        "1. One applied independently to image patches, which mixes the per-location features.\n",
        "2. The other applied across patches (along channels), which mixes spatial information.\n",
        "\n",
        "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
        "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
        "instead of batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvwg4e2n_SvA"
      },
      "source": [
        "### Implement the MLP-Mixer module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT6wVEki_SvA"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MLPMixerLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.mlp1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=num_patches),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.mlp2 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize(inputs)\n",
        "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "        x_channels = tf.linalg.matrix_transpose(x)\n",
        "        # Apply mlp1 on each channel independently.\n",
        "        mlp1_outputs = self.mlp1(x_channels)\n",
        "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
        "        # Add skip connection.\n",
        "        x = mlp1_outputs + inputs\n",
        "        # Apply layer normalization.\n",
        "        x_patches = self.normalize(x)\n",
        "        # Apply mlp2 on each patch independtenly.\n",
        "        mlp2_outputs = self.mlp2(x_patches)\n",
        "        # Add skip connection.\n",
        "        x = x + mlp2_outputs\n",
        "        return x\n",
        "\n",
        "    def get_config(self): \n",
        "        config = super(MLPMixerLayer, self).get_config().copy()\n",
        "        config.update ({\n",
        "            'num_patches' : num_patches,\n",
        "            'embedding_dim' : embedding_dim,\n",
        "            'dropout_rate' : dropout_rate,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUiufq92_SvA"
      },
      "source": [
        "## Build, train, and evaluate the MLP-Mixer model\n",
        "\n",
        "Note that training the model with the current settings on a V100 GPUs\n",
        "takes around 8 seconds per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Report: Learning Curve\n",
        "def curves(history):\n",
        "    ymax1 = min(history[\"loss\"])\n",
        "    xmax1 = history[\"loss\"].index(ymax1)\n",
        "    ymax2 = min(history[\"val_loss\"])\n",
        "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
        "    plt.title(\"Cross Entropy Loss\")\n",
        "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
        "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
        "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
        "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
        "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
        "    plt.xlim([0,num_epochs])\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # Graph accuracy\n",
        "    ymax3 = max(history[\"acc\"])\n",
        "    xmax3 = history[\"acc\"].index(ymax3)\n",
        "    ymax4 = max(history[\"val_acc\"])\n",
        "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
        "    ymax5 = max(history[\"top5-acc\"])\n",
        "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
        "    ymax6 = max(history[\"val_top5-acc\"])\n",
        "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.title('Classification accuracy')\n",
        "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
        "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
        "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
        "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
        "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
        "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.title('Classification top5-acc')\n",
        "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
        "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
        "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
        "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
        "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
        "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
        "    plt.xlim([0,num_epochs])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.suptitle(\"Learning Curves\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtain activations + Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing Layers + Patches + One dense layer\n",
        "def Preprocessing(num_example):\n",
        "    augmented = data_augmentation(x_train[num_example])\n",
        "    b = Patches(patch_size, num_patches)(augmented)\n",
        "    a = layers.Dense(units=embedding_dim)(b)\n",
        "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
        "    return inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
        "def Batch_Preprocessing(batch_size):\n",
        "    #Vector with the number of Sample of the Xtrain\n",
        "    a  = list(range(0,x_train.shape[0]))\n",
        "    b = random.sample(a,batch_size)\n",
        "    batch_regularization = list()\n",
        "    for i in range(0,batch_size):\n",
        "        inter_result = Preprocessing(b[i])\n",
        "        batch_regularization.append(inter_result)\n",
        "    return batch_regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_out(result,layer_number,example):\n",
        "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
        "    ax1.imshow(x_train[example])\n",
        "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
        "    ax2.imshow(result[layer_number])\n",
        "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
        "    #The input is fixed to the beginning of the mlp blocks\n",
        "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
        "    #This reshape is necessary for the input of the model\n",
        "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
        "    #Inference\n",
        "    intermediate_prediction =intermediate_model.predict(example)\n",
        "    #This reshape is standardize the output\n",
        "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
        "    return layactivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Computes the outputs of each MLP-mixer Layer\n",
        "def Mixer_Activations(model, example):\n",
        "    total_activations = list()\n",
        "    for i in range(num_blocks):\n",
        "        model_input = model.layers[4].layers[0]\n",
        "        model_output = model.layers[4].layers[i]\n",
        "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
        "        total_activations.append(int_total_activations)\n",
        "    return  total_activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Average of layer's activation\n",
        "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
        "    sum = list()\n",
        "    for i in range(0,num_blocks):\n",
        "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
        "        sum.append(sum_raw)\n",
        "    for i in range(0,batch_size):\n",
        "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
        "        for i in range(0,num_blocks):\n",
        "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
        "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
        "    return prom_mixer_activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CKA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
        "def Heatmap(result,type,sigma):\n",
        "    dim = len(result)\n",
        "    k = (dim - 1)\n",
        "    heatmap_CKA = np.zeros((dim,dim))\n",
        "    for i in range(0,dim):\n",
        "        tr = (dim - 1)\n",
        "        for j in range(0,dim):\n",
        "            if type == 'kernel':\n",
        "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
        "            elif type == 'linear':\n",
        "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
        "            else:\n",
        "                print('There is no such category, try again')\n",
        "                break\n",
        "\n",
        "            tr -= 1\n",
        "        k -= 1\n",
        "    #print('CKA' + type + 'calculated')\n",
        "    return heatmap_CKA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Average of heatmaps (obsolet)\n",
        "def Prom_Mixer_Heatmaps(batch_result,type):\n",
        "    mat_heatmaps = list()\n",
        "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
        "    for i in range(0,batch_size):\n",
        "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
        "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
        "        mat_heatmaps.append(heatmap_raw)\n",
        "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
        "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
        "    return prom_mixer_heatmap,mat_heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_Heatmap(heatmap,type,bl):\n",
        "    #Number of thats that you want to appear in the plot\n",
        "    tri = 4\n",
        "    if type == 'kernel' or type == 'linear':\n",
        "        dim = len(heatmap)\n",
        "        axis_labels = list()\n",
        "        for i in range(0,dim):\n",
        "            axis_labels_inter = str('%i'%(i+1))\n",
        "            axis_labels.append(axis_labels_inter)\n",
        "        _, ax = plt.subplots(figsize=(6,6))\n",
        "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
        "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
        "        ax.invert_xaxis()\n",
        "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
        "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
        "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
        "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
        "\n",
        "        ax.set_title(\"CKA-\"+ type)   \n",
        "        ax.set_xlabel(\"Layer\")\n",
        "        ax.set_ylabel(\"Layer\")\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.locator_params(axis='x',nbins=tri)\n",
        "        plt.locator_params(axis='y',nbins=tri)\n",
        "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
        "        \n",
        "    else:\n",
        "        print('There is no such category, try again')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 3 : Across datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with CIFAR 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in range(2):\n",
        "    mlpmixer_blocks = keras.Sequential(\n",
        "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
        "    )\n",
        "    mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
        "    history,accuracy,top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
        "    #Saving Results\n",
        "    pwd = 'Results_Article/3A/mlpmixer_' + str(date) + '_CF10_' + str(k+1)\n",
        "    mlpmixer_classifier.save(pwd)\n",
        "    np.save( pwd + '/history_' + str(date) +'.npy',history.history)\n",
        "    with open(pwd + '/accuracy.pkl','wb') as file:\n",
        "        pickle.dump(accuracy,file)\n",
        "    with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
        "        pickle.dump(top_5_accuracy,file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with CIFAR 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Dataset for training \n",
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in range(2):\n",
        "    mlpmixer_blocks = keras.Sequential(\n",
        "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
        "    )\n",
        "    mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
        "    history,accuracy,top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
        "    #Saving Results\n",
        "    pwd = 'Results_Article/3A/mlpmixer_' + str(date) + '_CF100_' + str(k+1)\n",
        "    mlpmixer_classifier.save(pwd)\n",
        "    np.save( pwd + '/history_' + str(date) +'.npy',history.history)\n",
        "    with open(pwd + '/accuracy.pkl','wb') as file:\n",
        "        pickle.dump(accuracy,file)\n",
        "    with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
        "        pickle.dump(top_5_accuracy,file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Untrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlpmixer_blocks = keras.Sequential(\n",
        "[MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
        ")\n",
        "mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
        "pwd = 'Results_Article/3A/mlpmixer_' + str(date) + '_Untrained'\n",
        "mlpmixer_classifier.save(pwd)\n",
        "#np.save( pwd + '/history_' + str(date) +'.npy',history.history)\n",
        "#with open(pwd + '/accuracy.pkl','wb') as file:\n",
        "#    pickle.dump(accuracy,file)\n",
        "#with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
        "#    pickle.dump(top_5_accuracy,file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the path below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = 'Results_Article/3A/mlpmixer_2022-02-21_15-20_'\n",
        "global_models = list()\n",
        "#Call the folder\n",
        "C10_mlpmixer_1 = tf.keras.models.load_model(path + 'CF10_1')\n",
        "global_models.append(C10_mlpmixer_1)\n",
        "C10_mlpmixer_2 = tf.keras.models.load_model(path + 'CF10_2')\n",
        "global_models.append(C10_mlpmixer_2)\n",
        "C100_mlpmixer_1 = tf.keras.models.load_model(path + 'CF100_1')\n",
        "global_models.append(C100_mlpmixer_1)\n",
        "C100_mlpmixer_2 = tf.keras.models.load_model(path + 'CF100_2')\n",
        "global_models.append(C100_mlpmixer_2)\n",
        "Unt_mlpmixer = tf.keras.models.load_model(path + 'Untrained')\n",
        "global_models.append(Unt_mlpmixer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intialization before testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def across_datasets(global_models,batch_prepro,type,sigma):\n",
        "    total_activations = list()\n",
        "    plot_raw = list()\n",
        "    plot_total = list()\n",
        "    for k in range(len(global_models)):\n",
        "        tested_model = global_models[k] \n",
        "        ave_mixer_activations = Prom_Mixer_Activations_Blocks(tested_model,batch_prepro)\n",
        "        total_activations.append(ave_mixer_activations)\n",
        "        \n",
        "    for pairs in set:\n",
        "        comp_1 = total_activations[pairs[0]]\n",
        "        comp_2 = total_activations[pairs[1]]\n",
        "        plot_raw = list()\n",
        "        for i in range(num_blocks):\n",
        "            if type == 'rbf':\n",
        "                inter_row = cka(gram_rbf(comp_1[i],sigma),gram_rbf(comp_2[i],sigma))\n",
        "            elif type == 'linear':\n",
        "                inter_row = cka(gram_linear(comp_1[i]),gram_linear(comp_2[i]))\n",
        "            plot_raw.append(inter_row)\n",
        "        plot_total.append(plot_raw)\n",
        "    return plot_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The experiment on the paper is with the linear type!\n",
        "#######################################################\n",
        "\n",
        "sigma = None\n",
        "type = 'linear'\n",
        "\n",
        "#Pairs of models that are going to be compared according to the order in the matrix\n",
        "set = [[0,1],[2,3],[0,2],[0,4],[2,4]]\n",
        "label_set = ['CIFAR-10 Net vs. CIFAR-10 Net',\n",
        "            'CIFAR-100 Net vs. CIFAR-100 Net',\n",
        "            'CIFAR-10 Net vs. CIFAR-100 Net ',\n",
        "            'CIFAR-10 Net vs. Untrained',\n",
        "            'CIFAR-100 Net vs. Untrained']\n",
        "num_models_set = len(set)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tested on CIFAR 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset for testing\n",
        "\n",
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "(x_train, y_train), _ = keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run Once to Avoid Randomness after setting the testing dataset\n",
        "batch_prepro = Batch_Preprocessing(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The experiment on the paper is with the linear type\n",
        "plot_total_1 = across_datasets(global_models,batch_prepro,type,sigma)\n",
        "with open('Results_Article/3A/plot_total_C10.pkl','wb') as file:\n",
        "    pickle.dump(plot_total_1,file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tested on CIFAR 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset for testing\n",
        "\n",
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "(x_train, y_train), _ = keras.datasets.cifar100.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run Once to Avoid Randomness after setting the testing dataset\n",
        "batch_prepro = Batch_Preprocessing(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The experiment on the paper is with the linear type\n",
        "plot_total_2 = across_datasets(global_models,batch_prepro,type,sigma)\n",
        "with open('Results_Article/3A/plot_total_C100.pkl','wb') as file:\n",
        "    pickle.dump(plot_total_2,file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tested on MNIST: (Appendix 6A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset for testing\n",
        "\n",
        "num_classes = 10\n",
        "input_shape = (28, 28)\n",
        "(x_train, y_train), _ = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run Once to Avoid Randomness after setting the testing dataset\n",
        "batch_prepro = Batch_Preprocessing(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_total_3 = across_datasets(global_models,batch_prepro,type,sigma)\n",
        "with open('Results_Article/6A/plot_total_MNIST.pkl','wb') as file:\n",
        "    pickle.dump(plot_total_3,file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EXPERIMENT 3: VERIFICATIONS (OPTIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tested_plot = plot_total_1\n",
        "name = '_tested_C10'\n",
        "######################################################\n",
        "x = list(range(1,num_blocks+1))\n",
        "for j in range(num_models_set):\n",
        "    plt.plot(x,tested_plot[j], label = label_set[j])\n",
        "plt.xlabel('Layer')\n",
        "plt.ylabel('CKA ('+ type +')')\n",
        "plt.locator_params(axis='x', nbins=num_blocks)\n",
        "plt.title('Similarity on CIFAR-10')\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.savefig('Results_Article/3A/Similarity_'+ type + name +'.png') \n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mlp_image_classification",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
