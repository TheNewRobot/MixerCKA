{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSzgroG_Suq"
   },
   "source": [
    "# Study of Image classification with modern MLP Mixer model and CKA\n",
    "\n",
    "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
    "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U087DdDw_Suy"
   },
   "source": [
    "# Setup for the MLP-Mixer Architecture\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AnTyoluw_Suz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import pickle\n",
    "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
    "from CKA_Google import *\n",
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 : Across Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DAOrIHu_Su2"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1iMiVS7o_Su3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224 = 50176\n",
      "Patch size: 32 X 32 = 1024 \n",
      "Patches per image: 49\n",
      "Elements per patch (3 channels): 3072\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 512 \n",
    "num_epochs = 50\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Selected Architecture: B/32\n",
    "\n",
    "image_size = 224  # We'll resize input images to this size. Square\n",
    "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
    "embedding_dim = 384  # Fixed Embedding Dimension\n",
    "num_blocks = 12\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d_%H-%M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuu2OAS_Su0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Dataset for training \n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "#plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mpnEZH_Su5"
   },
   "source": [
    "## Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ra-bXojQ_Su6"
   },
   "outputs": [],
   "source": [
    "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data. \n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches. \n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks. ## (sequential_82)\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation) \n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpQFU8H__Su8"
   },
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9E1zD2BY_Su8"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        #Negative Log Likelihood = Categorical Cross Entropy\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping regularization callback. \n",
    "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
    "    #early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    #    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    #)\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history, accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxeE0cmM_Su9"
   },
   "source": [
    "## Use data augmentation\n",
    "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zh6hFPWX_Su-"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G77cUgiu_Su-"
   },
   "source": [
    "## Implement patch extraction as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYKNktXe_Su_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        #Extract the shape dimension in the position 0 = columns\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Without overlapping, stride horizontally and vertically\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
    "            rates=[1, 1, 1, 1],\n",
    "            #Patches contained in the images are considered, no zero padding\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        #shape[-1], number of colummns, as well as shape[0]\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Patches, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'patch_size' : self.patch_size ,\n",
    "            'num_patches' : self.num_patches\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yehcSS_Su_"
   },
   "source": [
    "## The MLP-Mixer model\n",
    "\n",
    "The MLP-Mixer is an architecture based exclusively on\n",
    "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "1. One applied independently to image patches, which mixes the per-location features.\n",
    "2. The other applied across patches (along channels), which mixes spatial information.\n",
    "\n",
    "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
    "instead of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwg4e2n_SvA"
   },
   "source": [
    "### Implement the MLP-Mixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dT6wVEki_SvA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp2 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super(MLPMixerLayer, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'num_patches' : num_patches,\n",
    "            'embedding_dim' : embedding_dim,\n",
    "            'dropout_rate' : dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUiufq92_SvA"
   },
   "source": [
    "## Build, train, and evaluate the MLP-Mixer model\n",
    "\n",
    "Note that training the model with the current settings on a V100 GPUs\n",
    "takes around 8 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report: Learning Curve\n",
    "def curves(history):\n",
    "    ymax1 = min(history[\"loss\"])\n",
    "    xmax1 = history[\"loss\"].index(ymax1)\n",
    "    ymax2 = min(history[\"val_loss\"])\n",
    "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
    "    plt.title(\"Cross Entropy Loss\")\n",
    "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
    "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Graph accuracy\n",
    "    ymax3 = max(history[\"acc\"])\n",
    "    xmax3 = history[\"acc\"].index(ymax3)\n",
    "    ymax4 = max(history[\"val_acc\"])\n",
    "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
    "    ymax5 = max(history[\"top5-acc\"])\n",
    "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
    "    ymax6 = max(history[\"val_top5-acc\"])\n",
    "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Classification accuracy')\n",
    "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Classification top5-acc')\n",
    "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.suptitle(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain activations + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers + Patches + One dense layer\n",
    "def Preprocessing(num_example):\n",
    "    augmented = data_augmentation(x_train[num_example])\n",
    "    b = Patches(patch_size, num_patches)(augmented)\n",
    "    a = layers.Dense(units=embedding_dim)(b)\n",
    "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
    "def Batch_Preprocessing(batch_size):\n",
    "    #Vector with the number of Sample of the Xtrain\n",
    "    a  = list(range(0,x_train.shape[0]))\n",
    "    b = random.sample(a,batch_size)\n",
    "    batch_regularization = list()\n",
    "    for i in range(0,batch_size):\n",
    "        inter_result = Preprocessing(b[i])\n",
    "        batch_regularization.append(inter_result)\n",
    "    return batch_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_out(result,layer_number,example):\n",
    "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "    ax1.imshow(x_train[example])\n",
    "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
    "    ax2.imshow(result[layer_number])\n",
    "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
    "    #The input is fixed to the beginning of the mlp blocks\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[0]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of layer's activation\n",
    "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
    "def Heatmap(result,type,sigma):\n",
    "    dim = len(result)\n",
    "    k = (dim - 1)\n",
    "    heatmap_CKA = np.zeros((dim,dim))\n",
    "    for i in range(0,dim):\n",
    "        tr = (dim - 1)\n",
    "        for j in range(0,dim):\n",
    "            if type == 'kernel':\n",
    "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
    "            elif type == 'linear':\n",
    "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
    "            else:\n",
    "                print('There is no such category, try again')\n",
    "                break\n",
    "\n",
    "            tr -= 1\n",
    "        k -= 1\n",
    "    #print('CKA' + type + 'calculated')\n",
    "    return heatmap_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of heatmaps (obsolet)\n",
    "def Prom_Mixer_Heatmaps(batch_result,type):\n",
    "    mat_heatmaps = list()\n",
    "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
    "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
    "        mat_heatmaps.append(heatmap_raw)\n",
    "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
    "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
    "    return prom_mixer_heatmap,mat_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 4\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i'%(i+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(3,3))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 : Across datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained with CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 244ms/step - loss: 3.9389 - acc: 0.2483 - top5-acc: 0.7411 - val_loss: 1.8100 - val_acc: 0.3736 - val_top5-acc: 0.8580 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.5981 - acc: 0.4225 - top5-acc: 0.8957 - val_loss: 1.4520 - val_acc: 0.4810 - val_top5-acc: 0.9244 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.4260 - acc: 0.4867 - top5-acc: 0.9242 - val_loss: 1.2642 - val_acc: 0.5486 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3296 - acc: 0.5213 - top5-acc: 0.9367 - val_loss: 1.2582 - val_acc: 0.5598 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2696 - acc: 0.5472 - top5-acc: 0.9442 - val_loss: 1.1736 - val_acc: 0.5830 - val_top5-acc: 0.9566 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2185 - acc: 0.5677 - top5-acc: 0.9491 - val_loss: 1.1446 - val_acc: 0.5996 - val_top5-acc: 0.9588 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1793 - acc: 0.5811 - top5-acc: 0.9535 - val_loss: 1.0834 - val_acc: 0.6212 - val_top5-acc: 0.9602 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1663 - acc: 0.5862 - top5-acc: 0.9535 - val_loss: 1.0527 - val_acc: 0.6328 - val_top5-acc: 0.9666 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1227 - acc: 0.6009 - top5-acc: 0.9592 - val_loss: 1.0300 - val_acc: 0.6402 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0799 - acc: 0.6188 - top5-acc: 0.9624 - val_loss: 1.0208 - val_acc: 0.6386 - val_top5-acc: 0.9700 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0548 - acc: 0.6267 - top5-acc: 0.9640 - val_loss: 1.0118 - val_acc: 0.6432 - val_top5-acc: 0.9684 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0324 - acc: 0.6340 - top5-acc: 0.9636 - val_loss: 0.9921 - val_acc: 0.6576 - val_top5-acc: 0.9686 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0110 - acc: 0.6428 - top5-acc: 0.9657 - val_loss: 0.9504 - val_acc: 0.6708 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0012 - acc: 0.6485 - top5-acc: 0.9676 - val_loss: 0.9772 - val_acc: 0.6588 - val_top5-acc: 0.9714 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9908 - acc: 0.6541 - top5-acc: 0.9673 - val_loss: 0.9124 - val_acc: 0.6892 - val_top5-acc: 0.9706 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9720 - acc: 0.6579 - top5-acc: 0.9688 - val_loss: 0.9110 - val_acc: 0.6772 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9378 - acc: 0.6718 - top5-acc: 0.9716 - val_loss: 0.9335 - val_acc: 0.6790 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9283 - acc: 0.6742 - top5-acc: 0.9733 - val_loss: 0.8703 - val_acc: 0.6940 - val_top5-acc: 0.9732 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9178 - acc: 0.6768 - top5-acc: 0.9726 - val_loss: 0.8869 - val_acc: 0.6920 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9037 - acc: 0.6806 - top5-acc: 0.9745 - val_loss: 0.8847 - val_acc: 0.6866 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8928 - acc: 0.6858 - top5-acc: 0.9750 - val_loss: 0.8667 - val_acc: 0.6990 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8771 - acc: 0.6911 - top5-acc: 0.9766 - val_loss: 0.8390 - val_acc: 0.7146 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8809 - acc: 0.6908 - top5-acc: 0.9761 - val_loss: 0.8645 - val_acc: 0.7068 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8746 - acc: 0.6925 - top5-acc: 0.9770 - val_loss: 0.8725 - val_acc: 0.7090 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8442 - acc: 0.7040 - top5-acc: 0.9778 - val_loss: 0.8285 - val_acc: 0.7174 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8443 - acc: 0.7017 - top5-acc: 0.9780 - val_loss: 0.8323 - val_acc: 0.7054 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8244 - acc: 0.7115 - top5-acc: 0.9794 - val_loss: 0.8340 - val_acc: 0.7130 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8386 - acc: 0.7044 - top5-acc: 0.9787 - val_loss: 0.8190 - val_acc: 0.7268 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.8106 - acc: 0.7157 - top5-acc: 0.9799 - val_loss: 0.8368 - val_acc: 0.7214 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8224 - acc: 0.7091 - top5-acc: 0.9798 - val_loss: 0.8067 - val_acc: 0.7230 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7907 - acc: 0.7237 - top5-acc: 0.9818 - val_loss: 0.7816 - val_acc: 0.7320 - val_top5-acc: 0.9844 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7881 - acc: 0.7224 - top5-acc: 0.9815 - val_loss: 0.8740 - val_acc: 0.7116 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7976 - acc: 0.7202 - top5-acc: 0.9813 - val_loss: 0.8051 - val_acc: 0.7338 - val_top5-acc: 0.9832 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.7817 - acc: 0.7258 - top5-acc: 0.9821 - val_loss: 0.8141 - val_acc: 0.7296 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7752 - acc: 0.7290 - top5-acc: 0.9824 - val_loss: 0.8140 - val_acc: 0.7310 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 16.0484 - acc: 0.3231 - top5-acc: 0.7654 - val_loss: 1.5275 - val_acc: 0.4480 - val_top5-acc: 0.9116 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.4763 - acc: 0.4722 - top5-acc: 0.9174 - val_loss: 1.2905 - val_acc: 0.5400 - val_top5-acc: 0.9442 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.3299 - acc: 0.5226 - top5-acc: 0.9373 - val_loss: 1.1826 - val_acc: 0.5832 - val_top5-acc: 0.9534 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.2333 - acc: 0.5620 - top5-acc: 0.9477 - val_loss: 1.0974 - val_acc: 0.6124 - val_top5-acc: 0.9618 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.1649 - acc: 0.5851 - top5-acc: 0.9536 - val_loss: 1.0355 - val_acc: 0.6380 - val_top5-acc: 0.9684 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1070 - acc: 0.6069 - top5-acc: 0.9601 - val_loss: 1.0078 - val_acc: 0.6430 - val_top5-acc: 0.9698 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0550 - acc: 0.6265 - top5-acc: 0.9646 - val_loss: 0.9631 - val_acc: 0.6608 - val_top5-acc: 0.9728 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0320 - acc: 0.6354 - top5-acc: 0.9668 - val_loss: 0.9518 - val_acc: 0.6666 - val_top5-acc: 0.9734 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0110 - acc: 0.6433 - top5-acc: 0.9675 - val_loss: 0.9282 - val_acc: 0.6750 - val_top5-acc: 0.9750 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9844 - acc: 0.6515 - top5-acc: 0.9700 - val_loss: 0.9134 - val_acc: 0.6764 - val_top5-acc: 0.9768 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9731 - acc: 0.6560 - top5-acc: 0.9712 - val_loss: 0.8996 - val_acc: 0.6856 - val_top5-acc: 0.9768 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9497 - acc: 0.6654 - top5-acc: 0.9726 - val_loss: 0.8877 - val_acc: 0.6854 - val_top5-acc: 0.9770 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9326 - acc: 0.6683 - top5-acc: 0.9735 - val_loss: 0.8821 - val_acc: 0.6890 - val_top5-acc: 0.9772 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9337 - acc: 0.6725 - top5-acc: 0.9728 - val_loss: 0.8839 - val_acc: 0.6902 - val_top5-acc: 0.9772 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9184 - acc: 0.6762 - top5-acc: 0.9749 - val_loss: 0.8615 - val_acc: 0.6944 - val_top5-acc: 0.9774 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.8908 - acc: 0.6877 - top5-acc: 0.9740\n",
      "Test accuracy: 68.77%\n",
      "Test top 5 accuracy: 97.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn, layer_normalization_1_layer_call_and_return_conditional_losses, layer_normalization_2_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF10_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF10_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 246ms/step - loss: 4.1068 - acc: 0.2574 - top5-acc: 0.7468 - val_loss: 1.6281 - val_acc: 0.4094 - val_top5-acc: 0.8948 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5560 - acc: 0.4362 - top5-acc: 0.9036 - val_loss: 1.4308 - val_acc: 0.4878 - val_top5-acc: 0.9318 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.4239 - acc: 0.4872 - top5-acc: 0.9229 - val_loss: 1.3277 - val_acc: 0.5326 - val_top5-acc: 0.9468 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3354 - acc: 0.5233 - top5-acc: 0.9358 - val_loss: 1.2464 - val_acc: 0.5492 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2770 - acc: 0.5413 - top5-acc: 0.9432 - val_loss: 1.2270 - val_acc: 0.5626 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2296 - acc: 0.5618 - top5-acc: 0.9468 - val_loss: 1.1233 - val_acc: 0.6034 - val_top5-acc: 0.9604 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1975 - acc: 0.5738 - top5-acc: 0.9486 - val_loss: 1.1012 - val_acc: 0.6050 - val_top5-acc: 0.9642 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1533 - acc: 0.5924 - top5-acc: 0.9546 - val_loss: 1.0688 - val_acc: 0.6136 - val_top5-acc: 0.9670 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1500 - acc: 0.5924 - top5-acc: 0.9555 - val_loss: 1.0804 - val_acc: 0.6156 - val_top5-acc: 0.9676 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1170 - acc: 0.6039 - top5-acc: 0.9587 - val_loss: 1.0392 - val_acc: 0.6294 - val_top5-acc: 0.9674 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0837 - acc: 0.6162 - top5-acc: 0.9607 - val_loss: 0.9875 - val_acc: 0.6516 - val_top5-acc: 0.9704 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0640 - acc: 0.6208 - top5-acc: 0.9631 - val_loss: 0.9899 - val_acc: 0.6580 - val_top5-acc: 0.9714 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0515 - acc: 0.6256 - top5-acc: 0.9640 - val_loss: 0.9821 - val_acc: 0.6548 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0267 - acc: 0.6361 - top5-acc: 0.9661 - val_loss: 0.9597 - val_acc: 0.6564 - val_top5-acc: 0.9742 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0078 - acc: 0.6437 - top5-acc: 0.9662 - val_loss: 0.9553 - val_acc: 0.6630 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0027 - acc: 0.6469 - top5-acc: 0.9675 - val_loss: 0.9297 - val_acc: 0.6788 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9736 - acc: 0.6564 - top5-acc: 0.9698 - val_loss: 0.9643 - val_acc: 0.6628 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9699 - acc: 0.6554 - top5-acc: 0.9698 - val_loss: 0.9024 - val_acc: 0.6816 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9458 - acc: 0.6671 - top5-acc: 0.9714 - val_loss: 0.8866 - val_acc: 0.6930 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9347 - acc: 0.6697 - top5-acc: 0.9724 - val_loss: 0.9590 - val_acc: 0.6634 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9346 - acc: 0.6699 - top5-acc: 0.9727 - val_loss: 0.8551 - val_acc: 0.6922 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9029 - acc: 0.6805 - top5-acc: 0.9738 - val_loss: 0.8429 - val_acc: 0.7052 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8867 - acc: 0.6848 - top5-acc: 0.9754 - val_loss: 0.8487 - val_acc: 0.7000 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8826 - acc: 0.6882 - top5-acc: 0.9758 - val_loss: 0.8316 - val_acc: 0.7086 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8749 - acc: 0.6928 - top5-acc: 0.9768 - val_loss: 0.8603 - val_acc: 0.7032 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8807 - acc: 0.6905 - top5-acc: 0.9762 - val_loss: 0.8962 - val_acc: 0.6930 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8645 - acc: 0.6968 - top5-acc: 0.9759 - val_loss: 0.8522 - val_acc: 0.7068 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8506 - acc: 0.6985 - top5-acc: 0.9782 - val_loss: 0.8257 - val_acc: 0.7142 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8399 - acc: 0.7058 - top5-acc: 0.9780 - val_loss: 0.9629 - val_acc: 0.6846 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8583 - acc: 0.6993 - top5-acc: 0.9769 - val_loss: 0.8125 - val_acc: 0.7190 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.8277 - acc: 0.7097 - top5-acc: 0.9795 - val_loss: 0.8208 - val_acc: 0.7254 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8088 - acc: 0.7136 - top5-acc: 0.9794 - val_loss: 0.8452 - val_acc: 0.7188 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8145 - acc: 0.7132 - top5-acc: 0.9807 - val_loss: 0.8351 - val_acc: 0.7114 - val_top5-acc: 0.9796 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8020 - acc: 0.7193 - top5-acc: 0.9807 - val_loss: 0.8120 - val_acc: 0.7190 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.7958 - acc: 0.7174 - top5-acc: 0.9811 - val_loss: 0.8145 - val_acc: 0.7304 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8182 - acc: 0.7124 - top5-acc: 0.9805 - val_loss: 0.7980 - val_acc: 0.7298 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7882 - acc: 0.7224 - top5-acc: 0.9818 - val_loss: 0.8130 - val_acc: 0.7294 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7741 - acc: 0.7282 - top5-acc: 0.9817 - val_loss: 0.8033 - val_acc: 0.7328 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7502 - acc: 0.7334 - top5-acc: 0.9842 - val_loss: 0.8491 - val_acc: 0.7264 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 12.2417 - acc: 0.5122 - top5-acc: 0.8489 - val_loss: 11.4520 - val_acc: 0.2786 - val_top5-acc: 0.6532 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 3.6717 - acc: 0.4148 - top5-acc: 0.8592 - val_loss: 1.1719 - val_acc: 0.5912 - val_top5-acc: 0.9596 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1949 - acc: 0.5781 - top5-acc: 0.9507 - val_loss: 1.0605 - val_acc: 0.6312 - val_top5-acc: 0.9664 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.1021 - acc: 0.6115 - top5-acc: 0.9584 - val_loss: 0.9959 - val_acc: 0.6508 - val_top5-acc: 0.9714 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0313 - acc: 0.6354 - top5-acc: 0.9647 - val_loss: 0.9424 - val_acc: 0.6706 - val_top5-acc: 0.9742 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9713 - acc: 0.6578 - top5-acc: 0.9695 - val_loss: 0.8940 - val_acc: 0.6854 - val_top5-acc: 0.9748 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9223 - acc: 0.6744 - top5-acc: 0.9739 - val_loss: 0.8872 - val_acc: 0.6932 - val_top5-acc: 0.9754 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8904 - acc: 0.6860 - top5-acc: 0.9745 - val_loss: 0.8711 - val_acc: 0.6964 - val_top5-acc: 0.9766 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8729 - acc: 0.6925 - top5-acc: 0.9755 - val_loss: 0.8469 - val_acc: 0.7096 - val_top5-acc: 0.9784 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8518 - acc: 0.7006 - top5-acc: 0.9768 - val_loss: 0.8330 - val_acc: 0.7120 - val_top5-acc: 0.9800 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8333 - acc: 0.7053 - top5-acc: 0.9787 - val_loss: 0.8221 - val_acc: 0.7146 - val_top5-acc: 0.9804 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.8401 - acc: 0.7091 - top5-acc: 0.9778\n",
      "Test accuracy: 70.91%\n",
      "Test top 5 accuracy: 97.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_12_layer_call_fn, layer_normalization_12_layer_call_and_return_conditional_losses, layer_normalization_13_layer_call_fn, layer_normalization_13_layer_call_and_return_conditional_losses, layer_normalization_14_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF10_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF10_2\\assets\n"
     ]
    }
   ],
   "source": [
    "for k in range(2):\n",
    "    mlpmixer_blocks = keras.Sequential(\n",
    "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
    "    )\n",
    "    mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
    "    history,accuracy,top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "    #Saving Results\n",
    "    #pwd = 'Results_Article/3A/mlpmixer_' + str(date) + '_CF10_' + str(k+1)\n",
    "    pwd = 'Results_Article/3A/mlpmixer_CF10_' + str(k+1)\n",
    "    mlpmixer_classifier.save(pwd)\n",
    "    np.save( pwd + '/history.npy',history.history)\n",
    "    with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "        pickle.dump(accuracy,file)\n",
    "    with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "        pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained with CIFAR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for training \n",
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 245ms/step - loss: 5.8978 - acc: 0.0525 - top5-acc: 0.1735 - val_loss: 4.1339 - val_acc: 0.1000 - val_top5-acc: 0.2756 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.7259 - acc: 0.1278 - top5-acc: 0.3563 - val_loss: 3.5849 - val_acc: 0.1770 - val_top5-acc: 0.4230 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.4363 - acc: 0.1806 - top5-acc: 0.4406 - val_loss: 3.2814 - val_acc: 0.2172 - val_top5-acc: 0.4938 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.2665 - acc: 0.2095 - top5-acc: 0.4890 - val_loss: 3.2502 - val_acc: 0.2258 - val_top5-acc: 0.5156 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.1360 - acc: 0.2332 - top5-acc: 0.5240 - val_loss: 3.0083 - val_acc: 0.2778 - val_top5-acc: 0.5632 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 3.0284 - acc: 0.2520 - top5-acc: 0.5529 - val_loss: 3.0077 - val_acc: 0.2768 - val_top5-acc: 0.5708 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.9308 - acc: 0.2731 - top5-acc: 0.5743 - val_loss: 2.8614 - val_acc: 0.2942 - val_top5-acc: 0.6012 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.8385 - acc: 0.2926 - top5-acc: 0.5987 - val_loss: 2.7978 - val_acc: 0.3136 - val_top5-acc: 0.6242 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.7858 - acc: 0.3038 - top5-acc: 0.6110 - val_loss: 2.7716 - val_acc: 0.3114 - val_top5-acc: 0.6262 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.7179 - acc: 0.3155 - top5-acc: 0.6256 - val_loss: 2.7082 - val_acc: 0.3262 - val_top5-acc: 0.6440 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.6739 - acc: 0.3233 - top5-acc: 0.6377 - val_loss: 2.6378 - val_acc: 0.3280 - val_top5-acc: 0.6562 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.6131 - acc: 0.3371 - top5-acc: 0.6510 - val_loss: 2.5749 - val_acc: 0.3522 - val_top5-acc: 0.6672 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.5848 - acc: 0.3441 - top5-acc: 0.6546 - val_loss: 2.5940 - val_acc: 0.3520 - val_top5-acc: 0.6620 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.5426 - acc: 0.3521 - top5-acc: 0.6652 - val_loss: 2.6481 - val_acc: 0.3488 - val_top5-acc: 0.6552 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.4918 - acc: 0.3609 - top5-acc: 0.6747 - val_loss: 2.5151 - val_acc: 0.3712 - val_top5-acc: 0.6880 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.4635 - acc: 0.3680 - top5-acc: 0.6811 - val_loss: 2.4544 - val_acc: 0.3738 - val_top5-acc: 0.6866 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.4538 - acc: 0.3682 - top5-acc: 0.6876 - val_loss: 2.4616 - val_acc: 0.3804 - val_top5-acc: 0.6944 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 2.3978 - acc: 0.3817 - top5-acc: 0.6983 - val_loss: 2.4021 - val_acc: 0.3954 - val_top5-acc: 0.6996 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.3883 - acc: 0.3858 - top5-acc: 0.7005 - val_loss: 2.3861 - val_acc: 0.3932 - val_top5-acc: 0.7120 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.3475 - acc: 0.3932 - top5-acc: 0.7096 - val_loss: 2.4133 - val_acc: 0.3924 - val_top5-acc: 0.6980 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.3345 - acc: 0.3961 - top5-acc: 0.7125 - val_loss: 2.4079 - val_acc: 0.3988 - val_top5-acc: 0.7084 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.3168 - acc: 0.3997 - top5-acc: 0.7172 - val_loss: 2.3739 - val_acc: 0.3936 - val_top5-acc: 0.7116 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 2.2764 - acc: 0.4046 - top5-acc: 0.7229 - val_loss: 2.3964 - val_acc: 0.4054 - val_top5-acc: 0.7062 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 2.2353 - acc: 0.4151 - top5-acc: 0.7324 - val_loss: 2.3645 - val_acc: 0.4084 - val_top5-acc: 0.7230 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 10.3033 - acc: 0.2224 - top5-acc: 0.4644 - val_loss: 3.4180 - val_acc: 0.2898 - val_top5-acc: 0.5794 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.5498 - acc: 0.3599 - top5-acc: 0.6714 - val_loss: 2.3708 - val_acc: 0.4056 - val_top5-acc: 0.7172 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.2295 - acc: 0.4164 - top5-acc: 0.7318 - val_loss: 2.2781 - val_acc: 0.4290 - val_top5-acc: 0.7316 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.1066 - acc: 0.4475 - top5-acc: 0.7539 - val_loss: 2.2329 - val_acc: 0.4360 - val_top5-acc: 0.7362 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.0332 - acc: 0.4584 - top5-acc: 0.7686 - val_loss: 2.2026 - val_acc: 0.4436 - val_top5-acc: 0.7476 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.0021 - acc: 0.4691 - top5-acc: 0.7734 - val_loss: 2.1604 - val_acc: 0.4514 - val_top5-acc: 0.7514 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.9635 - acc: 0.4738 - top5-acc: 0.7818 - val_loss: 2.1427 - val_acc: 0.4548 - val_top5-acc: 0.7600 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.9292 - acc: 0.4805 - top5-acc: 0.7891 - val_loss: 2.1649 - val_acc: 0.4494 - val_top5-acc: 0.7584 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.9311 - acc: 0.4838 - top5-acc: 0.7880 - val_loss: 2.1502 - val_acc: 0.4516 - val_top5-acc: 0.7554 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.9030 - acc: 0.4879 - top5-acc: 0.7945 - val_loss: 2.1584 - val_acc: 0.4474 - val_top5-acc: 0.7552 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8906 - acc: 0.4923 - top5-acc: 0.7944 - val_loss: 2.1396 - val_acc: 0.4530 - val_top5-acc: 0.7550 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8619 - acc: 0.4971 - top5-acc: 0.7981 - val_loss: 2.1240 - val_acc: 0.4648 - val_top5-acc: 0.7634 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8773 - acc: 0.4918 - top5-acc: 0.7977 - val_loss: 2.1751 - val_acc: 0.4522 - val_top5-acc: 0.7528 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8594 - acc: 0.4968 - top5-acc: 0.8017 - val_loss: 2.1644 - val_acc: 0.4578 - val_top5-acc: 0.7612 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8420 - acc: 0.5006 - top5-acc: 0.8044 - val_loss: 2.1205 - val_acc: 0.4658 - val_top5-acc: 0.7714 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8273 - acc: 0.5048 - top5-acc: 0.8068 - val_loss: 2.1021 - val_acc: 0.4660 - val_top5-acc: 0.7650 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8367 - acc: 0.5003 - top5-acc: 0.8074 - val_loss: 2.1911 - val_acc: 0.4588 - val_top5-acc: 0.7548 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8212 - acc: 0.5036 - top5-acc: 0.8091 - val_loss: 2.1452 - val_acc: 0.4584 - val_top5-acc: 0.7640 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8115 - acc: 0.5067 - top5-acc: 0.8101 - val_loss: 2.1828 - val_acc: 0.4480 - val_top5-acc: 0.7628 - lr: 0.0050\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8178 - acc: 0.5064 - top5-acc: 0.8112 - val_loss: 2.1940 - val_acc: 0.4554 - val_top5-acc: 0.7596 - lr: 0.0050\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.8030 - acc: 0.5076 - top5-acc: 0.8149 - val_loss: 2.1243 - val_acc: 0.4674 - val_top5-acc: 0.7666 - lr: 0.0050\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.6316 - acc: 0.5497 - top5-acc: 0.8391 - val_loss: 2.0414 - val_acc: 0.4878 - val_top5-acc: 0.7824 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.5887 - acc: 0.5574 - top5-acc: 0.8473 - val_loss: 2.0375 - val_acc: 0.4798 - val_top5-acc: 0.7810 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.5706 - acc: 0.5612 - top5-acc: 0.8509 - val_loss: 2.0713 - val_acc: 0.4856 - val_top5-acc: 0.7770 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.5458 - acc: 0.5680 - top5-acc: 0.8536 - val_loss: 2.0648 - val_acc: 0.4850 - val_top5-acc: 0.7846 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.5372 - acc: 0.5741 - top5-acc: 0.8556 - val_loss: 2.0345 - val_acc: 0.4918 - val_top5-acc: 0.7802 - lr: 0.0025\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 1.9821 - acc: 0.5014 - top5-acc: 0.7881\n",
      "Test accuracy: 50.14%\n",
      "Test top 5 accuracy: 78.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_24_layer_call_fn, layer_normalization_24_layer_call_and_return_conditional_losses, layer_normalization_25_layer_call_fn, layer_normalization_25_layer_call_and_return_conditional_losses, layer_normalization_26_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF100_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF100_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 244ms/step - loss: 5.8102 - acc: 0.0492 - top5-acc: 0.1748 - val_loss: 4.1710 - val_acc: 0.0894 - val_top5-acc: 0.2924 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.7196 - acc: 0.1306 - top5-acc: 0.3597 - val_loss: 3.6789 - val_acc: 0.1656 - val_top5-acc: 0.4002 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.4151 - acc: 0.1810 - top5-acc: 0.4476 - val_loss: 3.2782 - val_acc: 0.2206 - val_top5-acc: 0.4924 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.2606 - acc: 0.2079 - top5-acc: 0.4919 - val_loss: 3.2293 - val_acc: 0.2394 - val_top5-acc: 0.5224 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.0992 - acc: 0.2390 - top5-acc: 0.5336 - val_loss: 3.1670 - val_acc: 0.2442 - val_top5-acc: 0.5484 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.0086 - acc: 0.2577 - top5-acc: 0.5569 - val_loss: 2.9952 - val_acc: 0.2760 - val_top5-acc: 0.5716 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.8951 - acc: 0.2800 - top5-acc: 0.5845 - val_loss: 2.8395 - val_acc: 0.2928 - val_top5-acc: 0.6086 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.8151 - acc: 0.2959 - top5-acc: 0.6062 - val_loss: 2.7511 - val_acc: 0.3166 - val_top5-acc: 0.6256 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.7524 - acc: 0.3068 - top5-acc: 0.6196 - val_loss: 2.8198 - val_acc: 0.2994 - val_top5-acc: 0.6124 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.6867 - acc: 0.3230 - top5-acc: 0.6336 - val_loss: 2.6776 - val_acc: 0.3296 - val_top5-acc: 0.6416 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.6370 - acc: 0.3326 - top5-acc: 0.6460 - val_loss: 2.6178 - val_acc: 0.3448 - val_top5-acc: 0.6584 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.5885 - acc: 0.3415 - top5-acc: 0.6555 - val_loss: 2.5932 - val_acc: 0.3478 - val_top5-acc: 0.6588 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.5924 - acc: 0.3423 - top5-acc: 0.6573 - val_loss: 2.5442 - val_acc: 0.3566 - val_top5-acc: 0.6666 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.5588 - acc: 0.3495 - top5-acc: 0.6626 - val_loss: 2.5194 - val_acc: 0.3658 - val_top5-acc: 0.6810 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.4961 - acc: 0.3629 - top5-acc: 0.6765 - val_loss: 2.5018 - val_acc: 0.3654 - val_top5-acc: 0.6774 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.4542 - acc: 0.3681 - top5-acc: 0.6858 - val_loss: 2.4759 - val_acc: 0.3742 - val_top5-acc: 0.6842 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 2.4499 - acc: 0.3721 - top5-acc: 0.6869 - val_loss: 2.4119 - val_acc: 0.3848 - val_top5-acc: 0.7092 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.3719 - acc: 0.3868 - top5-acc: 0.7027 - val_loss: 2.3947 - val_acc: 0.3890 - val_top5-acc: 0.7070 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 11.2833 - acc: 0.2873 - top5-acc: 0.5519 - val_loss: 27.7968 - val_acc: 0.0556 - val_top5-acc: 0.1848 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 7.6530 - acc: 0.1830 - top5-acc: 0.4210 - val_loss: 2.8140 - val_acc: 0.3128 - val_top5-acc: 0.6174 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.6802 - acc: 0.3261 - top5-acc: 0.6374 - val_loss: 2.5780 - val_acc: 0.3666 - val_top5-acc: 0.6676 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.4744 - acc: 0.3654 - top5-acc: 0.6811 - val_loss: 2.4649 - val_acc: 0.3816 - val_top5-acc: 0.6900 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.3754 - acc: 0.3875 - top5-acc: 0.7018 - val_loss: 2.3705 - val_acc: 0.4050 - val_top5-acc: 0.7006 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.2909 - acc: 0.4039 - top5-acc: 0.7180 - val_loss: 2.3370 - val_acc: 0.4096 - val_top5-acc: 0.7092 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.2368 - acc: 0.4152 - top5-acc: 0.7295 - val_loss: 2.3141 - val_acc: 0.4134 - val_top5-acc: 0.7210 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.1790 - acc: 0.4286 - top5-acc: 0.7399 - val_loss: 2.2702 - val_acc: 0.4244 - val_top5-acc: 0.7222 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.1432 - acc: 0.4361 - top5-acc: 0.7475 - val_loss: 2.2590 - val_acc: 0.4250 - val_top5-acc: 0.7242 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.1129 - acc: 0.4429 - top5-acc: 0.7526 - val_loss: 2.2661 - val_acc: 0.4284 - val_top5-acc: 0.7280 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.0902 - acc: 0.4462 - top5-acc: 0.7572 - val_loss: 2.2347 - val_acc: 0.4368 - val_top5-acc: 0.7302 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.0605 - acc: 0.4555 - top5-acc: 0.7630 - val_loss: 2.2571 - val_acc: 0.4314 - val_top5-acc: 0.7312 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.0471 - acc: 0.4564 - top5-acc: 0.7659 - val_loss: 2.2090 - val_acc: 0.4384 - val_top5-acc: 0.7382 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.0358 - acc: 0.4556 - top5-acc: 0.7690 - val_loss: 2.2010 - val_acc: 0.4386 - val_top5-acc: 0.7410 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.9969 - acc: 0.4663 - top5-acc: 0.7773 - val_loss: 2.2345 - val_acc: 0.4360 - val_top5-acc: 0.7360 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.9915 - acc: 0.4686 - top5-acc: 0.7776 - val_loss: 2.1889 - val_acc: 0.4476 - val_top5-acc: 0.7452 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9844 - acc: 0.4700 - top5-acc: 0.7797 - val_loss: 2.2066 - val_acc: 0.4438 - val_top5-acc: 0.7406 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9547 - acc: 0.4776 - top5-acc: 0.7814 - val_loss: 2.2166 - val_acc: 0.4416 - val_top5-acc: 0.7382 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.9640 - acc: 0.4732 - top5-acc: 0.7815 - val_loss: 2.1912 - val_acc: 0.4478 - val_top5-acc: 0.7534 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9411 - acc: 0.4784 - top5-acc: 0.7836 - val_loss: 2.1793 - val_acc: 0.4452 - val_top5-acc: 0.7510 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.9286 - acc: 0.4791 - top5-acc: 0.7907 - val_loss: 2.1675 - val_acc: 0.4476 - val_top5-acc: 0.7454 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9192 - acc: 0.4816 - top5-acc: 0.7933 - val_loss: 2.1823 - val_acc: 0.4488 - val_top5-acc: 0.7428 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9134 - acc: 0.4854 - top5-acc: 0.7907 - val_loss: 2.1821 - val_acc: 0.4526 - val_top5-acc: 0.7466 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.9115 - acc: 0.4850 - top5-acc: 0.7916 - val_loss: 2.1639 - val_acc: 0.4562 - val_top5-acc: 0.7540 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.8907 - acc: 0.4879 - top5-acc: 0.7956 - val_loss: 2.1656 - val_acc: 0.4464 - val_top5-acc: 0.7550 - lr: 0.0050\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8903 - acc: 0.4901 - top5-acc: 0.7959 - val_loss: 2.1379 - val_acc: 0.4548 - val_top5-acc: 0.7482 - lr: 0.0050\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8997 - acc: 0.4881 - top5-acc: 0.7944 - val_loss: 2.1397 - val_acc: 0.4526 - val_top5-acc: 0.7590 - lr: 0.0050\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8944 - acc: 0.4908 - top5-acc: 0.7978 - val_loss: 2.1646 - val_acc: 0.4544 - val_top5-acc: 0.7586 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8939 - acc: 0.4912 - top5-acc: 0.7954 - val_loss: 2.2158 - val_acc: 0.4434 - val_top5-acc: 0.7450 - lr: 0.0050\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8637 - acc: 0.4958 - top5-acc: 0.8021 - val_loss: 2.1958 - val_acc: 0.4518 - val_top5-acc: 0.7472 - lr: 0.0050\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8431 - acc: 0.4995 - top5-acc: 0.8062 - val_loss: 2.2129 - val_acc: 0.4516 - val_top5-acc: 0.7492 - lr: 0.0050\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.6646 - acc: 0.5444 - top5-acc: 0.8343 - val_loss: 2.0745 - val_acc: 0.4816 - val_top5-acc: 0.7702 - lr: 0.0025\n",
      "313/313 [==============================] - 15s 46ms/step - loss: 1.9920 - acc: 0.4888 - top5-acc: 0.7873\n",
      "Test accuracy: 48.88%\n",
      "Test top 5 accuracy: 78.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_36_layer_call_fn, layer_normalization_36_layer_call_and_return_conditional_losses, layer_normalization_37_layer_call_fn, layer_normalization_37_layer_call_and_return_conditional_losses, layer_normalization_38_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF100_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF100_2\\assets\n"
     ]
    }
   ],
   "source": [
    "for k in range(2):\n",
    "    mlpmixer_blocks = keras.Sequential(\n",
    "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
    "    )\n",
    "    mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
    "    history,accuracy,top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "    #Saving Results\n",
    "    pwd = 'Results_Article/3A/mlpmixer_CF100_' + str(k+1)\n",
    "    mlpmixer_classifier.save(pwd)\n",
    "    np.save( pwd + '/history.npy',history.history)\n",
    "    with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "        pickle.dump(accuracy,file)\n",
    "    with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "        pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as layer_normalization_48_layer_call_fn, layer_normalization_48_layer_call_and_return_conditional_losses, layer_normalization_49_layer_call_fn, layer_normalization_49_layer_call_and_return_conditional_losses, layer_normalization_50_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_Untrained\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_Untrained\\assets\n"
     ]
    }
   ],
   "source": [
    "mlpmixer_blocks = keras.Sequential(\n",
    "[MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
    ")\n",
    "mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
    "pwd = 'Results_Article/3A/mlpmixer_Untrained'\n",
    "mlpmixer_classifier.save(pwd)\n",
    "#np.save( pwd + '/history_' + str(date) +'.npy',history.history)\n",
    "#with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "#    pickle.dump(accuracy,file)\n",
    "#with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "#    pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "path = 'Results_Article/3A/mlpmixer_'\n",
    "global_models = list()\n",
    "#Call the folder\n",
    "C10_mlpmixer_1 = tf.keras.models.load_model(path + 'CF10_1')\n",
    "global_models.append(C10_mlpmixer_1)\n",
    "C10_mlpmixer_2 = tf.keras.models.load_model(path + 'CF10_2')\n",
    "global_models.append(C10_mlpmixer_2)\n",
    "C100_mlpmixer_1 = tf.keras.models.load_model(path + 'CF100_1')\n",
    "global_models.append(C100_mlpmixer_1)\n",
    "C100_mlpmixer_2 = tf.keras.models.load_model(path + 'CF100_2')\n",
    "global_models.append(C100_mlpmixer_2)\n",
    "Unt_mlpmixer = tf.keras.models.load_model(path + 'Untrained')\n",
    "global_models.append(Unt_mlpmixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.functional.Functional"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(C10_mlpmixer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization before testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def across_datasets(global_models,batch_prepro,type,sigma):\n",
    "    total_activations = list()\n",
    "    plot_raw = list()\n",
    "    plot_total = list()\n",
    "    for k in range(len(global_models)):\n",
    "        tested_model = global_models[k] \n",
    "        ave_mixer_activations = Prom_Mixer_Activations_Blocks(tested_model,batch_prepro)\n",
    "        total_activations.append(ave_mixer_activations)\n",
    "        \n",
    "    for pairs in set:\n",
    "        comp_1 = total_activations[pairs[0]]\n",
    "        comp_2 = total_activations[pairs[1]]\n",
    "        plot_raw = list()\n",
    "        for i in range(num_blocks):\n",
    "            if type == 'rbf':\n",
    "                inter_row = cka(gram_rbf(comp_1[i],sigma),gram_rbf(comp_2[i],sigma))\n",
    "            elif type == 'linear':\n",
    "                inter_row = cka(gram_linear(comp_1[i]),gram_linear(comp_2[i]))\n",
    "            plot_raw.append(inter_row)\n",
    "        plot_total.append(plot_raw)\n",
    "    return plot_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiment on the paper is with the linear type!\n",
    "#######################################################\n",
    "\n",
    "sigma = None\n",
    "type = 'linear'\n",
    "\n",
    "#Pairs of models that are going to be compared according to the order in the matrix\n",
    "set = [[0,1],[2,3],[0,2],[0,4],[2,4]]\n",
    "label_set = ['CIFAR-10 Net vs. CIFAR-10 Net',\n",
    "            'CIFAR-100 Net vs. CIFAR-100 Net',\n",
    "            'CIFAR-10 Net vs. CIFAR-100 Net ',\n",
    "            'CIFAR-10 Net vs. Untrained',\n",
    "            'CIFAR-100 Net vs. Untrained']\n",
    "num_models_set = len(set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested on CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for testing\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), _ = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Once to Avoid Randomness after setting the testing dataset\n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F45167E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F45167E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F6C29A280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021F6C29A280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# The experiment on the paper is with the linear type\n",
    "plot_total_1 = across_datasets(global_models,batch_prepro,type,sigma)\n",
    "with open('Results_Article/3A/plot_total_C10.pkl','wb') as file:\n",
    "    pickle.dump(plot_total_1,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested on CIFAR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for testing\n",
    "\n",
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), _ = keras.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Once to Avoid Randomness after setting the testing dataset\n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiment on the paper is with the linear type\n",
    "plot_total_2 = across_datasets(global_models,batch_prepro,type,sigma)\n",
    "with open('Results_Article/3A/plot_total_C100.pkl','wb') as file:\n",
    "    pickle.dump(plot_total_2,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested on MNIST: (Appendix 6A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for testing\n",
    "\n",
    "#num_classes = 10\n",
    "#input_shape = (28, 28)\n",
    "#(x_train, y_train), _ = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Once to Avoid Randomness after setting the testing dataset\n",
    "#batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_total_3 = across_datasets(global_models,batch_prepro,type,sigma)\n",
    "#with open('Results_Article/6A/plot_total_MNIST.pkl','wb') as file:\n",
    "#    pickle.dump(plot_total_3,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 3: VERIFICATIONS (OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Results_Article/3A'\n",
    "with open(path + '/plot_total_C100.pkl','rb') as file:\n",
    "        tested_plot = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = '_tested_C100'\n",
    "######################################################\n",
    "x = list(range(1,num_blocks+1))\n",
    "for j in range(num_models_set):\n",
    "    plt.plot(x,tested_plot[j], label = label_set[j])\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('CKA ('+ type +')')\n",
    "plt.locator_params(axis='x', nbins=num_blocks)\n",
    "plt.title('Similarity on CIFAR-100')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('Results_Article/3A/Similarity_'+ type + name +'.png') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_image_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
