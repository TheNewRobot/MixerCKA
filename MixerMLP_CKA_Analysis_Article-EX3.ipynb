{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSzgroG_Suq"
   },
   "source": [
    "# Study of Image classification with modern MLP Mixer model and CKA\n",
    "\n",
    "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
    "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U087DdDw_Suy"
   },
   "source": [
    "# Setup for the MLP-Mixer Architecture\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AnTyoluw_Suz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import pickle\n",
    "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
    "from CKA_Google import *\n",
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 : Across Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DAOrIHu_Su2"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1iMiVS7o_Su3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224 = 50176\n",
      "Patch size: 32 X 32 = 1024 \n",
      "Patches per image: 49\n",
      "Elements per patch (3 channels): 3072\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 512 \n",
    "num_epochs = 50\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Selected Architecture: B/32\n",
    "\n",
    "image_size = 224  # We'll resize input images to this size. Square\n",
    "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
    "embedding_dim = 384  # Fixed Embedding Dimension\n",
    "num_blocks = 12\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d_%H-%M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuu2OAS_Su0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Dataset for training \n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "#plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mpnEZH_Su5"
   },
   "source": [
    "## Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ra-bXojQ_Su6"
   },
   "outputs": [],
   "source": [
    "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data. \n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches. \n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks. ## (sequential_82)\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation) \n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpQFU8H__Su8"
   },
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9E1zD2BY_Su8"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        #Negative Log Likelihood = Categorical Cross Entropy\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping regularization callback. \n",
    "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history, accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxeE0cmM_Su9"
   },
   "source": [
    "## Use data augmentation\n",
    "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zh6hFPWX_Su-"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G77cUgiu_Su-"
   },
   "source": [
    "## Implement patch extraction as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYKNktXe_Su_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        #Extract the shape dimension in the position 0 = columns\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Without overlapping, stride horizontally and vertically\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
    "            rates=[1, 1, 1, 1],\n",
    "            #Patches contained in the images are considered, no zero padding\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        #shape[-1], number of colummns, as well as shape[0]\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Patches, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'patch_size' : self.patch_size ,\n",
    "            'num_patches' : self.num_patches\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yehcSS_Su_"
   },
   "source": [
    "## The MLP-Mixer model\n",
    "\n",
    "The MLP-Mixer is an architecture based exclusively on\n",
    "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "1. One applied independently to image patches, which mixes the per-location features.\n",
    "2. The other applied across patches (along channels), which mixes spatial information.\n",
    "\n",
    "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
    "instead of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwg4e2n_SvA"
   },
   "source": [
    "### Implement the MLP-Mixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dT6wVEki_SvA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp2 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super(MLPMixerLayer, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'num_patches' : num_patches,\n",
    "            'embedding_dim' : embedding_dim,\n",
    "            'dropout_rate' : dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUiufq92_SvA"
   },
   "source": [
    "## Build, train, and evaluate the MLP-Mixer model\n",
    "\n",
    "Note that training the model with the current settings on a V100 GPUs\n",
    "takes around 8 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report: Learning Curve\n",
    "def curves(history):\n",
    "    ymax1 = min(history[\"loss\"])\n",
    "    xmax1 = history[\"loss\"].index(ymax1)\n",
    "    ymax2 = min(history[\"val_loss\"])\n",
    "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
    "    plt.title(\"Cross Entropy Loss\")\n",
    "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
    "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Graph accuracy\n",
    "    ymax3 = max(history[\"acc\"])\n",
    "    xmax3 = history[\"acc\"].index(ymax3)\n",
    "    ymax4 = max(history[\"val_acc\"])\n",
    "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
    "    ymax5 = max(history[\"top5-acc\"])\n",
    "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
    "    ymax6 = max(history[\"val_top5-acc\"])\n",
    "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Classification accuracy')\n",
    "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Classification top5-acc')\n",
    "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.suptitle(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain activations + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers + Patches + One dense layer\n",
    "def Preprocessing(num_example):\n",
    "    augmented = data_augmentation(x_train[num_example])\n",
    "    b = Patches(patch_size, num_patches)(augmented)\n",
    "    a = layers.Dense(units=embedding_dim)(b)\n",
    "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
    "def Batch_Preprocessing(batch_size):\n",
    "    #Vector with the number of Sample of the Xtrain\n",
    "    a  = list(range(0,x_train.shape[0]))\n",
    "    b = random.sample(a,batch_size)\n",
    "    batch_regularization = list()\n",
    "    for i in range(0,batch_size):\n",
    "        inter_result = Preprocessing(b[i])\n",
    "        batch_regularization.append(inter_result)\n",
    "    return batch_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_out(result,layer_number,example):\n",
    "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "    ax1.imshow(x_train[example])\n",
    "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
    "    ax2.imshow(result[layer_number])\n",
    "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
    "    #The input is fixed to the beginning of the mlp blocks\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[0]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of layer's activation\n",
    "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
    "def Heatmap(result,type,sigma):\n",
    "    dim = len(result)\n",
    "    k = (dim - 1)\n",
    "    heatmap_CKA = np.zeros((dim,dim))\n",
    "    for i in range(0,dim):\n",
    "        tr = (dim - 1)\n",
    "        for j in range(0,dim):\n",
    "            if type == 'kernel':\n",
    "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
    "            elif type == 'linear':\n",
    "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
    "            else:\n",
    "                print('There is no such category, try again')\n",
    "                break\n",
    "\n",
    "            tr -= 1\n",
    "        k -= 1\n",
    "    #print('CKA' + type + 'calculated')\n",
    "    return heatmap_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of heatmaps (obsolet)\n",
    "def Prom_Mixer_Heatmaps(batch_result,type):\n",
    "    mat_heatmaps = list()\n",
    "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
    "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
    "        mat_heatmaps.append(heatmap_raw)\n",
    "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
    "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
    "    return prom_mixer_heatmap,mat_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 4\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i'%(i+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(3,3))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 : Across datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained with CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 31s 246ms/step - loss: 3.8681 - acc: 0.2619 - top5-acc: 0.7524 - val_loss: 1.6054 - val_acc: 0.4192 - val_top5-acc: 0.8956 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.5740 - acc: 0.4323 - top5-acc: 0.8994 - val_loss: 1.4207 - val_acc: 0.4954 - val_top5-acc: 0.9296 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.4183 - acc: 0.4909 - top5-acc: 0.9256 - val_loss: 1.2759 - val_acc: 0.5454 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3187 - acc: 0.5267 - top5-acc: 0.9369 - val_loss: 1.2024 - val_acc: 0.5760 - val_top5-acc: 0.9536 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2663 - acc: 0.5495 - top5-acc: 0.9440 - val_loss: 1.2455 - val_acc: 0.5678 - val_top5-acc: 0.9470 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2289 - acc: 0.5617 - top5-acc: 0.9460 - val_loss: 1.1697 - val_acc: 0.5804 - val_top5-acc: 0.9580 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1758 - acc: 0.5804 - top5-acc: 0.9528 - val_loss: 1.0592 - val_acc: 0.6244 - val_top5-acc: 0.9666 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1559 - acc: 0.5864 - top5-acc: 0.9559 - val_loss: 1.0459 - val_acc: 0.6322 - val_top5-acc: 0.9652 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1434 - acc: 0.5933 - top5-acc: 0.9549 - val_loss: 1.0175 - val_acc: 0.6412 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0956 - acc: 0.6132 - top5-acc: 0.9597 - val_loss: 1.0427 - val_acc: 0.6390 - val_top5-acc: 0.9658 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0842 - acc: 0.6161 - top5-acc: 0.9614 - val_loss: 1.0711 - val_acc: 0.6308 - val_top5-acc: 0.9618 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0519 - acc: 0.6271 - top5-acc: 0.9638 - val_loss: 1.0811 - val_acc: 0.6264 - val_top5-acc: 0.9648 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0483 - acc: 0.6292 - top5-acc: 0.9640 - val_loss: 1.0651 - val_acc: 0.6332 - val_top5-acc: 0.9676 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0202 - acc: 0.6387 - top5-acc: 0.9662 - val_loss: 0.9823 - val_acc: 0.6542 - val_top5-acc: 0.9708 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0110 - acc: 0.6442 - top5-acc: 0.9675 - val_loss: 0.9603 - val_acc: 0.6690 - val_top5-acc: 0.9684 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9866 - acc: 0.6515 - top5-acc: 0.9683 - val_loss: 0.9427 - val_acc: 0.6718 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.9791 - acc: 0.6555 - top5-acc: 0.9693 - val_loss: 0.9224 - val_acc: 0.6852 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9493 - acc: 0.6667 - top5-acc: 0.9720 - val_loss: 0.9211 - val_acc: 0.6836 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9479 - acc: 0.6659 - top5-acc: 0.9713 - val_loss: 0.9081 - val_acc: 0.6780 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9263 - acc: 0.6711 - top5-acc: 0.9730 - val_loss: 0.9277 - val_acc: 0.6838 - val_top5-acc: 0.9740 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9244 - acc: 0.6751 - top5-acc: 0.9740 - val_loss: 0.8645 - val_acc: 0.6984 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.9010 - acc: 0.6802 - top5-acc: 0.9746 - val_loss: 0.8358 - val_acc: 0.7122 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8939 - acc: 0.6861 - top5-acc: 0.9754 - val_loss: 1.0092 - val_acc: 0.6542 - val_top5-acc: 0.9738 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.9002 - acc: 0.6843 - top5-acc: 0.9749 - val_loss: 0.8082 - val_acc: 0.7172 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8661 - acc: 0.6959 - top5-acc: 0.9773 - val_loss: 0.8257 - val_acc: 0.7168 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8563 - acc: 0.6979 - top5-acc: 0.9778 - val_loss: 0.8311 - val_acc: 0.7110 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8518 - acc: 0.7023 - top5-acc: 0.9776 - val_loss: 0.8711 - val_acc: 0.6986 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8460 - acc: 0.7033 - top5-acc: 0.9789 - val_loss: 0.8316 - val_acc: 0.7208 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8321 - acc: 0.7095 - top5-acc: 0.9790 - val_loss: 0.8072 - val_acc: 0.7258 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 0.8263 - acc: 0.7088 - top5-acc: 0.9796 - val_loss: 0.8294 - val_acc: 0.7108 - val_top5-acc: 0.9816 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 0.9046 - acc: 0.6930 - top5-acc: 0.9745 - val_loss: 1.8384 - val_acc: 0.5700 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 15.7246 - acc: 0.3280 - top5-acc: 0.7901 - val_loss: 1.3734 - val_acc: 0.5108 - val_top5-acc: 0.9398 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 1.3120 - acc: 0.5282 - top5-acc: 0.9405 - val_loss: 1.1392 - val_acc: 0.5898 - val_top5-acc: 0.9598 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.1582 - acc: 0.5897 - top5-acc: 0.9550 - val_loss: 1.0224 - val_acc: 0.6380 - val_top5-acc: 0.9676 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0699 - acc: 0.6198 - top5-acc: 0.9623 - val_loss: 0.9793 - val_acc: 0.6540 - val_top5-acc: 0.9698 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0202 - acc: 0.6416 - top5-acc: 0.9653 - val_loss: 0.9579 - val_acc: 0.6644 - val_top5-acc: 0.9730 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.9812 - acc: 0.6528 - top5-acc: 0.9695 - val_loss: 0.9101 - val_acc: 0.6786 - val_top5-acc: 0.9734 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9538 - acc: 0.6629 - top5-acc: 0.9712 - val_loss: 0.8830 - val_acc: 0.6910 - val_top5-acc: 0.9752 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.9231 - acc: 0.6752 - top5-acc: 0.9730 - val_loss: 0.8669 - val_acc: 0.6946 - val_top5-acc: 0.9800 - lr: 0.0025\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 0.8321 - acc: 0.7119 - top5-acc: 0.9777\n",
      "Test accuracy: 71.19%\n",
      "Test top 5 accuracy: 97.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn, layer_normalization_1_layer_call_and_return_conditional_losses, layer_normalization_2_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF10_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF10_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 245ms/step - loss: 4.3536 - acc: 0.2489 - top5-acc: 0.7437 - val_loss: 1.7261 - val_acc: 0.4014 - val_top5-acc: 0.8672 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.5719 - acc: 0.4315 - top5-acc: 0.9015 - val_loss: 1.4193 - val_acc: 0.4880 - val_top5-acc: 0.9294 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.4415 - acc: 0.4818 - top5-acc: 0.9225 - val_loss: 1.3099 - val_acc: 0.5272 - val_top5-acc: 0.9404 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.3526 - acc: 0.5144 - top5-acc: 0.9355 - val_loss: 1.2562 - val_acc: 0.5458 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2928 - acc: 0.5332 - top5-acc: 0.9404 - val_loss: 1.2389 - val_acc: 0.5626 - val_top5-acc: 0.9464 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2652 - acc: 0.5459 - top5-acc: 0.9444 - val_loss: 1.1388 - val_acc: 0.5948 - val_top5-acc: 0.9580 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2085 - acc: 0.5677 - top5-acc: 0.9507 - val_loss: 1.1449 - val_acc: 0.5958 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1848 - acc: 0.5787 - top5-acc: 0.9523 - val_loss: 1.1353 - val_acc: 0.6022 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1587 - acc: 0.5878 - top5-acc: 0.9561 - val_loss: 1.0726 - val_acc: 0.6160 - val_top5-acc: 0.9618 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1230 - acc: 0.5986 - top5-acc: 0.9593 - val_loss: 1.0823 - val_acc: 0.6066 - val_top5-acc: 0.9656 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0952 - acc: 0.6128 - top5-acc: 0.9610 - val_loss: 1.0346 - val_acc: 0.6322 - val_top5-acc: 0.9658 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0834 - acc: 0.6166 - top5-acc: 0.9618 - val_loss: 1.0831 - val_acc: 0.6200 - val_top5-acc: 0.9640 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0740 - acc: 0.6203 - top5-acc: 0.9626 - val_loss: 1.0317 - val_acc: 0.6434 - val_top5-acc: 0.9672 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0487 - acc: 0.6295 - top5-acc: 0.9644 - val_loss: 0.9975 - val_acc: 0.6492 - val_top5-acc: 0.9684 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0382 - acc: 0.6323 - top5-acc: 0.9658 - val_loss: 1.0004 - val_acc: 0.6462 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0163 - acc: 0.6402 - top5-acc: 0.9661 - val_loss: 0.9707 - val_acc: 0.6556 - val_top5-acc: 0.9738 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9934 - acc: 0.6486 - top5-acc: 0.9692 - val_loss: 0.9659 - val_acc: 0.6606 - val_top5-acc: 0.9732 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9836 - acc: 0.6534 - top5-acc: 0.9695 - val_loss: 0.9534 - val_acc: 0.6658 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9646 - acc: 0.6583 - top5-acc: 0.9710 - val_loss: 0.9527 - val_acc: 0.6684 - val_top5-acc: 0.9700 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9675 - acc: 0.6586 - top5-acc: 0.9709 - val_loss: 0.9119 - val_acc: 0.6808 - val_top5-acc: 0.9762 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9256 - acc: 0.6740 - top5-acc: 0.9741 - val_loss: 0.8772 - val_acc: 0.7036 - val_top5-acc: 0.9760 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9286 - acc: 0.6719 - top5-acc: 0.9727 - val_loss: 0.9030 - val_acc: 0.6830 - val_top5-acc: 0.9760 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9207 - acc: 0.6733 - top5-acc: 0.9737 - val_loss: 0.8749 - val_acc: 0.6948 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9235 - acc: 0.6756 - top5-acc: 0.9740 - val_loss: 0.9238 - val_acc: 0.6858 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9032 - acc: 0.6820 - top5-acc: 0.9754 - val_loss: 0.8508 - val_acc: 0.7008 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8870 - acc: 0.6856 - top5-acc: 0.9769 - val_loss: 0.8738 - val_acc: 0.6952 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8767 - acc: 0.6918 - top5-acc: 0.9774 - val_loss: 0.8769 - val_acc: 0.6964 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9165 - acc: 0.6797 - top5-acc: 0.9734 - val_loss: 0.8867 - val_acc: 0.6898 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8673 - acc: 0.6931 - top5-acc: 0.9770 - val_loss: 0.8566 - val_acc: 0.7056 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8396 - acc: 0.7051 - top5-acc: 0.9788 - val_loss: 0.8222 - val_acc: 0.7110 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8486 - acc: 0.7025 - top5-acc: 0.9780 - val_loss: 0.8335 - val_acc: 0.7150 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8512 - acc: 0.7003 - top5-acc: 0.9788 - val_loss: 0.8256 - val_acc: 0.7150 - val_top5-acc: 0.9834 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8266 - acc: 0.7068 - top5-acc: 0.9796 - val_loss: 0.8423 - val_acc: 0.7178 - val_top5-acc: 0.9796 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8117 - acc: 0.7159 - top5-acc: 0.9797 - val_loss: 0.8417 - val_acc: 0.7078 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8112 - acc: 0.7150 - top5-acc: 0.9813 - val_loss: 0.8340 - val_acc: 0.7144 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7221 - acc: 0.7449 - top5-acc: 0.9856 - val_loss: 0.7299 - val_acc: 0.7474 - val_top5-acc: 0.9844 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6955 - acc: 0.7531 - top5-acc: 0.9859 - val_loss: 0.7454 - val_acc: 0.7468 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6869 - acc: 0.7581 - top5-acc: 0.9869 - val_loss: 0.7652 - val_acc: 0.7426 - val_top5-acc: 0.9842 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6847 - acc: 0.7570 - top5-acc: 0.9868 - val_loss: 0.7499 - val_acc: 0.7448 - val_top5-acc: 0.9872 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6794 - acc: 0.7600 - top5-acc: 0.9868 - val_loss: 0.7781 - val_acc: 0.7344 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6785 - acc: 0.7632 - top5-acc: 0.9866 - val_loss: 0.7569 - val_acc: 0.7400 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6302 - acc: 0.7767 - top5-acc: 0.9895 - val_loss: 0.7043 - val_acc: 0.7640 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6110 - acc: 0.7844 - top5-acc: 0.9899 - val_loss: 0.7235 - val_acc: 0.7554 - val_top5-acc: 0.9852 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6033 - acc: 0.7864 - top5-acc: 0.9899 - val_loss: 0.7167 - val_acc: 0.7624 - val_top5-acc: 0.9850 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.5936 - acc: 0.7920 - top5-acc: 0.9908 - val_loss: 0.7264 - val_acc: 0.7576 - val_top5-acc: 0.9866 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5889 - acc: 0.7916 - top5-acc: 0.9906 - val_loss: 0.7139 - val_acc: 0.7632 - val_top5-acc: 0.9866 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5890 - acc: 0.7932 - top5-acc: 0.9907 - val_loss: 0.7192 - val_acc: 0.7590 - val_top5-acc: 0.9852 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5587 - acc: 0.8036 - top5-acc: 0.9922 - val_loss: 0.6885 - val_acc: 0.7638 - val_top5-acc: 0.9864 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5431 - acc: 0.8084 - top5-acc: 0.9920 - val_loss: 0.7007 - val_acc: 0.7642 - val_top5-acc: 0.9864 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5474 - acc: 0.8068 - top5-acc: 0.9919 - val_loss: 0.7115 - val_acc: 0.7676 - val_top5-acc: 0.9864 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 0.7342 - acc: 0.7596 - top5-acc: 0.9841\n",
      "Test accuracy: 75.96%\n",
      "Test top 5 accuracy: 98.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_12_layer_call_fn, layer_normalization_12_layer_call_and_return_conditional_losses, layer_normalization_13_layer_call_fn, layer_normalization_13_layer_call_and_return_conditional_losses, layer_normalization_14_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF10_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF10_2\\assets\n"
     ]
    }
   ],
   "source": [
    "for k in range(2):\n",
    "    mlpmixer_blocks = keras.Sequential(\n",
    "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
    "    )\n",
    "    mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
    "    history,accuracy,top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "    #Saving Results\n",
    "    #pwd = 'Results_Article/3A/mlpmixer_' + str(date) + '_CF10_' + str(k+1)\n",
    "    pwd = 'Results_Article/3A/mlpmixer_CF10_' + str(k+1)\n",
    "    mlpmixer_classifier.save(pwd)\n",
    "    np.save( pwd + '/history.npy',history.history)\n",
    "    with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "        pickle.dump(accuracy,file)\n",
    "    with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "        pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained with CIFAR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for training \n",
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 246ms/step - loss: 5.9549 - acc: 0.0449 - top5-acc: 0.1633 - val_loss: 4.2852 - val_acc: 0.0838 - val_top5-acc: 0.2530 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 3.7323 - acc: 0.1265 - top5-acc: 0.3540 - val_loss: 3.6955 - val_acc: 0.1658 - val_top5-acc: 0.4130 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 3.4536 - acc: 0.1731 - top5-acc: 0.4380 - val_loss: 3.3434 - val_acc: 0.2104 - val_top5-acc: 0.4864 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 3.2946 - acc: 0.2037 - top5-acc: 0.4863 - val_loss: 3.2993 - val_acc: 0.2238 - val_top5-acc: 0.5006 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 3.1404 - acc: 0.2314 - top5-acc: 0.5237 - val_loss: 3.1267 - val_acc: 0.2480 - val_top5-acc: 0.5400 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 3.0393 - acc: 0.2514 - top5-acc: 0.5505 - val_loss: 2.9957 - val_acc: 0.2750 - val_top5-acc: 0.5712 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.9231 - acc: 0.2731 - top5-acc: 0.5757 - val_loss: 2.9300 - val_acc: 0.2866 - val_top5-acc: 0.5782 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.8628 - acc: 0.2847 - top5-acc: 0.5926 - val_loss: 2.7857 - val_acc: 0.3114 - val_top5-acc: 0.6116 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 2.7959 - acc: 0.3016 - top5-acc: 0.6072 - val_loss: 2.7675 - val_acc: 0.3110 - val_top5-acc: 0.6220 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.7211 - acc: 0.3141 - top5-acc: 0.6252 - val_loss: 2.7343 - val_acc: 0.3272 - val_top5-acc: 0.6254 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.6662 - acc: 0.3241 - top5-acc: 0.6406 - val_loss: 2.6346 - val_acc: 0.3410 - val_top5-acc: 0.6486 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.6222 - acc: 0.3356 - top5-acc: 0.6485 - val_loss: 2.6095 - val_acc: 0.3418 - val_top5-acc: 0.6546 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.5867 - acc: 0.3430 - top5-acc: 0.6542 - val_loss: 2.5997 - val_acc: 0.3460 - val_top5-acc: 0.6622 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.5491 - acc: 0.3521 - top5-acc: 0.6626 - val_loss: 2.5707 - val_acc: 0.3560 - val_top5-acc: 0.6688 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 2.4828 - acc: 0.3652 - top5-acc: 0.6785 - val_loss: 2.5136 - val_acc: 0.3728 - val_top5-acc: 0.6738 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.4537 - acc: 0.3715 - top5-acc: 0.6853 - val_loss: 2.5332 - val_acc: 0.3588 - val_top5-acc: 0.6696 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 2.4133 - acc: 0.3804 - top5-acc: 0.6938 - val_loss: 2.4975 - val_acc: 0.3752 - val_top5-acc: 0.6860 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.4194 - acc: 0.3797 - top5-acc: 0.6935 - val_loss: 2.3784 - val_acc: 0.3934 - val_top5-acc: 0.7042 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.4186 - acc: 0.3797 - top5-acc: 0.6970 - val_loss: 2.3481 - val_acc: 0.4074 - val_top5-acc: 0.7182 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.3543 - acc: 0.3897 - top5-acc: 0.7066 - val_loss: 2.3777 - val_acc: 0.4056 - val_top5-acc: 0.7018 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.3174 - acc: 0.3987 - top5-acc: 0.7154 - val_loss: 2.4667 - val_acc: 0.3942 - val_top5-acc: 0.7012 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 2.3466 - acc: 0.3993 - top5-acc: 0.7108 - val_loss: 2.5206 - val_acc: 0.3846 - val_top5-acc: 0.6922 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 7.1884 - acc: 0.2268 - top5-acc: 0.4844 - val_loss: 3.3004 - val_acc: 0.2892 - val_top5-acc: 0.6054 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.4327 - acc: 0.3864 - top5-acc: 0.6986 - val_loss: 2.3076 - val_acc: 0.4126 - val_top5-acc: 0.7240 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.1611 - acc: 0.4333 - top5-acc: 0.7439 - val_loss: 2.2488 - val_acc: 0.4318 - val_top5-acc: 0.7332 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.0844 - acc: 0.4495 - top5-acc: 0.7567 - val_loss: 2.2129 - val_acc: 0.4410 - val_top5-acc: 0.7430 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.0374 - acc: 0.4629 - top5-acc: 0.7671 - val_loss: 2.2133 - val_acc: 0.4378 - val_top5-acc: 0.7406 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 2.0106 - acc: 0.4649 - top5-acc: 0.7726 - val_loss: 2.2302 - val_acc: 0.4330 - val_top5-acc: 0.7388 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9915 - acc: 0.4686 - top5-acc: 0.7748 - val_loss: 2.2031 - val_acc: 0.4394 - val_top5-acc: 0.7508 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9827 - acc: 0.4701 - top5-acc: 0.7789 - val_loss: 2.1750 - val_acc: 0.4480 - val_top5-acc: 0.7416 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.9661 - acc: 0.4743 - top5-acc: 0.7818 - val_loss: 2.1746 - val_acc: 0.4478 - val_top5-acc: 0.7496 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.9430 - acc: 0.4797 - top5-acc: 0.7857 - val_loss: 2.1919 - val_acc: 0.4406 - val_top5-acc: 0.7452 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.9289 - acc: 0.4805 - top5-acc: 0.7880 - val_loss: 2.1991 - val_acc: 0.4468 - val_top5-acc: 0.7502 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9222 - acc: 0.4834 - top5-acc: 0.7895 - val_loss: 2.1801 - val_acc: 0.4450 - val_top5-acc: 0.7508 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9085 - acc: 0.4859 - top5-acc: 0.7908 - val_loss: 2.1602 - val_acc: 0.4488 - val_top5-acc: 0.7540 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8916 - acc: 0.4909 - top5-acc: 0.7927 - val_loss: 2.2080 - val_acc: 0.4506 - val_top5-acc: 0.7464 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8915 - acc: 0.4881 - top5-acc: 0.7949 - val_loss: 2.1739 - val_acc: 0.4582 - val_top5-acc: 0.7514 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8951 - acc: 0.4888 - top5-acc: 0.7958 - val_loss: 2.1560 - val_acc: 0.4492 - val_top5-acc: 0.7580 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.8791 - acc: 0.4924 - top5-acc: 0.7993 - val_loss: 2.1442 - val_acc: 0.4538 - val_top5-acc: 0.7566 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.8781 - acc: 0.4920 - top5-acc: 0.7987 - val_loss: 2.1865 - val_acc: 0.4436 - val_top5-acc: 0.7528 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.8804 - acc: 0.4936 - top5-acc: 0.8002 - val_loss: 2.1744 - val_acc: 0.4544 - val_top5-acc: 0.7510 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.8637 - acc: 0.4966 - top5-acc: 0.8024 - val_loss: 2.1724 - val_acc: 0.4574 - val_top5-acc: 0.7612 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.8551 - acc: 0.4965 - top5-acc: 0.8031 - val_loss: 2.1367 - val_acc: 0.4598 - val_top5-acc: 0.7628 - lr: 0.0050\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8532 - acc: 0.4996 - top5-acc: 0.8031 - val_loss: 2.1572 - val_acc: 0.4564 - val_top5-acc: 0.7604 - lr: 0.0050\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.8320 - acc: 0.5024 - top5-acc: 0.8088 - val_loss: 2.2362 - val_acc: 0.4374 - val_top5-acc: 0.7448 - lr: 0.0050\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8670 - acc: 0.4964 - top5-acc: 0.8049 - val_loss: 2.1778 - val_acc: 0.4462 - val_top5-acc: 0.7608 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8298 - acc: 0.5050 - top5-acc: 0.8088 - val_loss: 2.1879 - val_acc: 0.4530 - val_top5-acc: 0.7596 - lr: 0.0050\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8112 - acc: 0.5093 - top5-acc: 0.8122 - val_loss: 2.1490 - val_acc: 0.4660 - val_top5-acc: 0.7634 - lr: 0.0050\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.5983 - acc: 0.5562 - top5-acc: 0.8457 - val_loss: 2.0746 - val_acc: 0.4800 - val_top5-acc: 0.7750 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.5504 - acc: 0.5706 - top5-acc: 0.8535 - val_loss: 2.0322 - val_acc: 0.4876 - val_top5-acc: 0.7780 - lr: 0.0025\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 1.9821 - acc: 0.5021 - top5-acc: 0.7815\n",
      "Test accuracy: 50.21%\n",
      "Test top 5 accuracy: 78.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_24_layer_call_fn, layer_normalization_24_layer_call_and_return_conditional_losses, layer_normalization_25_layer_call_fn, layer_normalization_25_layer_call_and_return_conditional_losses, layer_normalization_26_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF100_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF100_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 245ms/step - loss: 5.9199 - acc: 0.0468 - top5-acc: 0.1671 - val_loss: 4.1968 - val_acc: 0.0962 - val_top5-acc: 0.2720 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.8033 - acc: 0.1177 - top5-acc: 0.3373 - val_loss: 3.8455 - val_acc: 0.1546 - val_top5-acc: 0.3812 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.4949 - acc: 0.1661 - top5-acc: 0.4259 - val_loss: 3.3983 - val_acc: 0.1970 - val_top5-acc: 0.4694 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.3274 - acc: 0.1960 - top5-acc: 0.4721 - val_loss: 3.1773 - val_acc: 0.2250 - val_top5-acc: 0.5188 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 3.1749 - acc: 0.2259 - top5-acc: 0.5163 - val_loss: 3.1870 - val_acc: 0.2386 - val_top5-acc: 0.5322 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 3.0251 - acc: 0.2528 - top5-acc: 0.5520 - val_loss: 2.9310 - val_acc: 0.2774 - val_top5-acc: 0.5770 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.9277 - acc: 0.2722 - top5-acc: 0.5766 - val_loss: 2.9440 - val_acc: 0.2804 - val_top5-acc: 0.5834 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.8506 - acc: 0.2892 - top5-acc: 0.5946 - val_loss: 2.7712 - val_acc: 0.3172 - val_top5-acc: 0.6226 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.7620 - acc: 0.3077 - top5-acc: 0.6165 - val_loss: 2.6840 - val_acc: 0.3300 - val_top5-acc: 0.6262 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.7211 - acc: 0.3145 - top5-acc: 0.6230 - val_loss: 2.7655 - val_acc: 0.3138 - val_top5-acc: 0.6320 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.6627 - acc: 0.3277 - top5-acc: 0.6385 - val_loss: 2.6330 - val_acc: 0.3390 - val_top5-acc: 0.6500 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.6299 - acc: 0.3331 - top5-acc: 0.6475 - val_loss: 2.6166 - val_acc: 0.3472 - val_top5-acc: 0.6546 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.5835 - acc: 0.3445 - top5-acc: 0.6552 - val_loss: 2.5612 - val_acc: 0.3544 - val_top5-acc: 0.6670 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.5482 - acc: 0.3505 - top5-acc: 0.6661 - val_loss: 2.5159 - val_acc: 0.3618 - val_top5-acc: 0.6762 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.5139 - acc: 0.3577 - top5-acc: 0.6706 - val_loss: 2.5472 - val_acc: 0.3540 - val_top5-acc: 0.6724 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.4971 - acc: 0.3627 - top5-acc: 0.6777 - val_loss: 2.5715 - val_acc: 0.3570 - val_top5-acc: 0.6768 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.4261 - acc: 0.3761 - top5-acc: 0.6914 - val_loss: 2.4366 - val_acc: 0.3774 - val_top5-acc: 0.7006 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 13.3189 - acc: 0.2817 - top5-acc: 0.5387 - val_loss: 25.9403 - val_acc: 0.0462 - val_top5-acc: 0.1560 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 9.0909 - acc: 0.1524 - top5-acc: 0.3685 - val_loss: 3.0671 - val_acc: 0.2648 - val_top5-acc: 0.5554 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 2.8983 - acc: 0.2834 - top5-acc: 0.5858 - val_loss: 2.7410 - val_acc: 0.3200 - val_top5-acc: 0.6340 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.6704 - acc: 0.3254 - top5-acc: 0.6381 - val_loss: 2.6160 - val_acc: 0.3402 - val_top5-acc: 0.6580 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.5296 - acc: 0.3572 - top5-acc: 0.6689 - val_loss: 2.5551 - val_acc: 0.3594 - val_top5-acc: 0.6760 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.4031 - acc: 0.3813 - top5-acc: 0.6933 - val_loss: 2.4485 - val_acc: 0.3822 - val_top5-acc: 0.6968 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.3464 - acc: 0.3927 - top5-acc: 0.7072 - val_loss: 2.3920 - val_acc: 0.3914 - val_top5-acc: 0.7044 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.3058 - acc: 0.4002 - top5-acc: 0.7142 - val_loss: 2.3691 - val_acc: 0.3974 - val_top5-acc: 0.7044 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.2597 - acc: 0.4117 - top5-acc: 0.7244 - val_loss: 2.3404 - val_acc: 0.3994 - val_top5-acc: 0.7164 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.2293 - acc: 0.4169 - top5-acc: 0.7290 - val_loss: 2.3159 - val_acc: 0.4090 - val_top5-acc: 0.7184 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.1967 - acc: 0.4244 - top5-acc: 0.7366 - val_loss: 2.3067 - val_acc: 0.4112 - val_top5-acc: 0.7200 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.1649 - acc: 0.4311 - top5-acc: 0.7437 - val_loss: 2.2883 - val_acc: 0.4152 - val_top5-acc: 0.7270 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.1419 - acc: 0.4348 - top5-acc: 0.7498 - val_loss: 2.2560 - val_acc: 0.4230 - val_top5-acc: 0.7270 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.1186 - acc: 0.4424 - top5-acc: 0.7521 - val_loss: 2.2558 - val_acc: 0.4182 - val_top5-acc: 0.7288 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.0894 - acc: 0.4465 - top5-acc: 0.7576 - val_loss: 2.2164 - val_acc: 0.4320 - val_top5-acc: 0.7362 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.0694 - acc: 0.4533 - top5-acc: 0.7614 - val_loss: 2.2105 - val_acc: 0.4314 - val_top5-acc: 0.7392 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 2.0525 - acc: 0.4564 - top5-acc: 0.7654 - val_loss: 2.2056 - val_acc: 0.4350 - val_top5-acc: 0.7440 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 2.0350 - acc: 0.4576 - top5-acc: 0.7704 - val_loss: 2.1851 - val_acc: 0.4330 - val_top5-acc: 0.7408 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 2.0145 - acc: 0.4628 - top5-acc: 0.7710 - val_loss: 2.1971 - val_acc: 0.4290 - val_top5-acc: 0.7392 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 2.0006 - acc: 0.4668 - top5-acc: 0.7720 - val_loss: 2.1543 - val_acc: 0.4440 - val_top5-acc: 0.7480 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.9743 - acc: 0.4720 - top5-acc: 0.7796 - val_loss: 2.1729 - val_acc: 0.4414 - val_top5-acc: 0.7506 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.9777 - acc: 0.4732 - top5-acc: 0.7802 - val_loss: 2.1569 - val_acc: 0.4438 - val_top5-acc: 0.7556 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 1.9495 - acc: 0.4798 - top5-acc: 0.7842 - val_loss: 2.1378 - val_acc: 0.4460 - val_top5-acc: 0.7508 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.9401 - acc: 0.4794 - top5-acc: 0.7854 - val_loss: 2.1269 - val_acc: 0.4512 - val_top5-acc: 0.7482 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.9167 - acc: 0.4840 - top5-acc: 0.7910 - val_loss: 2.1487 - val_acc: 0.4522 - val_top5-acc: 0.7460 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.9158 - acc: 0.4865 - top5-acc: 0.7892 - val_loss: 2.1334 - val_acc: 0.4562 - val_top5-acc: 0.7520 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8917 - acc: 0.4923 - top5-acc: 0.7962 - val_loss: 2.1546 - val_acc: 0.4496 - val_top5-acc: 0.7542 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8985 - acc: 0.4845 - top5-acc: 0.7950 - val_loss: 2.1273 - val_acc: 0.4492 - val_top5-acc: 0.7540 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.8888 - acc: 0.4920 - top5-acc: 0.7947 - val_loss: 2.1287 - val_acc: 0.4514 - val_top5-acc: 0.7536 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.7778 - acc: 0.5180 - top5-acc: 0.8149 - val_loss: 2.0581 - val_acc: 0.4704 - val_top5-acc: 0.7698 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.7528 - acc: 0.5224 - top5-acc: 0.8193 - val_loss: 2.0621 - val_acc: 0.4722 - val_top5-acc: 0.7672 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.7419 - acc: 0.5242 - top5-acc: 0.8198 - val_loss: 2.0626 - val_acc: 0.4752 - val_top5-acc: 0.7630 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.7291 - acc: 0.5286 - top5-acc: 0.8215 - val_loss: 2.0697 - val_acc: 0.4748 - val_top5-acc: 0.7656 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 1.9954 - acc: 0.4830 - top5-acc: 0.7766\n",
      "Test accuracy: 48.3%\n",
      "Test top 5 accuracy: 77.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_36_layer_call_fn, layer_normalization_36_layer_call_and_return_conditional_losses, layer_normalization_37_layer_call_fn, layer_normalization_37_layer_call_and_return_conditional_losses, layer_normalization_38_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF100_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_CF100_2\\assets\n"
     ]
    }
   ],
   "source": [
    "for k in range(2):\n",
    "    mlpmixer_blocks = keras.Sequential(\n",
    "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
    "    )\n",
    "    mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
    "    history,accuracy,top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "    #Saving Results\n",
    "    pwd = 'Results_Article/3A/mlpmixer_CF100_' + str(k+1)\n",
    "    mlpmixer_classifier.save(pwd)\n",
    "    np.save( pwd + '/history_' + str(date) +'.npy',history.history)\n",
    "    with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "        pickle.dump(accuracy,file)\n",
    "    with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "        pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as layer_normalization_48_layer_call_fn, layer_normalization_48_layer_call_and_return_conditional_losses, layer_normalization_49_layer_call_fn, layer_normalization_49_layer_call_and_return_conditional_losses, layer_normalization_50_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_Untrained\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/3A/mlpmixer_Untrained\\assets\n"
     ]
    }
   ],
   "source": [
    "mlpmixer_blocks = keras.Sequential(\n",
    "[MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
    ")\n",
    "mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
    "pwd = 'Results_Article/3A/mlpmixer_Untrained'\n",
    "mlpmixer_classifier.save(pwd)\n",
    "#np.save( pwd + '/history_' + str(date) +'.npy',history.history)\n",
    "#with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "#    pickle.dump(accuracy,file)\n",
    "#with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "#    pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "path = 'Results_Article/3A/mlpmixer_'\n",
    "global_models = list()\n",
    "#Call the folder\n",
    "C10_mlpmixer_1 = tf.keras.models.load_model(path + 'CF10_1')\n",
    "global_models.append(C10_mlpmixer_1)\n",
    "C10_mlpmixer_2 = tf.keras.models.load_model(path + 'CF10_2')\n",
    "global_models.append(C10_mlpmixer_2)\n",
    "C100_mlpmixer_1 = tf.keras.models.load_model(path + 'CF100_1')\n",
    "global_models.append(C100_mlpmixer_1)\n",
    "C100_mlpmixer_2 = tf.keras.models.load_model(path + 'CF100_2')\n",
    "global_models.append(C100_mlpmixer_2)\n",
    "Unt_mlpmixer = tf.keras.models.load_model(path + 'Untrained')\n",
    "global_models.append(Unt_mlpmixer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization before testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def across_datasets(global_models,batch_prepro,type,sigma):\n",
    "    total_activations = list()\n",
    "    plot_raw = list()\n",
    "    plot_total = list()\n",
    "    for k in range(len(global_models)):\n",
    "        tested_model = global_models[k] \n",
    "        ave_mixer_activations = Prom_Mixer_Activations_Blocks(tested_model,batch_prepro)\n",
    "        total_activations.append(ave_mixer_activations)\n",
    "        \n",
    "    for pairs in set:\n",
    "        comp_1 = total_activations[pairs[0]]\n",
    "        comp_2 = total_activations[pairs[1]]\n",
    "        plot_raw = list()\n",
    "        for i in range(num_blocks):\n",
    "            if type == 'rbf':\n",
    "                inter_row = cka(gram_rbf(comp_1[i],sigma),gram_rbf(comp_2[i],sigma))\n",
    "            elif type == 'linear':\n",
    "                inter_row = cka(gram_linear(comp_1[i]),gram_linear(comp_2[i]))\n",
    "            plot_raw.append(inter_row)\n",
    "        plot_total.append(plot_raw)\n",
    "    return plot_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiment on the paper is with the linear type!\n",
    "#######################################################\n",
    "\n",
    "sigma = None\n",
    "type = 'linear'\n",
    "\n",
    "#Pairs of models that are going to be compared according to the order in the matrix\n",
    "set = [[0,1],[2,3],[0,2],[0,4],[2,4]]\n",
    "label_set = ['CIFAR-10 Net vs. CIFAR-10 Net',\n",
    "            'CIFAR-100 Net vs. CIFAR-100 Net',\n",
    "            'CIFAR-10 Net vs. CIFAR-100 Net ',\n",
    "            'CIFAR-10 Net vs. Untrained',\n",
    "            'CIFAR-100 Net vs. Untrained']\n",
    "num_models_set = len(set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested on CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for testing\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), _ = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Once to Avoid Randomness after setting the testing dataset\n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001962643F040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001962643F040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001962643F5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001962643F5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# The experiment on the paper is with the linear type\n",
    "plot_total_1 = across_datasets(global_models,batch_prepro,type,sigma)\n",
    "with open('Results_Article/3A/plot_total_C10.pkl','wb') as file:\n",
    "    pickle.dump(plot_total_1,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested on CIFAR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for testing\n",
    "\n",
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), _ = keras.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Once to Avoid Randomness after setting the testing dataset\n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiment on the paper is with the linear type\n",
    "plot_total_2 = across_datasets(global_models,batch_prepro,type,sigma)\n",
    "with open('Results_Article/3A/plot_total_C100.pkl','wb') as file:\n",
    "    pickle.dump(plot_total_2,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested on MNIST: (Appendix 6A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for testing\n",
    "\n",
    "#num_classes = 10\n",
    "#input_shape = (28, 28)\n",
    "#(x_train, y_train), _ = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Once to Avoid Randomness after setting the testing dataset\n",
    "#batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_total_3 = across_datasets(global_models,batch_prepro,type,sigma)\n",
    "#with open('Results_Article/6A/plot_total_MNIST.pkl','wb') as file:\n",
    "#    pickle.dump(plot_total_3,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 3: VERIFICATIONS (OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tested_plot = plot_total_1\n",
    "name = '_tested_C10'\n",
    "######################################################\n",
    "x = list(range(1,num_blocks+1))\n",
    "for j in range(num_models_set):\n",
    "    plt.plot(x,tested_plot[j], label = label_set[j])\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('CKA ('+ type +')')\n",
    "plt.locator_params(axis='x', nbins=num_blocks)\n",
    "plt.title('Similarity on CIFAR-10')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('Results_Article/3A/Similarity_'+ type + name +'.png') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_image_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
