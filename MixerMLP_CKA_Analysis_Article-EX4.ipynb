{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSzgroG_Suq"
   },
   "source": [
    "# Study of Image classification with modern MLP Mixer model and CKA\n",
    "\n",
    "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
    "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U087DdDw_Suy"
   },
   "source": [
    "# Setup for the MLP-Mixer Architecture\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AnTyoluw_Suz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import pickle\n",
    "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
    "from CKA_Google import *\n",
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 : Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DAOrIHu_Su2"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1iMiVS7o_Su3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224 = 50176\n",
      "Patch size: 32 X 32 = 1024 \n",
      "Patches per image: 49\n",
      "Elements per patch (3 channels): 3072\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 512 \n",
    "num_epochs = 50\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Selected Architecture: B/32\n",
    "\n",
    "image_size = 224  # We'll resize input images to this size. Square\n",
    "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
    "embedding_dim = 384  # Fixed Embedding Dimension\n",
    "num_blocks = 12\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "num_models = 10\n",
    "rbf_index = [0.2,0.4,0.8] # For testing during Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuu2OAS_Su0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for training \n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mpnEZH_Su5"
   },
   "source": [
    "## Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ra-bXojQ_Su6"
   },
   "outputs": [],
   "source": [
    "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data. \n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches. \n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks. ## (sequential_82)\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation) \n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpQFU8H__Su8"
   },
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9E1zD2BY_Su8"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        #Negative Log Likelihood = Categorical Cross Entropy\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping regularization callback. \n",
    "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
    "    #early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    #    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    #)\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history, accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxeE0cmM_Su9"
   },
   "source": [
    "## Use data augmentation\n",
    "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zh6hFPWX_Su-"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G77cUgiu_Su-"
   },
   "source": [
    "## Implement patch extraction as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYKNktXe_Su_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        #Extract the shape dimension in the position 0 = columns\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Without overlapping, stride horizontally and vertically\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
    "            rates=[1, 1, 1, 1],\n",
    "            #Patches contained in the images are considered, no zero padding\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        #shape[-1], number of colummns, as well as shape[0]\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Patches, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'patch_size' : self.patch_size ,\n",
    "            'num_patches' : self.num_patches\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yehcSS_Su_"
   },
   "source": [
    "## The MLP-Mixer model\n",
    "\n",
    "The MLP-Mixer is an architecture based exclusively on\n",
    "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "1. One applied independently to image patches, which mixes the per-location features.\n",
    "2. The other applied across patches (along channels), which mixes spatial information.\n",
    "\n",
    "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
    "instead of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwg4e2n_SvA"
   },
   "source": [
    "### Implement the MLP-Mixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dT6wVEki_SvA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp2 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super(MLPMixerLayer, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'num_patches' : num_patches,\n",
    "            'embedding_dim' : embedding_dim,\n",
    "            'dropout_rate' : dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUiufq92_SvA"
   },
   "source": [
    "## Build, train, and evaluate the MLP-Mixer model\n",
    "\n",
    "Note that training the model with the current settings on a V100 GPUs\n",
    "takes around 8 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report: Learning Curve\n",
    "def curves(history):\n",
    "    ymax1 = min(history[\"loss\"])\n",
    "    xmax1 = history[\"loss\"].index(ymax1)\n",
    "    ymax2 = min(history[\"val_loss\"])\n",
    "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
    "    plt.title(\"Cross Entropy Loss\")\n",
    "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
    "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Graph accuracy\n",
    "    ymax3 = max(history[\"acc\"])\n",
    "    xmax3 = history[\"acc\"].index(ymax3)\n",
    "    ymax4 = max(history[\"val_acc\"])\n",
    "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
    "    ymax5 = max(history[\"top5-acc\"])\n",
    "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
    "    ymax6 = max(history[\"val_top5-acc\"])\n",
    "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Classification accuracy')\n",
    "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Classification top5-acc')\n",
    "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.suptitle(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain activations + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers + Patches + One dense layer\n",
    "def Preprocessing(num_example):\n",
    "    augmented = data_augmentation(x_train[num_example])\n",
    "    b = Patches(patch_size, num_patches)(augmented)\n",
    "    a = layers.Dense(units=embedding_dim)(b)\n",
    "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
    "def Batch_Preprocessing(batch_size):\n",
    "    #Vector with the number of Sample of the Xtrain\n",
    "    a  = list(range(0,x_train.shape[0]))\n",
    "    b = random.sample(a,batch_size)\n",
    "    batch_regularization = list()\n",
    "    for i in range(0,batch_size):\n",
    "        inter_result = Preprocessing(b[i])\n",
    "        batch_regularization.append(inter_result)\n",
    "    return batch_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_out(result,layer_number,example):\n",
    "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "    ax1.imshow(x_train[example])\n",
    "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
    "    ax2.imshow(result[layer_number])\n",
    "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
    "    #The input is fixed to the beginning of the mlp blocks\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[0]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of layer's activation\n",
    "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
    "def Heatmap(result,type,sigma):\n",
    "    dim = len(result)\n",
    "    k = (dim - 1)\n",
    "    heatmap_CKA = np.zeros((dim,dim))\n",
    "    for i in range(0,dim):\n",
    "        tr = (dim - 1)\n",
    "        for j in range(0,dim):\n",
    "            if type == 'kernel':\n",
    "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
    "            elif type == 'linear':\n",
    "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
    "            else:\n",
    "                print('There is no such category, try again')\n",
    "                break\n",
    "\n",
    "            tr -= 1\n",
    "        k -= 1\n",
    "    #print('CKA' + type + 'calculated')\n",
    "    return heatmap_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of heatmaps (obsolet)\n",
    "def Prom_Mixer_Heatmaps(batch_result,type):\n",
    "    mat_heatmaps = list()\n",
    "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
    "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
    "        mat_heatmaps.append(heatmap_raw)\n",
    "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
    "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
    "    return prom_mixer_heatmap,mat_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 4\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i'%(i+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(3,3))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 : Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlpmixer_generator(num_models):\n",
    "    #now = datetime.datetime.now()\n",
    "    #date = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    for i in range(num_models):\n",
    "        mlpmixer_blocks = keras.Sequential(\n",
    "        [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
    "        )\n",
    "        mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
    "        history,accuracy, top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "        #Saving Results\n",
    "        pwd = 'Results_Article/4A_SC/mlpmixer_B-32_' + str(i+1)\n",
    "        mlpmixer_classifier.save(pwd)\n",
    "        np.save( pwd + '/history.npy',history.history)\n",
    "        with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "            pickle.dump(accuracy,file)\n",
    "        with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "            pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available types = 'rbf' or 'linear'\n",
    "def sanity_check(total_activations,num_models,num_blocks,type,sigma):\n",
    "    row = []\n",
    "    total_events = 0\n",
    "    positive_events = 0\n",
    "    for i in range(num_models-1):\n",
    "        comp_1 = total_activations[i]\n",
    "        for j in range(i+1,num_models):\n",
    "            comp_2 = total_activations[j]\n",
    "            for m in range(num_blocks):\n",
    "                for n in range(num_blocks):\n",
    "                    if type == 'rbf':\n",
    "                        inter_row = cka(gram_rbf(comp_1[m],sigma),gram_rbf(comp_2[n],sigma))\n",
    "                    elif type == 'linear':\n",
    "                        inter_row = cka(gram_linear(comp_1[m]),gram_linear(comp_2[n]))\n",
    "                    row.append(inter_row)\n",
    "                b = [i for i, x in enumerate(row) if x == max(row)]\n",
    "                if len(b) == 1:\n",
    "                    if b[0] == m:\n",
    "                        #print(row)\n",
    "                        #print('Hello, Layer %i'%(m))\n",
    "                        positive_events += 1\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "                total_events +=1\n",
    "                row=[]\n",
    "    return positive_events, total_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 31s 244ms/step - loss: 4.1454 - acc: 0.2541 - top5-acc: 0.7429 - val_loss: 1.5725 - val_acc: 0.4312 - val_top5-acc: 0.9008 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.5655 - acc: 0.4323 - top5-acc: 0.9042 - val_loss: 1.3757 - val_acc: 0.5010 - val_top5-acc: 0.9326 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.4017 - acc: 0.4948 - top5-acc: 0.9294 - val_loss: 1.2936 - val_acc: 0.5402 - val_top5-acc: 0.9414 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3370 - acc: 0.5181 - top5-acc: 0.9372 - val_loss: 1.2786 - val_acc: 0.5408 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2709 - acc: 0.5462 - top5-acc: 0.9444 - val_loss: 1.1982 - val_acc: 0.5734 - val_top5-acc: 0.9542 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2385 - acc: 0.5537 - top5-acc: 0.9465 - val_loss: 1.1523 - val_acc: 0.5940 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2060 - acc: 0.5704 - top5-acc: 0.9489 - val_loss: 1.1169 - val_acc: 0.6066 - val_top5-acc: 0.9616 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1752 - acc: 0.5811 - top5-acc: 0.9536 - val_loss: 1.0863 - val_acc: 0.6070 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1408 - acc: 0.5918 - top5-acc: 0.9563 - val_loss: 1.0563 - val_acc: 0.6346 - val_top5-acc: 0.9642 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1127 - acc: 0.6050 - top5-acc: 0.9580 - val_loss: 1.0803 - val_acc: 0.6156 - val_top5-acc: 0.9642 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0909 - acc: 0.6138 - top5-acc: 0.9612 - val_loss: 1.0031 - val_acc: 0.6410 - val_top5-acc: 0.9676 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0800 - acc: 0.6169 - top5-acc: 0.9615 - val_loss: 0.9927 - val_acc: 0.6472 - val_top5-acc: 0.9714 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0550 - acc: 0.6254 - top5-acc: 0.9644 - val_loss: 1.0080 - val_acc: 0.6462 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0316 - acc: 0.6337 - top5-acc: 0.9658 - val_loss: 0.9683 - val_acc: 0.6582 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0006 - acc: 0.6477 - top5-acc: 0.9680 - val_loss: 0.9704 - val_acc: 0.6536 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9953 - acc: 0.6482 - top5-acc: 0.9682 - val_loss: 0.9512 - val_acc: 0.6648 - val_top5-acc: 0.9694 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9888 - acc: 0.6486 - top5-acc: 0.9682 - val_loss: 0.9038 - val_acc: 0.6784 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9668 - acc: 0.6598 - top5-acc: 0.9699 - val_loss: 0.9512 - val_acc: 0.6712 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9491 - acc: 0.6631 - top5-acc: 0.9717 - val_loss: 0.8921 - val_acc: 0.6904 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9519 - acc: 0.6625 - top5-acc: 0.9717 - val_loss: 0.8999 - val_acc: 0.6858 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9268 - acc: 0.6733 - top5-acc: 0.9728 - val_loss: 0.8694 - val_acc: 0.6976 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9298 - acc: 0.6721 - top5-acc: 0.9737 - val_loss: 0.8918 - val_acc: 0.6802 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9117 - acc: 0.6788 - top5-acc: 0.9743 - val_loss: 0.8779 - val_acc: 0.6910 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8942 - acc: 0.6856 - top5-acc: 0.9745 - val_loss: 0.8684 - val_acc: 0.6960 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8823 - acc: 0.6884 - top5-acc: 0.9762 - val_loss: 0.8544 - val_acc: 0.7042 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8792 - acc: 0.6921 - top5-acc: 0.9760 - val_loss: 0.8771 - val_acc: 0.6962 - val_top5-acc: 0.9768 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8713 - acc: 0.6948 - top5-acc: 0.9775 - val_loss: 0.8670 - val_acc: 0.7040 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8556 - acc: 0.6954 - top5-acc: 0.9782 - val_loss: 0.8966 - val_acc: 0.6946 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8363 - acc: 0.7059 - top5-acc: 0.9780 - val_loss: 0.8517 - val_acc: 0.7058 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8549 - acc: 0.6986 - top5-acc: 0.9771 - val_loss: 0.8243 - val_acc: 0.7200 - val_top5-acc: 0.9832 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8233 - acc: 0.7109 - top5-acc: 0.9789 - val_loss: 0.8138 - val_acc: 0.7246 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8370 - acc: 0.7055 - top5-acc: 0.9786 - val_loss: 0.8172 - val_acc: 0.7190 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8255 - acc: 0.7089 - top5-acc: 0.9811 - val_loss: 0.8554 - val_acc: 0.7100 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8234 - acc: 0.7092 - top5-acc: 0.9799 - val_loss: 0.8329 - val_acc: 0.7132 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8154 - acc: 0.7132 - top5-acc: 0.9804 - val_loss: 0.8457 - val_acc: 0.7180 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8002 - acc: 0.7199 - top5-acc: 0.9802 - val_loss: 0.8048 - val_acc: 0.7302 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7985 - acc: 0.7180 - top5-acc: 0.9811 - val_loss: 0.8289 - val_acc: 0.7194 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8060 - acc: 0.7176 - top5-acc: 0.9812 - val_loss: 0.8464 - val_acc: 0.7118 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7834 - acc: 0.7231 - top5-acc: 0.9825 - val_loss: 0.8040 - val_acc: 0.7250 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7676 - acc: 0.7300 - top5-acc: 0.9822 - val_loss: 0.8315 - val_acc: 0.7198 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7637 - acc: 0.7313 - top5-acc: 0.9828 - val_loss: 0.8559 - val_acc: 0.7122 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8016 - acc: 0.7193 - top5-acc: 0.9818 - val_loss: 0.8109 - val_acc: 0.7274 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7424 - acc: 0.7391 - top5-acc: 0.9838 - val_loss: 0.7933 - val_acc: 0.7360 - val_top5-acc: 0.9816 - lr: 0.0050\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 5.6547 - acc: 0.5659 - top5-acc: 0.8937 - val_loss: 11.8576 - val_acc: 0.2826 - val_top5-acc: 0.7506 - lr: 0.0050\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 2.1663 - acc: 0.5142 - top5-acc: 0.9212 - val_loss: 0.9509 - val_acc: 0.6676 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9522 - acc: 0.6631 - top5-acc: 0.9710 - val_loss: 0.8476 - val_acc: 0.7082 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8429 - acc: 0.7002 - top5-acc: 0.9774 - val_loss: 0.7902 - val_acc: 0.7312 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7823 - acc: 0.7246 - top5-acc: 0.9820 - val_loss: 0.7750 - val_acc: 0.7346 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7458 - acc: 0.7376 - top5-acc: 0.9840 - val_loss: 0.7451 - val_acc: 0.7500 - val_top5-acc: 0.9838 - lr: 0.0050\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7117 - acc: 0.7498 - top5-acc: 0.9854 - val_loss: 0.7331 - val_acc: 0.7518 - val_top5-acc: 0.9846 - lr: 0.0050\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.7570 - acc: 0.7430 - top5-acc: 0.9841\n",
      "Test accuracy: 74.3%\n",
      "Test top 5 accuracy: 98.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn, layer_normalization_1_layer_call_and_return_conditional_losses, layer_normalization_2_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 246ms/step - loss: 3.8345 - acc: 0.2450 - top5-acc: 0.7400 - val_loss: 1.7854 - val_acc: 0.3690 - val_top5-acc: 0.8716 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5870 - acc: 0.4263 - top5-acc: 0.8988 - val_loss: 1.5116 - val_acc: 0.4768 - val_top5-acc: 0.9250 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.4165 - acc: 0.4934 - top5-acc: 0.9270 - val_loss: 1.3301 - val_acc: 0.5260 - val_top5-acc: 0.9446 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3293 - acc: 0.5252 - top5-acc: 0.9365 - val_loss: 1.2640 - val_acc: 0.5506 - val_top5-acc: 0.9488 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2951 - acc: 0.5360 - top5-acc: 0.9410 - val_loss: 1.2057 - val_acc: 0.5680 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2448 - acc: 0.5586 - top5-acc: 0.9464 - val_loss: 1.1473 - val_acc: 0.5956 - val_top5-acc: 0.9606 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1900 - acc: 0.5776 - top5-acc: 0.9515 - val_loss: 1.0998 - val_acc: 0.6172 - val_top5-acc: 0.9612 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1746 - acc: 0.5830 - top5-acc: 0.9529 - val_loss: 1.1196 - val_acc: 0.5990 - val_top5-acc: 0.9622 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1400 - acc: 0.5942 - top5-acc: 0.9572 - val_loss: 1.0810 - val_acc: 0.6196 - val_top5-acc: 0.9626 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1072 - acc: 0.6046 - top5-acc: 0.9600 - val_loss: 1.0495 - val_acc: 0.6400 - val_top5-acc: 0.9640 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0961 - acc: 0.6088 - top5-acc: 0.9606 - val_loss: 1.0317 - val_acc: 0.6388 - val_top5-acc: 0.9680 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0733 - acc: 0.6176 - top5-acc: 0.9630 - val_loss: 1.0324 - val_acc: 0.6436 - val_top5-acc: 0.9656 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0342 - acc: 0.6321 - top5-acc: 0.9652 - val_loss: 0.9647 - val_acc: 0.6572 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0275 - acc: 0.6356 - top5-acc: 0.9664 - val_loss: 0.9922 - val_acc: 0.6542 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0051 - acc: 0.6437 - top5-acc: 0.9678 - val_loss: 0.9635 - val_acc: 0.6630 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 7.2547 - acc: 0.3616 - top5-acc: 0.8047 - val_loss: 1.6456 - val_acc: 0.4056 - val_top5-acc: 0.9022 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.4364 - acc: 0.4796 - top5-acc: 0.9235 - val_loss: 1.2728 - val_acc: 0.5474 - val_top5-acc: 0.9444 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2753 - acc: 0.5411 - top5-acc: 0.9432 - val_loss: 1.1713 - val_acc: 0.5722 - val_top5-acc: 0.9576 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1771 - acc: 0.5799 - top5-acc: 0.9542 - val_loss: 1.1271 - val_acc: 0.5942 - val_top5-acc: 0.9610 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1263 - acc: 0.5974 - top5-acc: 0.9592 - val_loss: 1.0447 - val_acc: 0.6200 - val_top5-acc: 0.9690 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0596 - acc: 0.6264 - top5-acc: 0.9632 - val_loss: 0.9826 - val_acc: 0.6484 - val_top5-acc: 0.9714 - lr: 0.0025\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0283 - acc: 0.6363 - top5-acc: 0.9667 - val_loss: 0.9704 - val_acc: 0.6554 - val_top5-acc: 0.9722 - lr: 0.0025\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0060 - acc: 0.6440 - top5-acc: 0.9682 - val_loss: 0.9468 - val_acc: 0.6648 - val_top5-acc: 0.9740 - lr: 0.0025\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9806 - acc: 0.6525 - top5-acc: 0.9704 - val_loss: 0.9414 - val_acc: 0.6666 - val_top5-acc: 0.9756 - lr: 0.0025\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9669 - acc: 0.6581 - top5-acc: 0.9706 - val_loss: 0.9349 - val_acc: 0.6670 - val_top5-acc: 0.9758 - lr: 0.0025\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9512 - acc: 0.6628 - top5-acc: 0.9716 - val_loss: 0.9337 - val_acc: 0.6692 - val_top5-acc: 0.9752 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9404 - acc: 0.6669 - top5-acc: 0.9722 - val_loss: 0.8968 - val_acc: 0.6834 - val_top5-acc: 0.9758 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9265 - acc: 0.6714 - top5-acc: 0.9733 - val_loss: 0.8792 - val_acc: 0.6960 - val_top5-acc: 0.9766 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9128 - acc: 0.6776 - top5-acc: 0.9748 - val_loss: 0.9051 - val_acc: 0.6826 - val_top5-acc: 0.9760 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9005 - acc: 0.6810 - top5-acc: 0.9749 - val_loss: 0.8532 - val_acc: 0.7030 - val_top5-acc: 0.9788 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8844 - acc: 0.6850 - top5-acc: 0.9767 - val_loss: 0.8620 - val_acc: 0.6992 - val_top5-acc: 0.9768 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8796 - acc: 0.6894 - top5-acc: 0.9767 - val_loss: 0.8716 - val_acc: 0.6976 - val_top5-acc: 0.9776 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8635 - acc: 0.6949 - top5-acc: 0.9778 - val_loss: 0.8725 - val_acc: 0.6926 - val_top5-acc: 0.9772 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8583 - acc: 0.6976 - top5-acc: 0.9784 - val_loss: 0.8568 - val_acc: 0.6970 - val_top5-acc: 0.9798 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8471 - acc: 0.7013 - top5-acc: 0.9782 - val_loss: 0.8465 - val_acc: 0.7018 - val_top5-acc: 0.9780 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8477 - acc: 0.7021 - top5-acc: 0.9788 - val_loss: 0.8340 - val_acc: 0.7056 - val_top5-acc: 0.9804 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8382 - acc: 0.7028 - top5-acc: 0.9792 - val_loss: 0.8285 - val_acc: 0.7078 - val_top5-acc: 0.9796 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8214 - acc: 0.7088 - top5-acc: 0.9807 - val_loss: 0.8359 - val_acc: 0.7128 - val_top5-acc: 0.9808 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8164 - acc: 0.7106 - top5-acc: 0.9802 - val_loss: 0.8331 - val_acc: 0.7058 - val_top5-acc: 0.9782 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8141 - acc: 0.7113 - top5-acc: 0.9805 - val_loss: 0.8263 - val_acc: 0.7148 - val_top5-acc: 0.9778 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8012 - acc: 0.7169 - top5-acc: 0.9806 - val_loss: 0.8044 - val_acc: 0.7196 - val_top5-acc: 0.9806 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7935 - acc: 0.7220 - top5-acc: 0.9806 - val_loss: 0.8037 - val_acc: 0.7186 - val_top5-acc: 0.9796 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7959 - acc: 0.7186 - top5-acc: 0.9812 - val_loss: 0.8224 - val_acc: 0.7174 - val_top5-acc: 0.9822 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7789 - acc: 0.7250 - top5-acc: 0.9826 - val_loss: 0.8048 - val_acc: 0.7208 - val_top5-acc: 0.9810 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7791 - acc: 0.7244 - top5-acc: 0.9819 - val_loss: 0.8046 - val_acc: 0.7196 - val_top5-acc: 0.9800 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7619 - acc: 0.7302 - top5-acc: 0.9830 - val_loss: 0.7969 - val_acc: 0.7268 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7562 - acc: 0.7316 - top5-acc: 0.9838 - val_loss: 0.8270 - val_acc: 0.7168 - val_top5-acc: 0.9812 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7615 - acc: 0.7303 - top5-acc: 0.9834 - val_loss: 0.8009 - val_acc: 0.7248 - val_top5-acc: 0.9802 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7475 - acc: 0.7337 - top5-acc: 0.9839 - val_loss: 0.7993 - val_acc: 0.7252 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7418 - acc: 0.7359 - top5-acc: 0.9847 - val_loss: 0.7834 - val_acc: 0.7264 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.7964 - acc: 0.7346 - top5-acc: 0.9811\n",
      "Test accuracy: 73.46%\n",
      "Test top 5 accuracy: 98.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_12_layer_call_fn, layer_normalization_12_layer_call_and_return_conditional_losses, layer_normalization_13_layer_call_fn, layer_normalization_13_layer_call_and_return_conditional_losses, layer_normalization_14_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 243ms/step - loss: 4.2334 - acc: 0.2490 - top5-acc: 0.7273 - val_loss: 1.6312 - val_acc: 0.4202 - val_top5-acc: 0.8980 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.5707 - acc: 0.4321 - top5-acc: 0.9001 - val_loss: 1.3989 - val_acc: 0.4976 - val_top5-acc: 0.9316 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.4277 - acc: 0.4840 - top5-acc: 0.9245 - val_loss: 1.2749 - val_acc: 0.5400 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3574 - acc: 0.5127 - top5-acc: 0.9334 - val_loss: 1.2582 - val_acc: 0.5538 - val_top5-acc: 0.9462 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2922 - acc: 0.5360 - top5-acc: 0.9412 - val_loss: 1.2038 - val_acc: 0.5770 - val_top5-acc: 0.9510 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2511 - acc: 0.5543 - top5-acc: 0.9466 - val_loss: 1.1607 - val_acc: 0.5944 - val_top5-acc: 0.9570 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2055 - acc: 0.5700 - top5-acc: 0.9512 - val_loss: 1.1213 - val_acc: 0.5984 - val_top5-acc: 0.9622 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1787 - acc: 0.5816 - top5-acc: 0.9530 - val_loss: 1.0902 - val_acc: 0.6144 - val_top5-acc: 0.9630 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1419 - acc: 0.5935 - top5-acc: 0.9558 - val_loss: 1.0464 - val_acc: 0.6340 - val_top5-acc: 0.9632 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1172 - acc: 0.6024 - top5-acc: 0.9586 - val_loss: 1.0372 - val_acc: 0.6352 - val_top5-acc: 0.9686 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0946 - acc: 0.6132 - top5-acc: 0.9608 - val_loss: 1.0518 - val_acc: 0.6286 - val_top5-acc: 0.9672 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0808 - acc: 0.6171 - top5-acc: 0.9626 - val_loss: 1.0736 - val_acc: 0.6266 - val_top5-acc: 0.9640 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0546 - acc: 0.6260 - top5-acc: 0.9644 - val_loss: 1.0272 - val_acc: 0.6384 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0322 - acc: 0.6341 - top5-acc: 0.9660 - val_loss: 0.9665 - val_acc: 0.6628 - val_top5-acc: 0.9740 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0128 - acc: 0.6401 - top5-acc: 0.9672 - val_loss: 0.9247 - val_acc: 0.6750 - val_top5-acc: 0.9758 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9942 - acc: 0.6480 - top5-acc: 0.9695 - val_loss: 0.9379 - val_acc: 0.6736 - val_top5-acc: 0.9712 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9728 - acc: 0.6548 - top5-acc: 0.9700 - val_loss: 1.0035 - val_acc: 0.6498 - val_top5-acc: 0.9708 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9727 - acc: 0.6577 - top5-acc: 0.9690 - val_loss: 0.9241 - val_acc: 0.6754 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9523 - acc: 0.6646 - top5-acc: 0.9714 - val_loss: 0.8931 - val_acc: 0.6906 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9380 - acc: 0.6713 - top5-acc: 0.9723 - val_loss: 0.8748 - val_acc: 0.6910 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9449 - acc: 0.6682 - top5-acc: 0.9722 - val_loss: 0.8710 - val_acc: 0.6982 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9110 - acc: 0.6779 - top5-acc: 0.9745 - val_loss: 0.8787 - val_acc: 0.6954 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9051 - acc: 0.6795 - top5-acc: 0.9741 - val_loss: 0.8822 - val_acc: 0.7050 - val_top5-acc: 0.9746 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8858 - acc: 0.6903 - top5-acc: 0.9754 - val_loss: 0.9334 - val_acc: 0.6738 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8987 - acc: 0.6847 - top5-acc: 0.9758 - val_loss: 0.8577 - val_acc: 0.7072 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8696 - acc: 0.6932 - top5-acc: 0.9769 - val_loss: 0.8629 - val_acc: 0.7072 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8773 - acc: 0.6926 - top5-acc: 0.9768 - val_loss: 0.8655 - val_acc: 0.7008 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8617 - acc: 0.6983 - top5-acc: 0.9781 - val_loss: 0.8529 - val_acc: 0.7044 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8627 - acc: 0.6961 - top5-acc: 0.9788 - val_loss: 0.8418 - val_acc: 0.7098 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8415 - acc: 0.7031 - top5-acc: 0.9788 - val_loss: 0.8153 - val_acc: 0.7248 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8313 - acc: 0.7072 - top5-acc: 0.9791 - val_loss: 0.8245 - val_acc: 0.7218 - val_top5-acc: 0.9816 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8370 - acc: 0.7050 - top5-acc: 0.9783 - val_loss: 0.8379 - val_acc: 0.7148 - val_top5-acc: 0.9832 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8191 - acc: 0.7121 - top5-acc: 0.9790 - val_loss: 0.8555 - val_acc: 0.7160 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8329 - acc: 0.7083 - top5-acc: 0.9792 - val_loss: 0.8486 - val_acc: 0.7134 - val_top5-acc: 0.9786 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8183 - acc: 0.7112 - top5-acc: 0.9810 - val_loss: 0.8300 - val_acc: 0.7146 - val_top5-acc: 0.9826 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7172 - acc: 0.7507 - top5-acc: 0.9853 - val_loss: 0.7441 - val_acc: 0.7422 - val_top5-acc: 0.9852 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6917 - acc: 0.7557 - top5-acc: 0.9863 - val_loss: 0.7324 - val_acc: 0.7536 - val_top5-acc: 0.9868 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6748 - acc: 0.7621 - top5-acc: 0.9870 - val_loss: 0.7274 - val_acc: 0.7504 - val_top5-acc: 0.9854 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6753 - acc: 0.7602 - top5-acc: 0.9875 - val_loss: 0.7566 - val_acc: 0.7392 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6708 - acc: 0.7623 - top5-acc: 0.9874 - val_loss: 0.7545 - val_acc: 0.7474 - val_top5-acc: 0.9868 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6719 - acc: 0.7618 - top5-acc: 0.9874 - val_loss: 0.7679 - val_acc: 0.7358 - val_top5-acc: 0.9838 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6637 - acc: 0.7651 - top5-acc: 0.9880 - val_loss: 0.7232 - val_acc: 0.7524 - val_top5-acc: 0.9854 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6576 - acc: 0.7669 - top5-acc: 0.9880 - val_loss: 0.7126 - val_acc: 0.7596 - val_top5-acc: 0.9884 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6566 - acc: 0.7687 - top5-acc: 0.9885 - val_loss: 0.7524 - val_acc: 0.7486 - val_top5-acc: 0.9854 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6535 - acc: 0.7695 - top5-acc: 0.9889 - val_loss: 0.7505 - val_acc: 0.7472 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6540 - acc: 0.7680 - top5-acc: 0.9883 - val_loss: 0.7380 - val_acc: 0.7556 - val_top5-acc: 0.9810 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6466 - acc: 0.7700 - top5-acc: 0.9885 - val_loss: 0.7607 - val_acc: 0.7466 - val_top5-acc: 0.9880 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6363 - acc: 0.7761 - top5-acc: 0.9891 - val_loss: 0.7704 - val_acc: 0.7458 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5869 - acc: 0.7910 - top5-acc: 0.9904 - val_loss: 0.7264 - val_acc: 0.7582 - val_top5-acc: 0.9840 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5630 - acc: 0.8000 - top5-acc: 0.9924 - val_loss: 0.7044 - val_acc: 0.7680 - val_top5-acc: 0.9874 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.7390 - acc: 0.7605 - top5-acc: 0.9855\n",
      "Test accuracy: 76.05%\n",
      "Test top 5 accuracy: 98.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_24_layer_call_fn, layer_normalization_24_layer_call_and_return_conditional_losses, layer_normalization_25_layer_call_fn, layer_normalization_25_layer_call_and_return_conditional_losses, layer_normalization_26_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 256ms/step - loss: 4.5161 - acc: 0.2469 - top5-acc: 0.7377 - val_loss: 1.6396 - val_acc: 0.4072 - val_top5-acc: 0.8918 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5970 - acc: 0.4228 - top5-acc: 0.8958 - val_loss: 1.4557 - val_acc: 0.4770 - val_top5-acc: 0.9240 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.4626 - acc: 0.4746 - top5-acc: 0.9200 - val_loss: 1.3734 - val_acc: 0.5056 - val_top5-acc: 0.9338 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3587 - acc: 0.5128 - top5-acc: 0.9328 - val_loss: 1.2478 - val_acc: 0.5494 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2991 - acc: 0.5339 - top5-acc: 0.9414 - val_loss: 1.2058 - val_acc: 0.5726 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2586 - acc: 0.5466 - top5-acc: 0.9459 - val_loss: 1.1774 - val_acc: 0.5782 - val_top5-acc: 0.9610 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2136 - acc: 0.5649 - top5-acc: 0.9492 - val_loss: 1.1566 - val_acc: 0.5882 - val_top5-acc: 0.9612 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1904 - acc: 0.5748 - top5-acc: 0.9510 - val_loss: 1.1251 - val_acc: 0.5976 - val_top5-acc: 0.9604 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1564 - acc: 0.5908 - top5-acc: 0.9548 - val_loss: 1.0677 - val_acc: 0.6318 - val_top5-acc: 0.9612 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1310 - acc: 0.5976 - top5-acc: 0.9578 - val_loss: 1.0558 - val_acc: 0.6256 - val_top5-acc: 0.9638 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1023 - acc: 0.6073 - top5-acc: 0.9599 - val_loss: 1.0621 - val_acc: 0.6268 - val_top5-acc: 0.9642 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0891 - acc: 0.6128 - top5-acc: 0.9612 - val_loss: 1.0263 - val_acc: 0.6376 - val_top5-acc: 0.9680 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0518 - acc: 0.6269 - top5-acc: 0.9637 - val_loss: 1.0637 - val_acc: 0.6288 - val_top5-acc: 0.9648 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0462 - acc: 0.6299 - top5-acc: 0.9639 - val_loss: 0.9658 - val_acc: 0.6588 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0259 - acc: 0.6356 - top5-acc: 0.9662 - val_loss: 0.9883 - val_acc: 0.6574 - val_top5-acc: 0.9706 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0023 - acc: 0.6459 - top5-acc: 0.9671 - val_loss: 0.9514 - val_acc: 0.6688 - val_top5-acc: 0.9734 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9869 - acc: 0.6498 - top5-acc: 0.9683 - val_loss: 0.9120 - val_acc: 0.6876 - val_top5-acc: 0.9760 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9668 - acc: 0.6573 - top5-acc: 0.9708 - val_loss: 0.9401 - val_acc: 0.6730 - val_top5-acc: 0.9730 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9610 - acc: 0.6604 - top5-acc: 0.9709 - val_loss: 0.9144 - val_acc: 0.6780 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9350 - acc: 0.6716 - top5-acc: 0.9720 - val_loss: 0.9276 - val_acc: 0.6840 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9146 - acc: 0.6751 - top5-acc: 0.9727 - val_loss: 0.8891 - val_acc: 0.6970 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9179 - acc: 0.6763 - top5-acc: 0.9730 - val_loss: 0.8662 - val_acc: 0.7060 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9023 - acc: 0.6815 - top5-acc: 0.9743 - val_loss: 0.9459 - val_acc: 0.6760 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8945 - acc: 0.6852 - top5-acc: 0.9759 - val_loss: 0.8766 - val_acc: 0.7008 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8728 - acc: 0.6932 - top5-acc: 0.9757 - val_loss: 0.8590 - val_acc: 0.7026 - val_top5-acc: 0.9796 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8744 - acc: 0.6906 - top5-acc: 0.9759 - val_loss: 0.8439 - val_acc: 0.7098 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8552 - acc: 0.6982 - top5-acc: 0.9773 - val_loss: 0.8381 - val_acc: 0.7102 - val_top5-acc: 0.9796 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8464 - acc: 0.6999 - top5-acc: 0.9783 - val_loss: 0.8774 - val_acc: 0.7022 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8480 - acc: 0.7001 - top5-acc: 0.9780 - val_loss: 0.8466 - val_acc: 0.7134 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8545 - acc: 0.7001 - top5-acc: 0.9769 - val_loss: 0.8261 - val_acc: 0.7202 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8239 - acc: 0.7090 - top5-acc: 0.9797 - val_loss: 0.8480 - val_acc: 0.7066 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8286 - acc: 0.7069 - top5-acc: 0.9792 - val_loss: 0.8448 - val_acc: 0.7124 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8171 - acc: 0.7123 - top5-acc: 0.9802 - val_loss: 0.8530 - val_acc: 0.7068 - val_top5-acc: 0.9762 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8112 - acc: 0.7136 - top5-acc: 0.9806 - val_loss: 0.8401 - val_acc: 0.7196 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7932 - acc: 0.7209 - top5-acc: 0.9821 - val_loss: 0.8144 - val_acc: 0.7220 - val_top5-acc: 0.9826 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8052 - acc: 0.7192 - top5-acc: 0.9806 - val_loss: 0.7934 - val_acc: 0.7312 - val_top5-acc: 0.9826 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8105 - acc: 0.7157 - top5-acc: 0.9805 - val_loss: 0.8314 - val_acc: 0.7208 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7930 - acc: 0.7208 - top5-acc: 0.9812 - val_loss: 0.7780 - val_acc: 0.7414 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7846 - acc: 0.7241 - top5-acc: 0.9826 - val_loss: 0.8338 - val_acc: 0.7284 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7815 - acc: 0.7240 - top5-acc: 0.9821 - val_loss: 0.7852 - val_acc: 0.7354 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 10.6877 - acc: 0.5099 - top5-acc: 0.8666 - val_loss: 2.9821 - val_acc: 0.3442 - val_top5-acc: 0.8382 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.4719 - acc: 0.5116 - top5-acc: 0.9291 - val_loss: 1.0603 - val_acc: 0.6258 - val_top5-acc: 0.9650 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0874 - acc: 0.6165 - top5-acc: 0.9610 - val_loss: 0.9452 - val_acc: 0.6704 - val_top5-acc: 0.9714 - lr: 0.0050\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9788 - acc: 0.6542 - top5-acc: 0.9689 - val_loss: 0.8855 - val_acc: 0.6832 - val_top5-acc: 0.9790 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9433 - acc: 0.6651 - top5-acc: 0.9719 - val_loss: 0.8502 - val_acc: 0.7010 - val_top5-acc: 0.9788 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9014 - acc: 0.6820 - top5-acc: 0.9751 - val_loss: 0.8329 - val_acc: 0.7076 - val_top5-acc: 0.9812 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8714 - acc: 0.6954 - top5-acc: 0.9764 - val_loss: 0.8192 - val_acc: 0.7120 - val_top5-acc: 0.9796 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8439 - acc: 0.7034 - top5-acc: 0.9772 - val_loss: 0.8052 - val_acc: 0.7190 - val_top5-acc: 0.9804 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8103 - acc: 0.7164 - top5-acc: 0.9798 - val_loss: 0.7832 - val_acc: 0.7330 - val_top5-acc: 0.9814 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8013 - acc: 0.7207 - top5-acc: 0.9794 - val_loss: 0.7720 - val_acc: 0.7300 - val_top5-acc: 0.9822 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.8056 - acc: 0.7226 - top5-acc: 0.9793\n",
      "Test accuracy: 72.26%\n",
      "Test top 5 accuracy: 97.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_36_layer_call_fn, layer_normalization_36_layer_call_and_return_conditional_losses, layer_normalization_37_layer_call_fn, layer_normalization_37_layer_call_and_return_conditional_losses, layer_normalization_38_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 244ms/step - loss: 3.9829 - acc: 0.2469 - top5-acc: 0.7389 - val_loss: 1.6770 - val_acc: 0.3994 - val_top5-acc: 0.9006 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5640 - acc: 0.4354 - top5-acc: 0.9021 - val_loss: 1.4041 - val_acc: 0.4876 - val_top5-acc: 0.9276 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.4262 - acc: 0.4852 - top5-acc: 0.9240 - val_loss: 1.3424 - val_acc: 0.5220 - val_top5-acc: 0.9386 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3394 - acc: 0.5208 - top5-acc: 0.9362 - val_loss: 1.2245 - val_acc: 0.5610 - val_top5-acc: 0.9476 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2609 - acc: 0.5458 - top5-acc: 0.9441 - val_loss: 1.1689 - val_acc: 0.5848 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2178 - acc: 0.5653 - top5-acc: 0.9512 - val_loss: 1.1155 - val_acc: 0.6042 - val_top5-acc: 0.9604 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1730 - acc: 0.5807 - top5-acc: 0.9536 - val_loss: 1.1367 - val_acc: 0.5950 - val_top5-acc: 0.9642 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1446 - acc: 0.5951 - top5-acc: 0.9578 - val_loss: 1.0528 - val_acc: 0.6236 - val_top5-acc: 0.9686 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1116 - acc: 0.6055 - top5-acc: 0.9598 - val_loss: 1.0442 - val_acc: 0.6264 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0830 - acc: 0.6158 - top5-acc: 0.9619 - val_loss: 1.0032 - val_acc: 0.6444 - val_top5-acc: 0.9668 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0578 - acc: 0.6229 - top5-acc: 0.9648 - val_loss: 0.9832 - val_acc: 0.6572 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0256 - acc: 0.6373 - top5-acc: 0.9664 - val_loss: 0.9783 - val_acc: 0.6572 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0019 - acc: 0.6468 - top5-acc: 0.9686 - val_loss: 0.9774 - val_acc: 0.6586 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0108 - acc: 0.6423 - top5-acc: 0.9665 - val_loss: 0.9344 - val_acc: 0.6730 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9803 - acc: 0.6535 - top5-acc: 0.9690 - val_loss: 0.9447 - val_acc: 0.6766 - val_top5-acc: 0.9704 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9606 - acc: 0.6617 - top5-acc: 0.9701 - val_loss: 0.8931 - val_acc: 0.6830 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9465 - acc: 0.6663 - top5-acc: 0.9726 - val_loss: 0.9676 - val_acc: 0.6618 - val_top5-acc: 0.9762 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9449 - acc: 0.6648 - top5-acc: 0.9728 - val_loss: 0.9103 - val_acc: 0.6888 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9220 - acc: 0.6718 - top5-acc: 0.9735 - val_loss: 0.8897 - val_acc: 0.6948 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9112 - acc: 0.6798 - top5-acc: 0.9736 - val_loss: 0.8545 - val_acc: 0.7030 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8889 - acc: 0.6880 - top5-acc: 0.9748 - val_loss: 0.8633 - val_acc: 0.7000 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8949 - acc: 0.6848 - top5-acc: 0.9758 - val_loss: 0.8636 - val_acc: 0.6978 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8559 - acc: 0.6982 - top5-acc: 0.9781 - val_loss: 0.8616 - val_acc: 0.6968 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8489 - acc: 0.7009 - top5-acc: 0.9786 - val_loss: 0.8773 - val_acc: 0.7072 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8451 - acc: 0.7016 - top5-acc: 0.9777 - val_loss: 0.8630 - val_acc: 0.7068 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7554 - acc: 0.7351 - top5-acc: 0.9828 - val_loss: 0.7615 - val_acc: 0.7384 - val_top5-acc: 0.9846 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7216 - acc: 0.7453 - top5-acc: 0.9850 - val_loss: 0.7644 - val_acc: 0.7432 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7123 - acc: 0.7471 - top5-acc: 0.9852 - val_loss: 0.7675 - val_acc: 0.7398 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7053 - acc: 0.7500 - top5-acc: 0.9854 - val_loss: 0.7473 - val_acc: 0.7494 - val_top5-acc: 0.9844 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7032 - acc: 0.7540 - top5-acc: 0.9858 - val_loss: 0.7829 - val_acc: 0.7382 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6970 - acc: 0.7522 - top5-acc: 0.9860 - val_loss: 0.7565 - val_acc: 0.7520 - val_top5-acc: 0.9842 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6945 - acc: 0.7516 - top5-acc: 0.9861 - val_loss: 0.7605 - val_acc: 0.7406 - val_top5-acc: 0.9810 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6876 - acc: 0.7570 - top5-acc: 0.9870 - val_loss: 0.7916 - val_acc: 0.7316 - val_top5-acc: 0.9814 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6785 - acc: 0.7604 - top5-acc: 0.9866 - val_loss: 0.7530 - val_acc: 0.7438 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6250 - acc: 0.7759 - top5-acc: 0.9893 - val_loss: 0.7282 - val_acc: 0.7586 - val_top5-acc: 0.9850 - lr: 0.0012\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6078 - acc: 0.7854 - top5-acc: 0.9901 - val_loss: 0.7133 - val_acc: 0.7630 - val_top5-acc: 0.9820 - lr: 0.0012\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6042 - acc: 0.7860 - top5-acc: 0.9898 - val_loss: 0.7130 - val_acc: 0.7638 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5962 - acc: 0.7879 - top5-acc: 0.9901 - val_loss: 0.7141 - val_acc: 0.7574 - val_top5-acc: 0.9844 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5842 - acc: 0.7927 - top5-acc: 0.9906 - val_loss: 0.7131 - val_acc: 0.7626 - val_top5-acc: 0.9850 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5876 - acc: 0.7911 - top5-acc: 0.9912 - val_loss: 0.7180 - val_acc: 0.7602 - val_top5-acc: 0.9868 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5764 - acc: 0.7959 - top5-acc: 0.9907 - val_loss: 0.7325 - val_acc: 0.7612 - val_top5-acc: 0.9844 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5857 - acc: 0.7904 - top5-acc: 0.9904 - val_loss: 0.7500 - val_acc: 0.7604 - val_top5-acc: 0.9842 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5414 - acc: 0.8069 - top5-acc: 0.9923 - val_loss: 0.7117 - val_acc: 0.7680 - val_top5-acc: 0.9862 - lr: 6.2500e-04\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5303 - acc: 0.8126 - top5-acc: 0.9927 - val_loss: 0.7048 - val_acc: 0.7714 - val_top5-acc: 0.9854 - lr: 6.2500e-04\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5200 - acc: 0.8138 - top5-acc: 0.9931 - val_loss: 0.7098 - val_acc: 0.7694 - val_top5-acc: 0.9832 - lr: 6.2500e-04\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5159 - acc: 0.8154 - top5-acc: 0.9934 - val_loss: 0.6962 - val_acc: 0.7722 - val_top5-acc: 0.9862 - lr: 6.2500e-04\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5110 - acc: 0.8181 - top5-acc: 0.9932 - val_loss: 0.7008 - val_acc: 0.7756 - val_top5-acc: 0.9858 - lr: 6.2500e-04\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5126 - acc: 0.8172 - top5-acc: 0.9935 - val_loss: 0.7061 - val_acc: 0.7678 - val_top5-acc: 0.9852 - lr: 6.2500e-04\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5178 - acc: 0.8165 - top5-acc: 0.9931 - val_loss: 0.7066 - val_acc: 0.7718 - val_top5-acc: 0.9844 - lr: 6.2500e-04\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5084 - acc: 0.8195 - top5-acc: 0.9929 - val_loss: 0.7330 - val_acc: 0.7676 - val_top5-acc: 0.9846 - lr: 6.2500e-04\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.7490 - acc: 0.7591 - top5-acc: 0.9854\n",
      "Test accuracy: 75.91%\n",
      "Test top 5 accuracy: 98.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_48_layer_call_fn, layer_normalization_48_layer_call_and_return_conditional_losses, layer_normalization_49_layer_call_fn, layer_normalization_49_layer_call_and_return_conditional_losses, layer_normalization_50_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 244ms/step - loss: 3.9825 - acc: 0.2541 - top5-acc: 0.7465 - val_loss: 1.6292 - val_acc: 0.4176 - val_top5-acc: 0.8976 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5630 - acc: 0.4360 - top5-acc: 0.9028 - val_loss: 1.4415 - val_acc: 0.4840 - val_top5-acc: 0.9258 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.4136 - acc: 0.4918 - top5-acc: 0.9263 - val_loss: 1.3235 - val_acc: 0.5164 - val_top5-acc: 0.9436 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3288 - acc: 0.5225 - top5-acc: 0.9359 - val_loss: 1.1975 - val_acc: 0.5690 - val_top5-acc: 0.9556 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2648 - acc: 0.5477 - top5-acc: 0.9442 - val_loss: 1.1769 - val_acc: 0.5760 - val_top5-acc: 0.9600 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2298 - acc: 0.5609 - top5-acc: 0.9481 - val_loss: 1.1321 - val_acc: 0.5928 - val_top5-acc: 0.9624 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1872 - acc: 0.5767 - top5-acc: 0.9528 - val_loss: 1.1019 - val_acc: 0.6086 - val_top5-acc: 0.9608 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1607 - acc: 0.5894 - top5-acc: 0.9545 - val_loss: 1.0793 - val_acc: 0.6148 - val_top5-acc: 0.9632 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1218 - acc: 0.6006 - top5-acc: 0.9586 - val_loss: 1.0256 - val_acc: 0.6420 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0996 - acc: 0.6088 - top5-acc: 0.9607 - val_loss: 1.0302 - val_acc: 0.6370 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0808 - acc: 0.6182 - top5-acc: 0.9613 - val_loss: 1.0191 - val_acc: 0.6374 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0571 - acc: 0.6268 - top5-acc: 0.9644 - val_loss: 0.9963 - val_acc: 0.6480 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0301 - acc: 0.6333 - top5-acc: 0.9658 - val_loss: 1.0135 - val_acc: 0.6438 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0196 - acc: 0.6396 - top5-acc: 0.9668 - val_loss: 0.9316 - val_acc: 0.6716 - val_top5-acc: 0.9742 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9965 - acc: 0.6482 - top5-acc: 0.9687 - val_loss: 0.9143 - val_acc: 0.6764 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9878 - acc: 0.6494 - top5-acc: 0.9691 - val_loss: 0.9395 - val_acc: 0.6744 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9648 - acc: 0.6601 - top5-acc: 0.9704 - val_loss: 0.9174 - val_acc: 0.6808 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9489 - acc: 0.6644 - top5-acc: 0.9719 - val_loss: 0.9304 - val_acc: 0.6776 - val_top5-acc: 0.9738 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9275 - acc: 0.6738 - top5-acc: 0.9729 - val_loss: 0.8891 - val_acc: 0.6866 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9220 - acc: 0.6746 - top5-acc: 0.9733 - val_loss: 0.8657 - val_acc: 0.6990 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9147 - acc: 0.6758 - top5-acc: 0.9735 - val_loss: 0.8821 - val_acc: 0.6918 - val_top5-acc: 0.9758 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8970 - acc: 0.6813 - top5-acc: 0.9750 - val_loss: 0.8972 - val_acc: 0.6920 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 6.1859 - acc: 0.4191 - top5-acc: 0.8396 - val_loss: 1.2649 - val_acc: 0.5450 - val_top5-acc: 0.9466 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2116 - acc: 0.5684 - top5-acc: 0.9493 - val_loss: 1.0134 - val_acc: 0.6434 - val_top5-acc: 0.9702 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0401 - acc: 0.6310 - top5-acc: 0.9645 - val_loss: 0.9522 - val_acc: 0.6716 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9570 - acc: 0.6626 - top5-acc: 0.9716 - val_loss: 0.8982 - val_acc: 0.6924 - val_top5-acc: 0.9752 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.9297 - acc: 0.6715 - top5-acc: 0.9729 - val_loss: 0.8940 - val_acc: 0.7000 - val_top5-acc: 0.9758 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8985 - acc: 0.6850 - top5-acc: 0.9737 - val_loss: 0.8634 - val_acc: 0.7058 - val_top5-acc: 0.9772 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8711 - acc: 0.6916 - top5-acc: 0.9774 - val_loss: 0.8493 - val_acc: 0.7118 - val_top5-acc: 0.9788 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8535 - acc: 0.6990 - top5-acc: 0.9779 - val_loss: 0.8246 - val_acc: 0.7186 - val_top5-acc: 0.9802 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8361 - acc: 0.7052 - top5-acc: 0.9778 - val_loss: 0.8172 - val_acc: 0.7222 - val_top5-acc: 0.9794 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.8175 - acc: 0.7118 - top5-acc: 0.9795 - val_loss: 0.8115 - val_acc: 0.7226 - val_top5-acc: 0.9798 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8040 - acc: 0.7162 - top5-acc: 0.9806 - val_loss: 0.7963 - val_acc: 0.7280 - val_top5-acc: 0.9798 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7981 - acc: 0.7179 - top5-acc: 0.9813 - val_loss: 0.8170 - val_acc: 0.7262 - val_top5-acc: 0.9796 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7759 - acc: 0.7271 - top5-acc: 0.9810 - val_loss: 0.7918 - val_acc: 0.7318 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7716 - acc: 0.7266 - top5-acc: 0.9821 - val_loss: 0.7870 - val_acc: 0.7350 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7638 - acc: 0.7312 - top5-acc: 0.9819 - val_loss: 0.7885 - val_acc: 0.7310 - val_top5-acc: 0.9802 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7624 - acc: 0.7272 - top5-acc: 0.9829 - val_loss: 0.7815 - val_acc: 0.7334 - val_top5-acc: 0.9808 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7465 - acc: 0.7362 - top5-acc: 0.9834 - val_loss: 0.7650 - val_acc: 0.7380 - val_top5-acc: 0.9826 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7437 - acc: 0.7366 - top5-acc: 0.9844 - val_loss: 0.8058 - val_acc: 0.7284 - val_top5-acc: 0.9788 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7436 - acc: 0.7349 - top5-acc: 0.9837 - val_loss: 0.7780 - val_acc: 0.7350 - val_top5-acc: 0.9808 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7303 - acc: 0.7413 - top5-acc: 0.9840 - val_loss: 0.7674 - val_acc: 0.7404 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7251 - acc: 0.7440 - top5-acc: 0.9846 - val_loss: 0.7827 - val_acc: 0.7364 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7225 - acc: 0.7448 - top5-acc: 0.9848 - val_loss: 0.7548 - val_acc: 0.7456 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7129 - acc: 0.7478 - top5-acc: 0.9850 - val_loss: 0.7739 - val_acc: 0.7408 - val_top5-acc: 0.9824 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7092 - acc: 0.7487 - top5-acc: 0.9862 - val_loss: 0.7750 - val_acc: 0.7350 - val_top5-acc: 0.9812 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7094 - acc: 0.7480 - top5-acc: 0.9856 - val_loss: 0.7993 - val_acc: 0.7326 - val_top5-acc: 0.9802 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7076 - acc: 0.7484 - top5-acc: 0.9854 - val_loss: 0.7770 - val_acc: 0.7424 - val_top5-acc: 0.9842 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6954 - acc: 0.7542 - top5-acc: 0.9854 - val_loss: 0.7819 - val_acc: 0.7320 - val_top5-acc: 0.9840 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6446 - acc: 0.7727 - top5-acc: 0.9884 - val_loss: 0.7138 - val_acc: 0.7608 - val_top5-acc: 0.9856 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.7614 - acc: 0.7422 - top5-acc: 0.9832\n",
      "Test accuracy: 74.22%\n",
      "Test top 5 accuracy: 98.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_60_layer_call_fn, layer_normalization_60_layer_call_and_return_conditional_losses, layer_normalization_61_layer_call_fn, layer_normalization_61_layer_call_and_return_conditional_losses, layer_normalization_62_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 28s 243ms/step - loss: 4.2709 - acc: 0.2501 - top5-acc: 0.7455 - val_loss: 1.6160 - val_acc: 0.4190 - val_top5-acc: 0.8970 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5643 - acc: 0.4350 - top5-acc: 0.9026 - val_loss: 1.4074 - val_acc: 0.4972 - val_top5-acc: 0.9350 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.4402 - acc: 0.4838 - top5-acc: 0.9229 - val_loss: 1.3475 - val_acc: 0.5270 - val_top5-acc: 0.9416 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3345 - acc: 0.5209 - top5-acc: 0.9356 - val_loss: 1.2315 - val_acc: 0.5588 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2993 - acc: 0.5340 - top5-acc: 0.9402 - val_loss: 1.2223 - val_acc: 0.5768 - val_top5-acc: 0.9504 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2577 - acc: 0.5519 - top5-acc: 0.9456 - val_loss: 1.1551 - val_acc: 0.5950 - val_top5-acc: 0.9594 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2031 - acc: 0.5711 - top5-acc: 0.9513 - val_loss: 1.1253 - val_acc: 0.5996 - val_top5-acc: 0.9590 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1744 - acc: 0.5823 - top5-acc: 0.9535 - val_loss: 1.0794 - val_acc: 0.6182 - val_top5-acc: 0.9616 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1470 - acc: 0.5912 - top5-acc: 0.9561 - val_loss: 1.0803 - val_acc: 0.6166 - val_top5-acc: 0.9648 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1132 - acc: 0.6065 - top5-acc: 0.9574 - val_loss: 1.0419 - val_acc: 0.6296 - val_top5-acc: 0.9668 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1049 - acc: 0.6079 - top5-acc: 0.9589 - val_loss: 0.9980 - val_acc: 0.6374 - val_top5-acc: 0.9694 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0759 - acc: 0.6180 - top5-acc: 0.9626 - val_loss: 1.0024 - val_acc: 0.6486 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0640 - acc: 0.6239 - top5-acc: 0.9620 - val_loss: 0.9642 - val_acc: 0.6612 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0301 - acc: 0.6333 - top5-acc: 0.9657 - val_loss: 0.9619 - val_acc: 0.6676 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0222 - acc: 0.6378 - top5-acc: 0.9661 - val_loss: 0.9804 - val_acc: 0.6536 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0061 - acc: 0.6426 - top5-acc: 0.9675 - val_loss: 0.9447 - val_acc: 0.6744 - val_top5-acc: 0.9712 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9836 - acc: 0.6518 - top5-acc: 0.9693 - val_loss: 0.9575 - val_acc: 0.6644 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9628 - acc: 0.6602 - top5-acc: 0.9712 - val_loss: 0.8987 - val_acc: 0.6842 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9490 - acc: 0.6637 - top5-acc: 0.9719 - val_loss: 0.9190 - val_acc: 0.6742 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9424 - acc: 0.6681 - top5-acc: 0.9716 - val_loss: 0.9201 - val_acc: 0.6770 - val_top5-acc: 0.9742 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9194 - acc: 0.6749 - top5-acc: 0.9738 - val_loss: 0.8879 - val_acc: 0.6924 - val_top5-acc: 0.9746 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9019 - acc: 0.6825 - top5-acc: 0.9755 - val_loss: 0.8772 - val_acc: 0.6994 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8979 - acc: 0.6830 - top5-acc: 0.9752 - val_loss: 0.8822 - val_acc: 0.6918 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9010 - acc: 0.6806 - top5-acc: 0.9741 - val_loss: 0.8812 - val_acc: 0.6984 - val_top5-acc: 0.9740 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8779 - acc: 0.6907 - top5-acc: 0.9765 - val_loss: 0.8448 - val_acc: 0.7112 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8689 - acc: 0.6958 - top5-acc: 0.9762 - val_loss: 0.8456 - val_acc: 0.7006 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8411 - acc: 0.7038 - top5-acc: 0.9790 - val_loss: 0.8201 - val_acc: 0.7058 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 6.5298 - acc: 0.6020 - top5-acc: 0.9160 - val_loss: 38.3749 - val_acc: 0.1608 - val_top5-acc: 0.6058 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 5.1075 - acc: 0.3622 - top5-acc: 0.8351 - val_loss: 1.3642 - val_acc: 0.5198 - val_top5-acc: 0.9392 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2550 - acc: 0.5510 - top5-acc: 0.9453 - val_loss: 1.1027 - val_acc: 0.6152 - val_top5-acc: 0.9602 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1227 - acc: 0.6012 - top5-acc: 0.9590 - val_loss: 1.0115 - val_acc: 0.6484 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0491 - acc: 0.6280 - top5-acc: 0.9643 - val_loss: 0.9498 - val_acc: 0.6742 - val_top5-acc: 0.9734 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9722 - acc: 0.6572 - top5-acc: 0.9713 - val_loss: 0.9112 - val_acc: 0.6794 - val_top5-acc: 0.9768 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9476 - acc: 0.6634 - top5-acc: 0.9709 - val_loss: 0.8895 - val_acc: 0.6918 - val_top5-acc: 0.9768 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9219 - acc: 0.6739 - top5-acc: 0.9738 - val_loss: 0.8599 - val_acc: 0.7012 - val_top5-acc: 0.9786 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8950 - acc: 0.6848 - top5-acc: 0.9756 - val_loss: 0.8547 - val_acc: 0.7022 - val_top5-acc: 0.9792 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8758 - acc: 0.6909 - top5-acc: 0.9765 - val_loss: 0.8458 - val_acc: 0.7032 - val_top5-acc: 0.9792 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8452 - acc: 0.7014 - top5-acc: 0.9781 - val_loss: 0.8144 - val_acc: 0.7168 - val_top5-acc: 0.9810 - lr: 0.0012\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8317 - acc: 0.7056 - top5-acc: 0.9790 - val_loss: 0.8053 - val_acc: 0.7164 - val_top5-acc: 0.9820 - lr: 0.0012\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8238 - acc: 0.7088 - top5-acc: 0.9796 - val_loss: 0.8042 - val_acc: 0.7196 - val_top5-acc: 0.9820 - lr: 0.0012\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8181 - acc: 0.7126 - top5-acc: 0.9792 - val_loss: 0.8032 - val_acc: 0.7200 - val_top5-acc: 0.9826 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8026 - acc: 0.7157 - top5-acc: 0.9813 - val_loss: 0.7908 - val_acc: 0.7230 - val_top5-acc: 0.9818 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7998 - acc: 0.7193 - top5-acc: 0.9808 - val_loss: 0.7800 - val_acc: 0.7258 - val_top5-acc: 0.9816 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7883 - acc: 0.7235 - top5-acc: 0.9809 - val_loss: 0.7816 - val_acc: 0.7238 - val_top5-acc: 0.9822 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7791 - acc: 0.7236 - top5-acc: 0.9817 - val_loss: 0.7750 - val_acc: 0.7284 - val_top5-acc: 0.9828 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7665 - acc: 0.7306 - top5-acc: 0.9826 - val_loss: 0.7734 - val_acc: 0.7332 - val_top5-acc: 0.9826 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7689 - acc: 0.7295 - top5-acc: 0.9820 - val_loss: 0.7694 - val_acc: 0.7294 - val_top5-acc: 0.9812 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7616 - acc: 0.7305 - top5-acc: 0.9821 - val_loss: 0.7778 - val_acc: 0.7288 - val_top5-acc: 0.9826 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7545 - acc: 0.7360 - top5-acc: 0.9821 - val_loss: 0.7678 - val_acc: 0.7350 - val_top5-acc: 0.9830 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7488 - acc: 0.7359 - top5-acc: 0.9827 - val_loss: 0.7510 - val_acc: 0.7372 - val_top5-acc: 0.9828 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.7785 - acc: 0.7297 - top5-acc: 0.9813\n",
      "Test accuracy: 72.97%\n",
      "Test top 5 accuracy: 98.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_72_layer_call_fn, layer_normalization_72_layer_call_and_return_conditional_losses, layer_normalization_73_layer_call_fn, layer_normalization_73_layer_call_and_return_conditional_losses, layer_normalization_74_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_7\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 245ms/step - loss: 4.1155 - acc: 0.2505 - top5-acc: 0.7437 - val_loss: 1.6489 - val_acc: 0.4124 - val_top5-acc: 0.8914 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5885 - acc: 0.4224 - top5-acc: 0.8988 - val_loss: 1.4443 - val_acc: 0.4896 - val_top5-acc: 0.9280 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.4355 - acc: 0.4826 - top5-acc: 0.9242 - val_loss: 1.2843 - val_acc: 0.5350 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3628 - acc: 0.5084 - top5-acc: 0.9321 - val_loss: 1.2649 - val_acc: 0.5444 - val_top5-acc: 0.9516 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2986 - acc: 0.5339 - top5-acc: 0.9417 - val_loss: 1.2300 - val_acc: 0.5628 - val_top5-acc: 0.9498 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2649 - acc: 0.5493 - top5-acc: 0.9440 - val_loss: 1.1701 - val_acc: 0.5834 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2043 - acc: 0.5680 - top5-acc: 0.9517 - val_loss: 1.1509 - val_acc: 0.5960 - val_top5-acc: 0.9584 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1811 - acc: 0.5793 - top5-acc: 0.9535 - val_loss: 1.0794 - val_acc: 0.6188 - val_top5-acc: 0.9660 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1405 - acc: 0.5951 - top5-acc: 0.9565 - val_loss: 1.0666 - val_acc: 0.6274 - val_top5-acc: 0.9652 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1171 - acc: 0.6014 - top5-acc: 0.9590 - val_loss: 1.0326 - val_acc: 0.6360 - val_top5-acc: 0.9644 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0969 - acc: 0.6107 - top5-acc: 0.9609 - val_loss: 1.0338 - val_acc: 0.6378 - val_top5-acc: 0.9640 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0619 - acc: 0.6220 - top5-acc: 0.9627 - val_loss: 0.9919 - val_acc: 0.6460 - val_top5-acc: 0.9724 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0685 - acc: 0.6234 - top5-acc: 0.9622 - val_loss: 1.0188 - val_acc: 0.6446 - val_top5-acc: 0.9686 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0255 - acc: 0.6341 - top5-acc: 0.9672 - val_loss: 1.0078 - val_acc: 0.6486 - val_top5-acc: 0.9724 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0142 - acc: 0.6420 - top5-acc: 0.9674 - val_loss: 0.9718 - val_acc: 0.6644 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9868 - acc: 0.6501 - top5-acc: 0.9705 - val_loss: 0.9215 - val_acc: 0.6818 - val_top5-acc: 0.9734 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9613 - acc: 0.6610 - top5-acc: 0.9708 - val_loss: 0.8949 - val_acc: 0.6928 - val_top5-acc: 0.9742 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9609 - acc: 0.6595 - top5-acc: 0.9703 - val_loss: 0.9184 - val_acc: 0.6758 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9426 - acc: 0.6667 - top5-acc: 0.9716 - val_loss: 0.8853 - val_acc: 0.6950 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9307 - acc: 0.6730 - top5-acc: 0.9733 - val_loss: 0.8973 - val_acc: 0.6884 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9198 - acc: 0.6754 - top5-acc: 0.9734 - val_loss: 0.8647 - val_acc: 0.6966 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9186 - acc: 0.6755 - top5-acc: 0.9740 - val_loss: 0.8765 - val_acc: 0.6932 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9049 - acc: 0.6807 - top5-acc: 0.9759 - val_loss: 0.8910 - val_acc: 0.6934 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8921 - acc: 0.6875 - top5-acc: 0.9757 - val_loss: 0.8520 - val_acc: 0.7050 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8721 - acc: 0.6924 - top5-acc: 0.9770 - val_loss: 0.8942 - val_acc: 0.6854 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8739 - acc: 0.6902 - top5-acc: 0.9772 - val_loss: 0.8542 - val_acc: 0.6972 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 6.2800 - acc: 0.5457 - top5-acc: 0.8857 - val_loss: 7.0323 - val_acc: 0.2758 - val_top5-acc: 0.7616 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.9119 - acc: 0.4604 - top5-acc: 0.9025 - val_loss: 1.1019 - val_acc: 0.6150 - val_top5-acc: 0.9616 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1179 - acc: 0.6032 - top5-acc: 0.9596 - val_loss: 0.9754 - val_acc: 0.6550 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0085 - acc: 0.6407 - top5-acc: 0.9679 - val_loss: 0.9116 - val_acc: 0.6856 - val_top5-acc: 0.9750 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9596 - acc: 0.6609 - top5-acc: 0.9706 - val_loss: 0.8833 - val_acc: 0.6990 - val_top5-acc: 0.9786 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9223 - acc: 0.6739 - top5-acc: 0.9737 - val_loss: 0.8549 - val_acc: 0.7124 - val_top5-acc: 0.9804 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8935 - acc: 0.6826 - top5-acc: 0.9762 - val_loss: 0.8356 - val_acc: 0.7138 - val_top5-acc: 0.9786 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8684 - acc: 0.6920 - top5-acc: 0.9770 - val_loss: 0.8219 - val_acc: 0.7208 - val_top5-acc: 0.9788 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8491 - acc: 0.7017 - top5-acc: 0.9780 - val_loss: 0.8369 - val_acc: 0.7184 - val_top5-acc: 0.9820 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8282 - acc: 0.7082 - top5-acc: 0.9798 - val_loss: 0.8019 - val_acc: 0.7238 - val_top5-acc: 0.9794 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8083 - acc: 0.7142 - top5-acc: 0.9798 - val_loss: 0.7870 - val_acc: 0.7314 - val_top5-acc: 0.9826 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7973 - acc: 0.7198 - top5-acc: 0.9814 - val_loss: 0.7769 - val_acc: 0.7290 - val_top5-acc: 0.9826 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7807 - acc: 0.7242 - top5-acc: 0.9813 - val_loss: 0.7835 - val_acc: 0.7304 - val_top5-acc: 0.9824 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7784 - acc: 0.7245 - top5-acc: 0.9821 - val_loss: 0.7699 - val_acc: 0.7320 - val_top5-acc: 0.9848 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7649 - acc: 0.7312 - top5-acc: 0.9822 - val_loss: 0.7653 - val_acc: 0.7366 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7532 - acc: 0.7344 - top5-acc: 0.9828 - val_loss: 0.7653 - val_acc: 0.7376 - val_top5-acc: 0.9848 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7451 - acc: 0.7385 - top5-acc: 0.9834 - val_loss: 0.7790 - val_acc: 0.7338 - val_top5-acc: 0.9844 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7362 - acc: 0.7409 - top5-acc: 0.9837 - val_loss: 0.7520 - val_acc: 0.7402 - val_top5-acc: 0.9822 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7270 - acc: 0.7457 - top5-acc: 0.9846 - val_loss: 0.7620 - val_acc: 0.7388 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7278 - acc: 0.7436 - top5-acc: 0.9854 - val_loss: 0.7513 - val_acc: 0.7422 - val_top5-acc: 0.9840 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7220 - acc: 0.7448 - top5-acc: 0.9850 - val_loss: 0.7770 - val_acc: 0.7318 - val_top5-acc: 0.9838 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7086 - acc: 0.7507 - top5-acc: 0.9855 - val_loss: 0.7571 - val_acc: 0.7396 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7084 - acc: 0.7496 - top5-acc: 0.9852 - val_loss: 0.7678 - val_acc: 0.7396 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7062 - acc: 0.7513 - top5-acc: 0.9858 - val_loss: 0.7449 - val_acc: 0.7412 - val_top5-acc: 0.9848 - lr: 0.0025\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.7742 - acc: 0.7384 - top5-acc: 0.9814\n",
      "Test accuracy: 73.84%\n",
      "Test top 5 accuracy: 98.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_84_layer_call_fn, layer_normalization_84_layer_call_and_return_conditional_losses, layer_normalization_85_layer_call_fn, layer_normalization_85_layer_call_and_return_conditional_losses, layer_normalization_86_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 244ms/step - loss: 4.1407 - acc: 0.2541 - top5-acc: 0.7445 - val_loss: 1.7104 - val_acc: 0.3828 - val_top5-acc: 0.8878 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5731 - acc: 0.4304 - top5-acc: 0.9031 - val_loss: 1.3740 - val_acc: 0.5090 - val_top5-acc: 0.9350 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.4249 - acc: 0.4874 - top5-acc: 0.9234 - val_loss: 1.2982 - val_acc: 0.5298 - val_top5-acc: 0.9426 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3546 - acc: 0.5124 - top5-acc: 0.9340 - val_loss: 1.2246 - val_acc: 0.5538 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2803 - acc: 0.5422 - top5-acc: 0.9426 - val_loss: 1.1809 - val_acc: 0.5712 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2368 - acc: 0.5574 - top5-acc: 0.9470 - val_loss: 1.1118 - val_acc: 0.6018 - val_top5-acc: 0.9604 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2044 - acc: 0.5708 - top5-acc: 0.9498 - val_loss: 1.1268 - val_acc: 0.5900 - val_top5-acc: 0.9632 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1738 - acc: 0.5828 - top5-acc: 0.9534 - val_loss: 1.0990 - val_acc: 0.6002 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1308 - acc: 0.5996 - top5-acc: 0.9578 - val_loss: 1.0884 - val_acc: 0.6180 - val_top5-acc: 0.9670 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0997 - acc: 0.6097 - top5-acc: 0.9601 - val_loss: 1.0115 - val_acc: 0.6386 - val_top5-acc: 0.9692 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0959 - acc: 0.6105 - top5-acc: 0.9598 - val_loss: 1.0415 - val_acc: 0.6316 - val_top5-acc: 0.9624 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0703 - acc: 0.6203 - top5-acc: 0.9623 - val_loss: 1.0019 - val_acc: 0.6410 - val_top5-acc: 0.9710 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0570 - acc: 0.6256 - top5-acc: 0.9628 - val_loss: 0.9777 - val_acc: 0.6556 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0396 - acc: 0.6315 - top5-acc: 0.9645 - val_loss: 0.9504 - val_acc: 0.6648 - val_top5-acc: 0.9706 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0004 - acc: 0.6485 - top5-acc: 0.9670 - val_loss: 0.9086 - val_acc: 0.6788 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9954 - acc: 0.6478 - top5-acc: 0.9673 - val_loss: 0.9240 - val_acc: 0.6832 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9869 - acc: 0.6533 - top5-acc: 0.9687 - val_loss: 0.9273 - val_acc: 0.6784 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9499 - acc: 0.6627 - top5-acc: 0.9716 - val_loss: 0.9185 - val_acc: 0.6782 - val_top5-acc: 0.9740 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9444 - acc: 0.6654 - top5-acc: 0.9717 - val_loss: 0.8771 - val_acc: 0.6926 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9321 - acc: 0.6705 - top5-acc: 0.9734 - val_loss: 0.9817 - val_acc: 0.6652 - val_top5-acc: 0.9706 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9390 - acc: 0.6700 - top5-acc: 0.9716 - val_loss: 0.8954 - val_acc: 0.6888 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9201 - acc: 0.6730 - top5-acc: 0.9745 - val_loss: 0.8550 - val_acc: 0.6964 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9242 - acc: 0.6749 - top5-acc: 0.9724 - val_loss: 0.9261 - val_acc: 0.6762 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8960 - acc: 0.6845 - top5-acc: 0.9746 - val_loss: 0.8451 - val_acc: 0.7022 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8793 - acc: 0.6905 - top5-acc: 0.9754 - val_loss: 0.8605 - val_acc: 0.6948 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8705 - acc: 0.6932 - top5-acc: 0.9757 - val_loss: 0.9068 - val_acc: 0.6862 - val_top5-acc: 0.9742 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8750 - acc: 0.6902 - top5-acc: 0.9765 - val_loss: 0.8740 - val_acc: 0.6956 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8464 - acc: 0.7019 - top5-acc: 0.9781 - val_loss: 0.8490 - val_acc: 0.7134 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8437 - acc: 0.7047 - top5-acc: 0.9774 - val_loss: 0.8522 - val_acc: 0.7022 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7563 - acc: 0.7322 - top5-acc: 0.9827 - val_loss: 0.7647 - val_acc: 0.7334 - val_top5-acc: 0.9840 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7314 - acc: 0.7420 - top5-acc: 0.9843 - val_loss: 0.7629 - val_acc: 0.7402 - val_top5-acc: 0.9840 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7214 - acc: 0.7486 - top5-acc: 0.9840 - val_loss: 0.8022 - val_acc: 0.7258 - val_top5-acc: 0.9810 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7215 - acc: 0.7451 - top5-acc: 0.9849 - val_loss: 0.7541 - val_acc: 0.7412 - val_top5-acc: 0.9838 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7170 - acc: 0.7458 - top5-acc: 0.9851 - val_loss: 0.7639 - val_acc: 0.7330 - val_top5-acc: 0.9848 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7102 - acc: 0.7484 - top5-acc: 0.9853 - val_loss: 0.7650 - val_acc: 0.7348 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7017 - acc: 0.7510 - top5-acc: 0.9856 - val_loss: 0.7457 - val_acc: 0.7390 - val_top5-acc: 0.9870 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7013 - acc: 0.7529 - top5-acc: 0.9857 - val_loss: 0.7476 - val_acc: 0.7382 - val_top5-acc: 0.9856 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6966 - acc: 0.7558 - top5-acc: 0.9856 - val_loss: 0.7395 - val_acc: 0.7470 - val_top5-acc: 0.9860 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6939 - acc: 0.7555 - top5-acc: 0.9862 - val_loss: 0.7636 - val_acc: 0.7360 - val_top5-acc: 0.9854 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6899 - acc: 0.7564 - top5-acc: 0.9866 - val_loss: 0.7386 - val_acc: 0.7472 - val_top5-acc: 0.9860 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6882 - acc: 0.7570 - top5-acc: 0.9864 - val_loss: 0.7479 - val_acc: 0.7412 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6855 - acc: 0.7604 - top5-acc: 0.9861 - val_loss: 0.7473 - val_acc: 0.7486 - val_top5-acc: 0.9858 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6651 - acc: 0.7665 - top5-acc: 0.9866 - val_loss: 0.7372 - val_acc: 0.7486 - val_top5-acc: 0.9860 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6731 - acc: 0.7630 - top5-acc: 0.9865 - val_loss: 0.7475 - val_acc: 0.7412 - val_top5-acc: 0.9878 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6705 - acc: 0.7622 - top5-acc: 0.9875 - val_loss: 0.7658 - val_acc: 0.7412 - val_top5-acc: 0.9842 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6667 - acc: 0.7630 - top5-acc: 0.9876 - val_loss: 0.7358 - val_acc: 0.7480 - val_top5-acc: 0.9862 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6563 - acc: 0.7664 - top5-acc: 0.9878 - val_loss: 0.7558 - val_acc: 0.7364 - val_top5-acc: 0.9884 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6536 - acc: 0.7685 - top5-acc: 0.9877 - val_loss: 0.7694 - val_acc: 0.7424 - val_top5-acc: 0.9838 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6441 - acc: 0.7730 - top5-acc: 0.9878 - val_loss: 0.7527 - val_acc: 0.7526 - val_top5-acc: 0.9866 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6450 - acc: 0.7720 - top5-acc: 0.9878 - val_loss: 0.7448 - val_acc: 0.7498 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "313/313 [==============================] - 15s 46ms/step - loss: 0.7840 - acc: 0.7413 - top5-acc: 0.9821\n",
      "Test accuracy: 74.13%\n",
      "Test top 5 accuracy: 98.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_96_layer_call_fn, layer_normalization_96_layer_call_and_return_conditional_losses, layer_normalization_97_layer_call_fn, layer_normalization_97_layer_call_and_return_conditional_losses, layer_normalization_98_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 245ms/step - loss: 4.0948 - acc: 0.2546 - top5-acc: 0.7349 - val_loss: 1.6423 - val_acc: 0.4176 - val_top5-acc: 0.8916 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.5580 - acc: 0.4366 - top5-acc: 0.9046 - val_loss: 1.4172 - val_acc: 0.4954 - val_top5-acc: 0.9262 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.4228 - acc: 0.4878 - top5-acc: 0.9262 - val_loss: 1.2744 - val_acc: 0.5368 - val_top5-acc: 0.9472 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3314 - acc: 0.5234 - top5-acc: 0.9384 - val_loss: 1.2433 - val_acc: 0.5542 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2799 - acc: 0.5416 - top5-acc: 0.9427 - val_loss: 1.1809 - val_acc: 0.5738 - val_top5-acc: 0.9568 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.2255 - acc: 0.5638 - top5-acc: 0.9488 - val_loss: 1.1707 - val_acc: 0.5848 - val_top5-acc: 0.9576 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1911 - acc: 0.5748 - top5-acc: 0.9521 - val_loss: 1.1349 - val_acc: 0.5970 - val_top5-acc: 0.9628 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 21s 242ms/step - loss: 1.1725 - acc: 0.5820 - top5-acc: 0.9539 - val_loss: 1.1054 - val_acc: 0.5980 - val_top5-acc: 0.9596 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.1347 - acc: 0.5949 - top5-acc: 0.9574 - val_loss: 1.0538 - val_acc: 0.6254 - val_top5-acc: 0.9634 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1085 - acc: 0.6073 - top5-acc: 0.9602 - val_loss: 1.0687 - val_acc: 0.6254 - val_top5-acc: 0.9640 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0781 - acc: 0.6168 - top5-acc: 0.9614 - val_loss: 0.9994 - val_acc: 0.6476 - val_top5-acc: 0.9662 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0528 - acc: 0.6240 - top5-acc: 0.9638 - val_loss: 1.0062 - val_acc: 0.6454 - val_top5-acc: 0.9694 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0530 - acc: 0.6266 - top5-acc: 0.9628 - val_loss: 0.9806 - val_acc: 0.6584 - val_top5-acc: 0.9684 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 1.0085 - acc: 0.6392 - top5-acc: 0.9670 - val_loss: 0.9537 - val_acc: 0.6652 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.0041 - acc: 0.6440 - top5-acc: 0.9677 - val_loss: 0.9296 - val_acc: 0.6746 - val_top5-acc: 0.9738 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9933 - acc: 0.6468 - top5-acc: 0.9681 - val_loss: 0.9142 - val_acc: 0.6818 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9615 - acc: 0.6609 - top5-acc: 0.9708 - val_loss: 0.9089 - val_acc: 0.6840 - val_top5-acc: 0.9738 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.9475 - acc: 0.6639 - top5-acc: 0.9723 - val_loss: 0.9254 - val_acc: 0.6762 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9548 - acc: 0.6641 - top5-acc: 0.9719 - val_loss: 0.8793 - val_acc: 0.6902 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9213 - acc: 0.6750 - top5-acc: 0.9744 - val_loss: 0.8849 - val_acc: 0.6908 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9212 - acc: 0.6768 - top5-acc: 0.9736 - val_loss: 0.8752 - val_acc: 0.7048 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.8987 - acc: 0.6827 - top5-acc: 0.9747 - val_loss: 0.8776 - val_acc: 0.7016 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8959 - acc: 0.6836 - top5-acc: 0.9758 - val_loss: 0.8461 - val_acc: 0.7052 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.8723 - acc: 0.6940 - top5-acc: 0.9775 - val_loss: 0.8438 - val_acc: 0.7108 - val_top5-acc: 0.9812 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8560 - acc: 0.6992 - top5-acc: 0.9774 - val_loss: 0.8528 - val_acc: 0.7086 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.8587 - acc: 0.6948 - top5-acc: 0.9772 - val_loss: 0.8607 - val_acc: 0.7080 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8684 - acc: 0.6962 - top5-acc: 0.9768 - val_loss: 0.8349 - val_acc: 0.7144 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8485 - acc: 0.7014 - top5-acc: 0.9782 - val_loss: 0.8212 - val_acc: 0.7180 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8328 - acc: 0.7059 - top5-acc: 0.9787 - val_loss: 0.8125 - val_acc: 0.7190 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.8126 - acc: 0.7145 - top5-acc: 0.9797 - val_loss: 0.8068 - val_acc: 0.7302 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8373 - acc: 0.7055 - top5-acc: 0.9788 - val_loss: 0.8630 - val_acc: 0.7032 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8165 - acc: 0.7121 - top5-acc: 0.9798 - val_loss: 0.8036 - val_acc: 0.7238 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8004 - acc: 0.7165 - top5-acc: 0.9808 - val_loss: 0.8094 - val_acc: 0.7266 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8028 - acc: 0.7154 - top5-acc: 0.9823 - val_loss: 0.8463 - val_acc: 0.7164 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7960 - acc: 0.7194 - top5-acc: 0.9818 - val_loss: 0.8161 - val_acc: 0.7294 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7709 - acc: 0.7277 - top5-acc: 0.9826 - val_loss: 0.7725 - val_acc: 0.7410 - val_top5-acc: 0.9836 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7854 - acc: 0.7220 - top5-acc: 0.9823 - val_loss: 0.7767 - val_acc: 0.7412 - val_top5-acc: 0.9848 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7741 - acc: 0.7268 - top5-acc: 0.9836 - val_loss: 0.7683 - val_acc: 0.7370 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7742 - acc: 0.7289 - top5-acc: 0.9830 - val_loss: 0.7789 - val_acc: 0.7430 - val_top5-acc: 0.9852 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7568 - acc: 0.7358 - top5-acc: 0.9830 - val_loss: 0.7766 - val_acc: 0.7358 - val_top5-acc: 0.9840 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.7357 - acc: 0.7403 - top5-acc: 0.9842 - val_loss: 0.7698 - val_acc: 0.7472 - val_top5-acc: 0.9846 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 16.8527 - acc: 0.4229 - top5-acc: 0.8084 - val_loss: 2.1001 - val_acc: 0.4128 - val_top5-acc: 0.9000 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 1.4807 - acc: 0.4934 - top5-acc: 0.9292 - val_loss: 1.1401 - val_acc: 0.6000 - val_top5-acc: 0.9618 - lr: 0.0050\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1807 - acc: 0.5783 - top5-acc: 0.9532 - val_loss: 1.1020 - val_acc: 0.6148 - val_top5-acc: 0.9668 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1058 - acc: 0.6076 - top5-acc: 0.9603 - val_loss: 1.0181 - val_acc: 0.6426 - val_top5-acc: 0.9724 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0472 - acc: 0.6271 - top5-acc: 0.9651 - val_loss: 0.9384 - val_acc: 0.6748 - val_top5-acc: 0.9762 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.9943 - acc: 0.6498 - top5-acc: 0.9680 - val_loss: 0.9126 - val_acc: 0.6804 - val_top5-acc: 0.9756 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9508 - acc: 0.6631 - top5-acc: 0.9717 - val_loss: 0.8962 - val_acc: 0.6858 - val_top5-acc: 0.9772 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9181 - acc: 0.6739 - top5-acc: 0.9749 - val_loss: 0.8605 - val_acc: 0.6992 - val_top5-acc: 0.9776 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8945 - acc: 0.6836 - top5-acc: 0.9753 - val_loss: 0.8434 - val_acc: 0.7060 - val_top5-acc: 0.9776 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.8767 - acc: 0.6939 - top5-acc: 0.9775\n",
      "Test accuracy: 69.39%\n",
      "Test top 5 accuracy: 97.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_108_layer_call_fn, layer_normalization_108_layer_call_and_return_conditional_losses, layer_normalization_109_layer_call_fn, layer_normalization_109_layer_call_and_return_conditional_losses, layer_normalization_110_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_10\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_10\\assets\n"
     ]
    }
   ],
   "source": [
    "mlpmixer_generator(num_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Results_Article/4A_SC/mlpmixer_B-32_'\n",
    "total_models = list()\n",
    "for k in range(num_models):  \n",
    "    current_model = tf.keras.models.load_model(path + str(k+1))\n",
    "    total_models.append(current_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell once to avoid randomness\n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024098360E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002412FA38A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "total_activations = list()\n",
    "for k in range(num_models):\n",
    "    tested_model = total_models[k] \n",
    "    ave_mixer_activations = Prom_Mixer_Activations_Blocks(tested_model,batch_prepro)\n",
    "    total_activations.append(ave_mixer_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check with RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd='Results_Article/4A_SC'\n",
    "with open(pwd + '/Activations_10Net.pkl','wb') as file:\n",
    "        pickle.dump(total_activations,file)\n",
    "\n",
    "for sigma in rbf_index:\n",
    "    positive_events, total_events = sanity_check(total_activations, num_models, num_blocks,'rbf',sigma)\n",
    "    SC_accuracy = (positive_events/total_events) \n",
    "    print(f\"The Sanity check with a CKA RBF: {sigma} has an accuracy of {round(SC_accuracy * 100, 2)}%\")\n",
    "    #Multiplying by 100 to ensure proper saving of the file\n",
    "    with open(pwd + '/SCaccuracy_RBF'+ str(round(sigma*100))  +'.pkl','wb') as file:\n",
    "        pickle.dump(SC_accuracy,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check with Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_events, total_events = sanity_check(total_activations, num_models, num_blocks,'linear',sigma=None)\n",
    "SC_accuracy = (positive_events/total_events) \n",
    "print(f\"The Sanity check with a CKA linear has an accuracy of {round(SC_accuracy * 100, 2)}%\")\n",
    "with open(pwd + '/SCaccuracy_linear.pkl','wb') as file:\n",
    "        pickle.dump(SC_accuracy,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_image_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
