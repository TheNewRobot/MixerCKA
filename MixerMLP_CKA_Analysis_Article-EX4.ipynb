{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSzgroG_Suq"
   },
   "source": [
    "# Study of Image classification with modern MLP Mixer model and CKA\n",
    "\n",
    "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
    "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U087DdDw_Suy"
   },
   "source": [
    "# Setup for the MLP-Mixer Architecture\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AnTyoluw_Suz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alach\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import pickle\n",
    "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
    "from CKA_Google import *\n",
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 : Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DAOrIHu_Su2"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1iMiVS7o_Su3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224 = 50176\n",
      "Patch size: 32 X 32 = 1024 \n",
      "Patches per image: 49\n",
      "Elements per patch (3 channels): 3072\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 512 \n",
    "num_epochs = 50\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Selected Architecture: B/32\n",
    "\n",
    "image_size = 224  # We'll resize input images to this size. Square\n",
    "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
    "embedding_dim = 384  # Fixed Embedding Dimension\n",
    "num_blocks = 12\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "num_models = 10\n",
    "rbf_index = [0.2,0.4,0.8] # For testing during Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuu2OAS_Su0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for training \n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mpnEZH_Su5"
   },
   "source": [
    "## Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ra-bXojQ_Su6"
   },
   "outputs": [],
   "source": [
    "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data. \n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches. \n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks. ## (sequential_82)\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation) \n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpQFU8H__Su8"
   },
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9E1zD2BY_Su8"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        #Negative Log Likelihood = Categorical Cross Entropy\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping regularization callback. \n",
    "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history, accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxeE0cmM_Su9"
   },
   "source": [
    "## Use data augmentation\n",
    "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zh6hFPWX_Su-"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G77cUgiu_Su-"
   },
   "source": [
    "## Implement patch extraction as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYKNktXe_Su_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        #Extract the shape dimension in the position 0 = columns\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Without overlapping, stride horizontally and vertically\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
    "            rates=[1, 1, 1, 1],\n",
    "            #Patches contained in the images are considered, no zero padding\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        #shape[-1], number of colummns, as well as shape[0]\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Patches, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'patch_size' : self.patch_size ,\n",
    "            'num_patches' : self.num_patches\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yehcSS_Su_"
   },
   "source": [
    "## The MLP-Mixer model\n",
    "\n",
    "The MLP-Mixer is an architecture based exclusively on\n",
    "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "1. One applied independently to image patches, which mixes the per-location features.\n",
    "2. The other applied across patches (along channels), which mixes spatial information.\n",
    "\n",
    "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
    "instead of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwg4e2n_SvA"
   },
   "source": [
    "### Implement the MLP-Mixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dT6wVEki_SvA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp2 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super(MLPMixerLayer, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'num_patches' : num_patches,\n",
    "            'embedding_dim' : embedding_dim,\n",
    "            'dropout_rate' : dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUiufq92_SvA"
   },
   "source": [
    "## Build, train, and evaluate the MLP-Mixer model\n",
    "\n",
    "Note that training the model with the current settings on a V100 GPUs\n",
    "takes around 8 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report: Learning Curve\n",
    "def curves(history):\n",
    "    ymax1 = min(history[\"loss\"])\n",
    "    xmax1 = history[\"loss\"].index(ymax1)\n",
    "    ymax2 = min(history[\"val_loss\"])\n",
    "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
    "    plt.title(\"Cross Entropy Loss\")\n",
    "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
    "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Graph accuracy\n",
    "    ymax3 = max(history[\"acc\"])\n",
    "    xmax3 = history[\"acc\"].index(ymax3)\n",
    "    ymax4 = max(history[\"val_acc\"])\n",
    "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
    "    ymax5 = max(history[\"top5-acc\"])\n",
    "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
    "    ymax6 = max(history[\"val_top5-acc\"])\n",
    "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Classification accuracy')\n",
    "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Classification top5-acc')\n",
    "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.suptitle(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain activations + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers + Patches + One dense layer\n",
    "def Preprocessing(num_example):\n",
    "    augmented = data_augmentation(x_train[num_example])\n",
    "    b = Patches(patch_size, num_patches)(augmented)\n",
    "    a = layers.Dense(units=embedding_dim)(b)\n",
    "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
    "def Batch_Preprocessing(batch_size):\n",
    "    #Vector with the number of Sample of the Xtrain\n",
    "    a  = list(range(0,x_train.shape[0]))\n",
    "    b = random.sample(a,batch_size)\n",
    "    batch_regularization = list()\n",
    "    for i in range(0,batch_size):\n",
    "        inter_result = Preprocessing(b[i])\n",
    "        batch_regularization.append(inter_result)\n",
    "    return batch_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_out(result,layer_number,example):\n",
    "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "    ax1.imshow(x_train[example])\n",
    "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
    "    ax2.imshow(result[layer_number])\n",
    "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
    "    #The input is fixed to the beginning of the mlp blocks\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[0]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of layer's activation\n",
    "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
    "def Heatmap(result,type,sigma):\n",
    "    dim = len(result)\n",
    "    k = (dim - 1)\n",
    "    heatmap_CKA = np.zeros((dim,dim))\n",
    "    for i in range(0,dim):\n",
    "        tr = (dim - 1)\n",
    "        for j in range(0,dim):\n",
    "            if type == 'kernel':\n",
    "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
    "            elif type == 'linear':\n",
    "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
    "            else:\n",
    "                print('There is no such category, try again')\n",
    "                break\n",
    "\n",
    "            tr -= 1\n",
    "        k -= 1\n",
    "    #print('CKA' + type + 'calculated')\n",
    "    return heatmap_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of heatmaps (obsolet)\n",
    "def Prom_Mixer_Heatmaps(batch_result,type):\n",
    "    mat_heatmaps = list()\n",
    "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
    "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
    "        mat_heatmaps.append(heatmap_raw)\n",
    "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
    "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
    "    return prom_mixer_heatmap,mat_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 4\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i'%(i+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(3,3))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 : Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlpmixer_generator(num_models):\n",
    "    #now = datetime.datetime.now()\n",
    "    #date = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    for i in range(num_models):\n",
    "        mlpmixer_blocks = keras.Sequential(\n",
    "        [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
    "        )\n",
    "        mlpmixer_classifier = build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model\n",
    "        history,accuracy, top_5_accuracy = run_experiment(mlpmixer_classifier)\n",
    "        #Saving Results\n",
    "        pwd = 'Results_Article/4A_SC/mlpmixer_B-32_' + str(i+1)\n",
    "        mlpmixer_classifier.save(pwd)\n",
    "        np.save( pwd + '/history.npy',history.history)\n",
    "        with open(pwd + '/accuracy.pkl','wb') as file:\n",
    "            pickle.dump(accuracy,file)\n",
    "        with open(pwd + '/top5-accuracy.pkl','wb') as file:\n",
    "            pickle.dump(top_5_accuracy,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available types = 'rbf' or 'linear'\n",
    "def sanity_check(total_activations,num_models,num_blocks,type,sigma):\n",
    "    row = []\n",
    "    total_events = 0\n",
    "    positive_events = 0\n",
    "    for i in range(num_models-1):\n",
    "        comp_1 = total_activations[i]\n",
    "        for j in range(i+1,num_models):\n",
    "            comp_2 = total_activations[j]\n",
    "            for m in range(num_blocks):\n",
    "                for n in range(num_blocks):\n",
    "                    if type == 'rbf':\n",
    "                        inter_row = cka(gram_rbf(comp_1[m],sigma),gram_rbf(comp_2[n],sigma))\n",
    "                    elif type == 'linear':\n",
    "                        inter_row = cka(gram_linear(comp_1[m]),gram_linear(comp_2[n]))\n",
    "                    row.append(inter_row)\n",
    "                b = [i for i, x in enumerate(row) if x == max(row)]\n",
    "                if len(b) == 1:\n",
    "                    if b[0] == m:\n",
    "                        #print(row)\n",
    "                        #print('Hello, Layer %i'%(m))\n",
    "                        positive_events += 1\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "                total_events +=1\n",
    "                row=[]\n",
    "    return positive_events, total_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 243ms/step - loss: 4.5321 - acc: 0.2397 - top5-acc: 0.7214 - val_loss: 1.7452 - val_acc: 0.3734 - val_top5-acc: 0.8732 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.6104 - acc: 0.4173 - top5-acc: 0.8950 - val_loss: 1.4344 - val_acc: 0.4928 - val_top5-acc: 0.9284 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.4460 - acc: 0.4782 - top5-acc: 0.9233 - val_loss: 1.3376 - val_acc: 0.5268 - val_top5-acc: 0.9370 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.3528 - acc: 0.5129 - top5-acc: 0.9335 - val_loss: 1.2309 - val_acc: 0.5640 - val_top5-acc: 0.9502 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2811 - acc: 0.5407 - top5-acc: 0.9424 - val_loss: 1.1870 - val_acc: 0.5726 - val_top5-acc: 0.9572 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2462 - acc: 0.5541 - top5-acc: 0.9474 - val_loss: 1.1369 - val_acc: 0.5946 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2043 - acc: 0.5670 - top5-acc: 0.9492 - val_loss: 1.0903 - val_acc: 0.6156 - val_top5-acc: 0.9620 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1598 - acc: 0.5870 - top5-acc: 0.9538 - val_loss: 1.0771 - val_acc: 0.6220 - val_top5-acc: 0.9632 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1340 - acc: 0.5958 - top5-acc: 0.9559 - val_loss: 1.0489 - val_acc: 0.6310 - val_top5-acc: 0.9676 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1257 - acc: 0.5970 - top5-acc: 0.9583 - val_loss: 1.0598 - val_acc: 0.6240 - val_top5-acc: 0.9654 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0915 - acc: 0.6136 - top5-acc: 0.9615 - val_loss: 1.0349 - val_acc: 0.6350 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0642 - acc: 0.6228 - top5-acc: 0.9628 - val_loss: 1.0044 - val_acc: 0.6470 - val_top5-acc: 0.9660 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0654 - acc: 0.6219 - top5-acc: 0.9627 - val_loss: 0.9841 - val_acc: 0.6524 - val_top5-acc: 0.9702 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0344 - acc: 0.6344 - top5-acc: 0.9652 - val_loss: 0.9708 - val_acc: 0.6564 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0092 - acc: 0.6386 - top5-acc: 0.9676 - val_loss: 0.9494 - val_acc: 0.6688 - val_top5-acc: 0.9702 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9930 - acc: 0.6505 - top5-acc: 0.9679 - val_loss: 0.9923 - val_acc: 0.6542 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9766 - acc: 0.6551 - top5-acc: 0.9691 - val_loss: 0.9264 - val_acc: 0.6750 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9633 - acc: 0.6593 - top5-acc: 0.9714 - val_loss: 1.0176 - val_acc: 0.6478 - val_top5-acc: 0.9674 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9594 - acc: 0.6599 - top5-acc: 0.9717 - val_loss: 0.8985 - val_acc: 0.6836 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9335 - acc: 0.6702 - top5-acc: 0.9724 - val_loss: 0.9019 - val_acc: 0.6870 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9203 - acc: 0.6746 - top5-acc: 0.9743 - val_loss: 0.9255 - val_acc: 0.6784 - val_top5-acc: 0.9768 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9194 - acc: 0.6754 - top5-acc: 0.9730 - val_loss: 0.8631 - val_acc: 0.7032 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8927 - acc: 0.6846 - top5-acc: 0.9764 - val_loss: 0.8910 - val_acc: 0.6930 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8917 - acc: 0.6838 - top5-acc: 0.9757 - val_loss: 0.8392 - val_acc: 0.7096 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8757 - acc: 0.6916 - top5-acc: 0.9758 - val_loss: 0.8905 - val_acc: 0.6952 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8607 - acc: 0.6972 - top5-acc: 0.9776 - val_loss: 0.8365 - val_acc: 0.7088 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8455 - acc: 0.7038 - top5-acc: 0.9789 - val_loss: 0.8460 - val_acc: 0.7130 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8379 - acc: 0.7048 - top5-acc: 0.9789 - val_loss: 0.8980 - val_acc: 0.6990 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8469 - acc: 0.7007 - top5-acc: 0.9781 - val_loss: 0.8573 - val_acc: 0.7076 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8330 - acc: 0.7046 - top5-acc: 0.9791 - val_loss: 0.8564 - val_acc: 0.7150 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8391 - acc: 0.7046 - top5-acc: 0.9787 - val_loss: 0.8905 - val_acc: 0.6984 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7492 - acc: 0.7365 - top5-acc: 0.9835 - val_loss: 0.7668 - val_acc: 0.7446 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.7093 - acc: 0.7509 - top5-acc: 0.9853 - val_loss: 0.7439 - val_acc: 0.7518 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.7015 - acc: 0.7532 - top5-acc: 0.9859 - val_loss: 0.7407 - val_acc: 0.7500 - val_top5-acc: 0.9862 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.6936 - acc: 0.7556 - top5-acc: 0.9864 - val_loss: 0.7461 - val_acc: 0.7534 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.6908 - acc: 0.7561 - top5-acc: 0.9859 - val_loss: 0.7380 - val_acc: 0.7516 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6847 - acc: 0.7566 - top5-acc: 0.9872 - val_loss: 0.7549 - val_acc: 0.7430 - val_top5-acc: 0.9840 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6794 - acc: 0.7601 - top5-acc: 0.9870 - val_loss: 0.7435 - val_acc: 0.7496 - val_top5-acc: 0.9850 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6798 - acc: 0.7597 - top5-acc: 0.9861 - val_loss: 0.7361 - val_acc: 0.7502 - val_top5-acc: 0.9852 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6728 - acc: 0.7623 - top5-acc: 0.9877 - val_loss: 0.7788 - val_acc: 0.7460 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6717 - acc: 0.7645 - top5-acc: 0.9865 - val_loss: 0.7171 - val_acc: 0.7608 - val_top5-acc: 0.9844 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6706 - acc: 0.7626 - top5-acc: 0.9872 - val_loss: 0.7414 - val_acc: 0.7504 - val_top5-acc: 0.9860 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6663 - acc: 0.7650 - top5-acc: 0.9876 - val_loss: 0.7537 - val_acc: 0.7438 - val_top5-acc: 0.9846 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6644 - acc: 0.7667 - top5-acc: 0.9875 - val_loss: 0.7697 - val_acc: 0.7464 - val_top5-acc: 0.9824 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6606 - acc: 0.7692 - top5-acc: 0.9877 - val_loss: 0.7535 - val_acc: 0.7488 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6601 - acc: 0.7675 - top5-acc: 0.9880 - val_loss: 0.7659 - val_acc: 0.7518 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5920 - acc: 0.7898 - top5-acc: 0.9902 - val_loss: 0.7125 - val_acc: 0.7670 - val_top5-acc: 0.9866 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.5804 - acc: 0.7949 - top5-acc: 0.9902 - val_loss: 0.6968 - val_acc: 0.7688 - val_top5-acc: 0.9880 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5670 - acc: 0.7999 - top5-acc: 0.9916 - val_loss: 0.7315 - val_acc: 0.7598 - val_top5-acc: 0.9862 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5636 - acc: 0.8006 - top5-acc: 0.9914 - val_loss: 0.7009 - val_acc: 0.7648 - val_top5-acc: 0.9880 - lr: 0.0012\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.7207 - acc: 0.7602 - top5-acc: 0.9846\n",
      "Test accuracy: 76.02%\n",
      "Test top 5 accuracy: 98.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn, layer_normalization_1_layer_call_and_return_conditional_losses, layer_normalization_2_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 242ms/step - loss: 4.6662 - acc: 0.2352 - top5-acc: 0.7204 - val_loss: 1.7736 - val_acc: 0.3894 - val_top5-acc: 0.8592 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.6101 - acc: 0.4167 - top5-acc: 0.8932 - val_loss: 1.4676 - val_acc: 0.4788 - val_top5-acc: 0.9238 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.4564 - acc: 0.4776 - top5-acc: 0.9191 - val_loss: 1.3331 - val_acc: 0.5152 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3782 - acc: 0.5030 - top5-acc: 0.9304 - val_loss: 1.2472 - val_acc: 0.5594 - val_top5-acc: 0.9496 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3120 - acc: 0.5300 - top5-acc: 0.9394 - val_loss: 1.1979 - val_acc: 0.5706 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2504 - acc: 0.5529 - top5-acc: 0.9450 - val_loss: 1.1953 - val_acc: 0.5806 - val_top5-acc: 0.9540 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2183 - acc: 0.5641 - top5-acc: 0.9486 - val_loss: 1.1626 - val_acc: 0.5856 - val_top5-acc: 0.9636 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1837 - acc: 0.5796 - top5-acc: 0.9534 - val_loss: 1.1439 - val_acc: 0.5918 - val_top5-acc: 0.9606 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1501 - acc: 0.5902 - top5-acc: 0.9570 - val_loss: 1.0655 - val_acc: 0.6302 - val_top5-acc: 0.9650 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1332 - acc: 0.5977 - top5-acc: 0.9565 - val_loss: 1.0561 - val_acc: 0.6344 - val_top5-acc: 0.9636 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1169 - acc: 0.6032 - top5-acc: 0.9588 - val_loss: 1.0550 - val_acc: 0.6332 - val_top5-acc: 0.9624 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0863 - acc: 0.6129 - top5-acc: 0.9613 - val_loss: 1.0363 - val_acc: 0.6394 - val_top5-acc: 0.9658 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0684 - acc: 0.6208 - top5-acc: 0.9624 - val_loss: 1.0247 - val_acc: 0.6426 - val_top5-acc: 0.9676 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0594 - acc: 0.6256 - top5-acc: 0.9638 - val_loss: 0.9893 - val_acc: 0.6578 - val_top5-acc: 0.9702 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0421 - acc: 0.6298 - top5-acc: 0.9653 - val_loss: 0.9742 - val_acc: 0.6606 - val_top5-acc: 0.9712 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0199 - acc: 0.6370 - top5-acc: 0.9661 - val_loss: 0.9437 - val_acc: 0.6668 - val_top5-acc: 0.9722 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9987 - acc: 0.6444 - top5-acc: 0.9686 - val_loss: 0.9379 - val_acc: 0.6764 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9879 - acc: 0.6476 - top5-acc: 0.9691 - val_loss: 0.9340 - val_acc: 0.6712 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9698 - acc: 0.6571 - top5-acc: 0.9700 - val_loss: 0.8866 - val_acc: 0.6932 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9507 - acc: 0.6648 - top5-acc: 0.9715 - val_loss: 0.9202 - val_acc: 0.6810 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9474 - acc: 0.6648 - top5-acc: 0.9728 - val_loss: 0.9034 - val_acc: 0.6816 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9422 - acc: 0.6673 - top5-acc: 0.9732 - val_loss: 0.9367 - val_acc: 0.6722 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9489 - acc: 0.6672 - top5-acc: 0.9727 - val_loss: 0.8699 - val_acc: 0.7012 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9024 - acc: 0.6827 - top5-acc: 0.9748 - val_loss: 0.8679 - val_acc: 0.7002 - val_top5-acc: 0.9758 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8929 - acc: 0.6829 - top5-acc: 0.9750 - val_loss: 0.8882 - val_acc: 0.6920 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8913 - acc: 0.6846 - top5-acc: 0.9755 - val_loss: 0.8731 - val_acc: 0.6966 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8783 - acc: 0.6909 - top5-acc: 0.9770 - val_loss: 0.8529 - val_acc: 0.7056 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9043 - acc: 0.6848 - top5-acc: 0.9738 - val_loss: 0.8860 - val_acc: 0.6966 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8541 - acc: 0.6992 - top5-acc: 0.9771 - val_loss: 0.8458 - val_acc: 0.7074 - val_top5-acc: 0.9796 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8553 - acc: 0.6973 - top5-acc: 0.9767 - val_loss: 0.8369 - val_acc: 0.7130 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8698 - acc: 0.6942 - top5-acc: 0.9779 - val_loss: 0.8620 - val_acc: 0.7074 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8364 - acc: 0.7055 - top5-acc: 0.9793 - val_loss: 0.8082 - val_acc: 0.7164 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8170 - acc: 0.7101 - top5-acc: 0.9814 - val_loss: 0.8009 - val_acc: 0.7346 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8200 - acc: 0.7120 - top5-acc: 0.9797 - val_loss: 0.8407 - val_acc: 0.7164 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8146 - acc: 0.7110 - top5-acc: 0.9810 - val_loss: 0.8253 - val_acc: 0.7192 - val_top5-acc: 0.9826 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7990 - acc: 0.7185 - top5-acc: 0.9804 - val_loss: 0.8270 - val_acc: 0.7200 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8196 - acc: 0.7140 - top5-acc: 0.9798 - val_loss: 0.8152 - val_acc: 0.7276 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8072 - acc: 0.7149 - top5-acc: 0.9804 - val_loss: 0.8265 - val_acc: 0.7296 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7032 - acc: 0.7513 - top5-acc: 0.9867 - val_loss: 0.7418 - val_acc: 0.7498 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6742 - acc: 0.7619 - top5-acc: 0.9867 - val_loss: 0.7362 - val_acc: 0.7512 - val_top5-acc: 0.9846 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6700 - acc: 0.7627 - top5-acc: 0.9879 - val_loss: 0.7720 - val_acc: 0.7428 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6621 - acc: 0.7666 - top5-acc: 0.9876 - val_loss: 0.7413 - val_acc: 0.7504 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6561 - acc: 0.7684 - top5-acc: 0.9880 - val_loss: 0.7464 - val_acc: 0.7526 - val_top5-acc: 0.9844 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6523 - acc: 0.7690 - top5-acc: 0.9879 - val_loss: 0.7608 - val_acc: 0.7396 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6483 - acc: 0.7691 - top5-acc: 0.9887 - val_loss: 0.7494 - val_acc: 0.7452 - val_top5-acc: 0.9852 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6020 - acc: 0.7868 - top5-acc: 0.9906 - val_loss: 0.7225 - val_acc: 0.7598 - val_top5-acc: 0.9852 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5836 - acc: 0.7910 - top5-acc: 0.9908 - val_loss: 0.7022 - val_acc: 0.7626 - val_top5-acc: 0.9866 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5737 - acc: 0.7961 - top5-acc: 0.9910 - val_loss: 0.7055 - val_acc: 0.7628 - val_top5-acc: 0.9872 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5683 - acc: 0.7972 - top5-acc: 0.9921 - val_loss: 0.7162 - val_acc: 0.7598 - val_top5-acc: 0.9872 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5721 - acc: 0.7959 - top5-acc: 0.9910 - val_loss: 0.7335 - val_acc: 0.7548 - val_top5-acc: 0.9848 - lr: 0.0012\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.7598 - acc: 0.7477 - top5-acc: 0.9835\n",
      "Test accuracy: 74.77%\n",
      "Test top 5 accuracy: 98.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_12_layer_call_fn, layer_normalization_12_layer_call_and_return_conditional_losses, layer_normalization_13_layer_call_fn, layer_normalization_13_layer_call_and_return_conditional_losses, layer_normalization_14_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 243ms/step - loss: 4.0146 - acc: 0.2485 - top5-acc: 0.7383 - val_loss: 1.6237 - val_acc: 0.4152 - val_top5-acc: 0.8918 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.5747 - acc: 0.4307 - top5-acc: 0.9013 - val_loss: 1.4955 - val_acc: 0.4736 - val_top5-acc: 0.9148 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.4336 - acc: 0.4840 - top5-acc: 0.9243 - val_loss: 1.3574 - val_acc: 0.5146 - val_top5-acc: 0.9382 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3543 - acc: 0.5140 - top5-acc: 0.9352 - val_loss: 1.2766 - val_acc: 0.5424 - val_top5-acc: 0.9474 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2845 - acc: 0.5380 - top5-acc: 0.9424 - val_loss: 1.1941 - val_acc: 0.5760 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2394 - acc: 0.5600 - top5-acc: 0.9464 - val_loss: 1.1230 - val_acc: 0.6054 - val_top5-acc: 0.9588 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1743 - acc: 0.5798 - top5-acc: 0.9528 - val_loss: 1.1527 - val_acc: 0.5992 - val_top5-acc: 0.9600 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1602 - acc: 0.5893 - top5-acc: 0.9540 - val_loss: 1.0586 - val_acc: 0.6296 - val_top5-acc: 0.9674 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1306 - acc: 0.5949 - top5-acc: 0.9599 - val_loss: 1.0388 - val_acc: 0.6246 - val_top5-acc: 0.9686 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1028 - acc: 0.6064 - top5-acc: 0.9590 - val_loss: 1.1581 - val_acc: 0.6012 - val_top5-acc: 0.9548 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0776 - acc: 0.6177 - top5-acc: 0.9606 - val_loss: 1.0385 - val_acc: 0.6358 - val_top5-acc: 0.9708 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0646 - acc: 0.6236 - top5-acc: 0.9631 - val_loss: 0.9865 - val_acc: 0.6496 - val_top5-acc: 0.9704 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0357 - acc: 0.6316 - top5-acc: 0.9653 - val_loss: 1.0229 - val_acc: 0.6350 - val_top5-acc: 0.9682 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0218 - acc: 0.6373 - top5-acc: 0.9669 - val_loss: 0.9499 - val_acc: 0.6662 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9959 - acc: 0.6471 - top5-acc: 0.9675 - val_loss: 0.9202 - val_acc: 0.6688 - val_top5-acc: 0.9760 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9864 - acc: 0.6500 - top5-acc: 0.9684 - val_loss: 0.9120 - val_acc: 0.6750 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9619 - acc: 0.6600 - top5-acc: 0.9705 - val_loss: 0.9066 - val_acc: 0.6842 - val_top5-acc: 0.9768 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9576 - acc: 0.6632 - top5-acc: 0.9707 - val_loss: 0.8917 - val_acc: 0.6822 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9399 - acc: 0.6695 - top5-acc: 0.9716 - val_loss: 0.9200 - val_acc: 0.6758 - val_top5-acc: 0.9746 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9350 - acc: 0.6705 - top5-acc: 0.9726 - val_loss: 0.9435 - val_acc: 0.6714 - val_top5-acc: 0.9744 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9281 - acc: 0.6724 - top5-acc: 0.9734 - val_loss: 0.8841 - val_acc: 0.6950 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8910 - acc: 0.6862 - top5-acc: 0.9760 - val_loss: 0.8366 - val_acc: 0.7090 - val_top5-acc: 0.9816 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8879 - acc: 0.6873 - top5-acc: 0.9749 - val_loss: 0.8625 - val_acc: 0.6978 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8843 - acc: 0.6916 - top5-acc: 0.9739 - val_loss: 0.8467 - val_acc: 0.7038 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8735 - acc: 0.6925 - top5-acc: 0.9768 - val_loss: 0.8638 - val_acc: 0.6978 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8498 - acc: 0.7010 - top5-acc: 0.9777 - val_loss: 0.8303 - val_acc: 0.7110 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8554 - acc: 0.6982 - top5-acc: 0.9784 - val_loss: 0.8104 - val_acc: 0.7172 - val_top5-acc: 0.9820 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8381 - acc: 0.7067 - top5-acc: 0.9794 - val_loss: 0.7962 - val_acc: 0.7204 - val_top5-acc: 0.9832 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8422 - acc: 0.7038 - top5-acc: 0.9792 - val_loss: 0.8524 - val_acc: 0.7066 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8228 - acc: 0.7117 - top5-acc: 0.9784 - val_loss: 0.8256 - val_acc: 0.7164 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8218 - acc: 0.7104 - top5-acc: 0.9803 - val_loss: 0.7904 - val_acc: 0.7174 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8153 - acc: 0.7147 - top5-acc: 0.9808 - val_loss: 0.8130 - val_acc: 0.7208 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8141 - acc: 0.7151 - top5-acc: 0.9814 - val_loss: 0.8094 - val_acc: 0.7204 - val_top5-acc: 0.9840 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.7966 - acc: 0.7198 - top5-acc: 0.9821 - val_loss: 0.8016 - val_acc: 0.7284 - val_top5-acc: 0.9848 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8009 - acc: 0.7204 - top5-acc: 0.9810 - val_loss: 0.8347 - val_acc: 0.7190 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.7828 - acc: 0.7238 - top5-acc: 0.9817 - val_loss: 0.7972 - val_acc: 0.7266 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6905 - acc: 0.7578 - top5-acc: 0.9859 - val_loss: 0.7251 - val_acc: 0.7492 - val_top5-acc: 0.9864 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6495 - acc: 0.7696 - top5-acc: 0.9880 - val_loss: 0.7346 - val_acc: 0.7478 - val_top5-acc: 0.9858 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.6524 - acc: 0.7690 - top5-acc: 0.9882 - val_loss: 0.7369 - val_acc: 0.7512 - val_top5-acc: 0.9868 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6407 - acc: 0.7717 - top5-acc: 0.9889 - val_loss: 0.7402 - val_acc: 0.7534 - val_top5-acc: 0.9870 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6441 - acc: 0.7727 - top5-acc: 0.9880 - val_loss: 0.7307 - val_acc: 0.7572 - val_top5-acc: 0.9878 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6404 - acc: 0.7741 - top5-acc: 0.9885 - val_loss: 0.7577 - val_acc: 0.7472 - val_top5-acc: 0.9852 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5912 - acc: 0.7920 - top5-acc: 0.9905 - val_loss: 0.7122 - val_acc: 0.7594 - val_top5-acc: 0.9864 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5652 - acc: 0.8006 - top5-acc: 0.9912 - val_loss: 0.7035 - val_acc: 0.7634 - val_top5-acc: 0.9860 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5591 - acc: 0.8025 - top5-acc: 0.9913 - val_loss: 0.6954 - val_acc: 0.7650 - val_top5-acc: 0.9874 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5581 - acc: 0.8023 - top5-acc: 0.9926 - val_loss: 0.6988 - val_acc: 0.7670 - val_top5-acc: 0.9890 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5522 - acc: 0.8049 - top5-acc: 0.9919 - val_loss: 0.7064 - val_acc: 0.7658 - val_top5-acc: 0.9858 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5485 - acc: 0.8051 - top5-acc: 0.9920 - val_loss: 0.7030 - val_acc: 0.7668 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5463 - acc: 0.8054 - top5-acc: 0.9928 - val_loss: 0.7157 - val_acc: 0.7616 - val_top5-acc: 0.9852 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5508 - acc: 0.8033 - top5-acc: 0.9924 - val_loss: 0.6935 - val_acc: 0.7692 - val_top5-acc: 0.9880 - lr: 0.0012\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.7190 - acc: 0.7626 - top5-acc: 0.9846\n",
      "Test accuracy: 76.26%\n",
      "Test top 5 accuracy: 98.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_24_layer_call_fn, layer_normalization_24_layer_call_and_return_conditional_losses, layer_normalization_25_layer_call_fn, layer_normalization_25_layer_call_and_return_conditional_losses, layer_normalization_26_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 243ms/step - loss: 4.0089 - acc: 0.2508 - top5-acc: 0.7380 - val_loss: 1.6251 - val_acc: 0.4080 - val_top5-acc: 0.8944 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5820 - acc: 0.4287 - top5-acc: 0.8988 - val_loss: 1.4428 - val_acc: 0.4882 - val_top5-acc: 0.9286 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.4414 - acc: 0.4810 - top5-acc: 0.9236 - val_loss: 1.2931 - val_acc: 0.5328 - val_top5-acc: 0.9460 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3548 - acc: 0.5129 - top5-acc: 0.9351 - val_loss: 1.2080 - val_acc: 0.5666 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2898 - acc: 0.5358 - top5-acc: 0.9414 - val_loss: 1.1565 - val_acc: 0.5878 - val_top5-acc: 0.9594 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2408 - acc: 0.5574 - top5-acc: 0.9469 - val_loss: 1.1268 - val_acc: 0.5958 - val_top5-acc: 0.9596 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2083 - acc: 0.5698 - top5-acc: 0.9503 - val_loss: 1.1204 - val_acc: 0.6124 - val_top5-acc: 0.9626 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1609 - acc: 0.5873 - top5-acc: 0.9548 - val_loss: 1.1257 - val_acc: 0.6054 - val_top5-acc: 0.9610 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1437 - acc: 0.5920 - top5-acc: 0.9575 - val_loss: 1.0703 - val_acc: 0.6204 - val_top5-acc: 0.9650 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1198 - acc: 0.6026 - top5-acc: 0.9580 - val_loss: 1.0750 - val_acc: 0.6198 - val_top5-acc: 0.9650 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0900 - acc: 0.6123 - top5-acc: 0.9605 - val_loss: 1.0516 - val_acc: 0.6270 - val_top5-acc: 0.9662 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0667 - acc: 0.6213 - top5-acc: 0.9619 - val_loss: 1.0289 - val_acc: 0.6378 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0549 - acc: 0.6244 - top5-acc: 0.9632 - val_loss: 1.0022 - val_acc: 0.6406 - val_top5-acc: 0.9722 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0386 - acc: 0.6320 - top5-acc: 0.9646 - val_loss: 0.9925 - val_acc: 0.6480 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0217 - acc: 0.6374 - top5-acc: 0.9666 - val_loss: 0.9831 - val_acc: 0.6532 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0016 - acc: 0.6437 - top5-acc: 0.9672 - val_loss: 0.9279 - val_acc: 0.6694 - val_top5-acc: 0.9746 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9895 - acc: 0.6500 - top5-acc: 0.9687 - val_loss: 0.9497 - val_acc: 0.6638 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9790 - acc: 0.6540 - top5-acc: 0.9699 - val_loss: 0.9430 - val_acc: 0.6652 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9536 - acc: 0.6636 - top5-acc: 0.9710 - val_loss: 0.9133 - val_acc: 0.6766 - val_top5-acc: 0.9740 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9577 - acc: 0.6589 - top5-acc: 0.9708 - val_loss: 0.9671 - val_acc: 0.6618 - val_top5-acc: 0.9708 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9373 - acc: 0.6665 - top5-acc: 0.9743 - val_loss: 0.8881 - val_acc: 0.6840 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 4.9045 - acc: 0.5841 - top5-acc: 0.9126 - val_loss: 11.5795 - val_acc: 0.2524 - val_top5-acc: 0.7558 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 2.3958 - acc: 0.4744 - top5-acc: 0.8998 - val_loss: 1.0604 - val_acc: 0.6198 - val_top5-acc: 0.9634 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0833 - acc: 0.6149 - top5-acc: 0.9610 - val_loss: 0.9630 - val_acc: 0.6560 - val_top5-acc: 0.9734 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9928 - acc: 0.6464 - top5-acc: 0.9688 - val_loss: 0.9246 - val_acc: 0.6736 - val_top5-acc: 0.9760 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9453 - acc: 0.6670 - top5-acc: 0.9720 - val_loss: 0.8972 - val_acc: 0.6864 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8844 - acc: 0.6870 - top5-acc: 0.9758 - val_loss: 0.8430 - val_acc: 0.6984 - val_top5-acc: 0.9804 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8648 - acc: 0.6928 - top5-acc: 0.9778 - val_loss: 0.8229 - val_acc: 0.7086 - val_top5-acc: 0.9800 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8440 - acc: 0.7006 - top5-acc: 0.9777 - val_loss: 0.8281 - val_acc: 0.7140 - val_top5-acc: 0.9814 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8403 - acc: 0.7009 - top5-acc: 0.9783 - val_loss: 0.8169 - val_acc: 0.7100 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8257 - acc: 0.7081 - top5-acc: 0.9792 - val_loss: 0.8184 - val_acc: 0.7136 - val_top5-acc: 0.9810 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8146 - acc: 0.7123 - top5-acc: 0.9803 - val_loss: 0.8243 - val_acc: 0.7134 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8054 - acc: 0.7166 - top5-acc: 0.9808 - val_loss: 0.7913 - val_acc: 0.7236 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7983 - acc: 0.7166 - top5-acc: 0.9814 - val_loss: 0.7883 - val_acc: 0.7188 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7847 - acc: 0.7246 - top5-acc: 0.9811 - val_loss: 0.7991 - val_acc: 0.7186 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7827 - acc: 0.7214 - top5-acc: 0.9815 - val_loss: 0.7791 - val_acc: 0.7214 - val_top5-acc: 0.9826 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7738 - acc: 0.7276 - top5-acc: 0.9819 - val_loss: 0.7856 - val_acc: 0.7210 - val_top5-acc: 0.9810 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7673 - acc: 0.7289 - top5-acc: 0.9836 - val_loss: 0.7834 - val_acc: 0.7256 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7620 - acc: 0.7306 - top5-acc: 0.9837 - val_loss: 0.7973 - val_acc: 0.7272 - val_top5-acc: 0.9804 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7576 - acc: 0.7336 - top5-acc: 0.9831 - val_loss: 0.7880 - val_acc: 0.7280 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7498 - acc: 0.7355 - top5-acc: 0.9838 - val_loss: 0.7872 - val_acc: 0.7298 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7072 - acc: 0.7515 - top5-acc: 0.9857 - val_loss: 0.7490 - val_acc: 0.7414 - val_top5-acc: 0.9836 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6973 - acc: 0.7523 - top5-acc: 0.9865 - val_loss: 0.7510 - val_acc: 0.7412 - val_top5-acc: 0.9840 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6907 - acc: 0.7557 - top5-acc: 0.9868 - val_loss: 0.7417 - val_acc: 0.7436 - val_top5-acc: 0.9834 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6828 - acc: 0.7581 - top5-acc: 0.9858 - val_loss: 0.7304 - val_acc: 0.7494 - val_top5-acc: 0.9840 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6791 - acc: 0.7607 - top5-acc: 0.9865 - val_loss: 0.7307 - val_acc: 0.7500 - val_top5-acc: 0.9850 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6792 - acc: 0.7596 - top5-acc: 0.9866 - val_loss: 0.7519 - val_acc: 0.7360 - val_top5-acc: 0.9856 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6736 - acc: 0.7600 - top5-acc: 0.9880 - val_loss: 0.7525 - val_acc: 0.7412 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6686 - acc: 0.7645 - top5-acc: 0.9872 - val_loss: 0.7363 - val_acc: 0.7478 - val_top5-acc: 0.9866 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6646 - acc: 0.7658 - top5-acc: 0.9870 - val_loss: 0.7358 - val_acc: 0.7496 - val_top5-acc: 0.9850 - lr: 0.0012\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.7556 - acc: 0.7377 - top5-acc: 0.9833\n",
      "Test accuracy: 73.77%\n",
      "Test top 5 accuracy: 98.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_36_layer_call_fn, layer_normalization_36_layer_call_and_return_conditional_losses, layer_normalization_37_layer_call_fn, layer_normalization_37_layer_call_and_return_conditional_losses, layer_normalization_38_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 247ms/step - loss: 4.0095 - acc: 0.2532 - top5-acc: 0.7477 - val_loss: 1.6365 - val_acc: 0.4092 - val_top5-acc: 0.8940 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5471 - acc: 0.4407 - top5-acc: 0.9070 - val_loss: 1.3898 - val_acc: 0.5070 - val_top5-acc: 0.9290 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3948 - acc: 0.4958 - top5-acc: 0.9295 - val_loss: 1.2735 - val_acc: 0.5414 - val_top5-acc: 0.9486 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.3270 - acc: 0.5236 - top5-acc: 0.9375 - val_loss: 1.2590 - val_acc: 0.5476 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2734 - acc: 0.5416 - top5-acc: 0.9443 - val_loss: 1.1774 - val_acc: 0.5806 - val_top5-acc: 0.9596 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2323 - acc: 0.5573 - top5-acc: 0.9487 - val_loss: 1.1434 - val_acc: 0.5916 - val_top5-acc: 0.9602 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1962 - acc: 0.5727 - top5-acc: 0.9532 - val_loss: 1.1338 - val_acc: 0.5894 - val_top5-acc: 0.9606 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1525 - acc: 0.5884 - top5-acc: 0.9549 - val_loss: 1.0968 - val_acc: 0.6212 - val_top5-acc: 0.9644 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1229 - acc: 0.5999 - top5-acc: 0.9588 - val_loss: 1.0485 - val_acc: 0.6258 - val_top5-acc: 0.9696 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0966 - acc: 0.6105 - top5-acc: 0.9610 - val_loss: 1.0346 - val_acc: 0.6304 - val_top5-acc: 0.9658 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0769 - acc: 0.6176 - top5-acc: 0.9628 - val_loss: 0.9901 - val_acc: 0.6460 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0417 - acc: 0.6289 - top5-acc: 0.9650 - val_loss: 0.9667 - val_acc: 0.6628 - val_top5-acc: 0.9702 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0295 - acc: 0.6337 - top5-acc: 0.9657 - val_loss: 0.9569 - val_acc: 0.6614 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0035 - acc: 0.6445 - top5-acc: 0.9685 - val_loss: 0.9508 - val_acc: 0.6638 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9936 - acc: 0.6485 - top5-acc: 0.9694 - val_loss: 0.9776 - val_acc: 0.6566 - val_top5-acc: 0.9738 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9742 - acc: 0.6544 - top5-acc: 0.9704 - val_loss: 0.9326 - val_acc: 0.6714 - val_top5-acc: 0.9758 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9559 - acc: 0.6631 - top5-acc: 0.9712 - val_loss: 0.8909 - val_acc: 0.6806 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9376 - acc: 0.6692 - top5-acc: 0.9716 - val_loss: 0.8691 - val_acc: 0.6868 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9253 - acc: 0.6709 - top5-acc: 0.9734 - val_loss: 0.9066 - val_acc: 0.6818 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9124 - acc: 0.6772 - top5-acc: 0.9744 - val_loss: 0.8461 - val_acc: 0.7050 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9087 - acc: 0.6801 - top5-acc: 0.9739 - val_loss: 0.8444 - val_acc: 0.7028 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9036 - acc: 0.6812 - top5-acc: 0.9743 - val_loss: 0.9105 - val_acc: 0.6878 - val_top5-acc: 0.9730 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8869 - acc: 0.6848 - top5-acc: 0.9769 - val_loss: 0.8385 - val_acc: 0.7026 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8734 - acc: 0.6916 - top5-acc: 0.9763 - val_loss: 0.8192 - val_acc: 0.7110 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8699 - acc: 0.6926 - top5-acc: 0.9769 - val_loss: 0.8277 - val_acc: 0.7090 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8673 - acc: 0.6950 - top5-acc: 0.9778 - val_loss: 0.8124 - val_acc: 0.7152 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8433 - acc: 0.7006 - top5-acc: 0.9799 - val_loss: 0.8204 - val_acc: 0.7128 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8400 - acc: 0.7032 - top5-acc: 0.9788 - val_loss: 0.7960 - val_acc: 0.7194 - val_top5-acc: 0.9818 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8179 - acc: 0.7122 - top5-acc: 0.9796 - val_loss: 0.7854 - val_acc: 0.7272 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8330 - acc: 0.7048 - top5-acc: 0.9787 - val_loss: 0.7847 - val_acc: 0.7296 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8076 - acc: 0.7130 - top5-acc: 0.9813 - val_loss: 0.8047 - val_acc: 0.7210 - val_top5-acc: 0.9814 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8208 - acc: 0.7090 - top5-acc: 0.9802 - val_loss: 0.8657 - val_acc: 0.7102 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8075 - acc: 0.7171 - top5-acc: 0.9805 - val_loss: 1.4954 - val_acc: 0.6176 - val_top5-acc: 0.9694 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 8.6819 - acc: 0.3517 - top5-acc: 0.8112 - val_loss: 1.2084 - val_acc: 0.5808 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1702 - acc: 0.5863 - top5-acc: 0.9533 - val_loss: 0.9927 - val_acc: 0.6554 - val_top5-acc: 0.9696 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0231 - acc: 0.6370 - top5-acc: 0.9661 - val_loss: 0.9167 - val_acc: 0.6834 - val_top5-acc: 0.9748 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9553 - acc: 0.6609 - top5-acc: 0.9714 - val_loss: 0.8945 - val_acc: 0.6928 - val_top5-acc: 0.9762 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9125 - acc: 0.6809 - top5-acc: 0.9743 - val_loss: 0.8571 - val_acc: 0.7054 - val_top5-acc: 0.9764 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8792 - acc: 0.6909 - top5-acc: 0.9764 - val_loss: 0.8243 - val_acc: 0.7134 - val_top5-acc: 0.9786 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8448 - acc: 0.7003 - top5-acc: 0.9776 - val_loss: 0.7946 - val_acc: 0.7216 - val_top5-acc: 0.9802 - lr: 0.0025\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.8157 - acc: 0.7240 - top5-acc: 0.9805\n",
      "Test accuracy: 72.4%\n",
      "Test top 5 accuracy: 98.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_48_layer_call_fn, layer_normalization_48_layer_call_and_return_conditional_losses, layer_normalization_49_layer_call_fn, layer_normalization_49_layer_call_and_return_conditional_losses, layer_normalization_50_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 244ms/step - loss: 5.2078 - acc: 0.2340 - top5-acc: 0.7219 - val_loss: 1.7161 - val_acc: 0.3844 - val_top5-acc: 0.8808 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.6137 - acc: 0.4182 - top5-acc: 0.8926 - val_loss: 1.5188 - val_acc: 0.4720 - val_top5-acc: 0.9162 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.4514 - acc: 0.4775 - top5-acc: 0.9206 - val_loss: 1.3805 - val_acc: 0.5034 - val_top5-acc: 0.9286 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3786 - acc: 0.5030 - top5-acc: 0.9311 - val_loss: 1.2568 - val_acc: 0.5422 - val_top5-acc: 0.9478 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3002 - acc: 0.5337 - top5-acc: 0.9397 - val_loss: 1.2737 - val_acc: 0.5528 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2589 - acc: 0.5478 - top5-acc: 0.9448 - val_loss: 1.1845 - val_acc: 0.5756 - val_top5-acc: 0.9572 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2187 - acc: 0.5629 - top5-acc: 0.9490 - val_loss: 1.1359 - val_acc: 0.5992 - val_top5-acc: 0.9586 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1845 - acc: 0.5794 - top5-acc: 0.9531 - val_loss: 1.0982 - val_acc: 0.6136 - val_top5-acc: 0.9638 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1584 - acc: 0.5907 - top5-acc: 0.9539 - val_loss: 1.1118 - val_acc: 0.5996 - val_top5-acc: 0.9634 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1234 - acc: 0.6016 - top5-acc: 0.9577 - val_loss: 1.0904 - val_acc: 0.6156 - val_top5-acc: 0.9672 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1048 - acc: 0.6067 - top5-acc: 0.9596 - val_loss: 1.0485 - val_acc: 0.6260 - val_top5-acc: 0.9642 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0830 - acc: 0.6154 - top5-acc: 0.9615 - val_loss: 1.0056 - val_acc: 0.6470 - val_top5-acc: 0.9680 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0547 - acc: 0.6245 - top5-acc: 0.9633 - val_loss: 1.0331 - val_acc: 0.6428 - val_top5-acc: 0.9674 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0414 - acc: 0.6310 - top5-acc: 0.9653 - val_loss: 1.0043 - val_acc: 0.6502 - val_top5-acc: 0.9674 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0249 - acc: 0.6372 - top5-acc: 0.9654 - val_loss: 0.9427 - val_acc: 0.6690 - val_top5-acc: 0.9718 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9958 - acc: 0.6490 - top5-acc: 0.9675 - val_loss: 0.9795 - val_acc: 0.6624 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9865 - acc: 0.6490 - top5-acc: 0.9680 - val_loss: 0.9486 - val_acc: 0.6744 - val_top5-acc: 0.9708 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9541 - acc: 0.6612 - top5-acc: 0.9701 - val_loss: 0.9224 - val_acc: 0.6746 - val_top5-acc: 0.9724 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9361 - acc: 0.6684 - top5-acc: 0.9718 - val_loss: 0.8795 - val_acc: 0.6946 - val_top5-acc: 0.9758 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9330 - acc: 0.6702 - top5-acc: 0.9724 - val_loss: 0.9207 - val_acc: 0.6764 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9160 - acc: 0.6749 - top5-acc: 0.9734 - val_loss: 0.8681 - val_acc: 0.6962 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8999 - acc: 0.6809 - top5-acc: 0.9746 - val_loss: 0.8693 - val_acc: 0.6962 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9046 - acc: 0.6810 - top5-acc: 0.9748 - val_loss: 0.9149 - val_acc: 0.6854 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8745 - acc: 0.6901 - top5-acc: 0.9770 - val_loss: 0.8531 - val_acc: 0.7072 - val_top5-acc: 0.9800 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8734 - acc: 0.6931 - top5-acc: 0.9766 - val_loss: 0.8408 - val_acc: 0.7086 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8634 - acc: 0.6946 - top5-acc: 0.9779 - val_loss: 0.8278 - val_acc: 0.7078 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 233ms/step - loss: 0.8475 - acc: 0.6988 - top5-acc: 0.9785 - val_loss: 0.8045 - val_acc: 0.7180 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 21s 233ms/step - loss: 0.8347 - acc: 0.7038 - top5-acc: 0.9796 - val_loss: 0.8586 - val_acc: 0.7086 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 21s 234ms/step - loss: 0.8385 - acc: 0.7032 - top5-acc: 0.9780 - val_loss: 0.8004 - val_acc: 0.7226 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8283 - acc: 0.7080 - top5-acc: 0.9795 - val_loss: 0.8398 - val_acc: 0.7064 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8247 - acc: 0.7080 - top5-acc: 0.9798 - val_loss: 0.8294 - val_acc: 0.7136 - val_top5-acc: 0.9782 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.8066 - acc: 0.7144 - top5-acc: 0.9805 - val_loss: 0.8410 - val_acc: 0.7164 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8052 - acc: 0.7127 - top5-acc: 0.9808 - val_loss: 0.8008 - val_acc: 0.7234 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7826 - acc: 0.7242 - top5-acc: 0.9821 - val_loss: 0.8109 - val_acc: 0.7224 - val_top5-acc: 0.9796 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7082 - acc: 0.7500 - top5-acc: 0.9850 - val_loss: 0.7397 - val_acc: 0.7432 - val_top5-acc: 0.9866 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 21s 238ms/step - loss: 0.6842 - acc: 0.7570 - top5-acc: 0.9861 - val_loss: 0.7684 - val_acc: 0.7372 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6697 - acc: 0.7626 - top5-acc: 0.9878 - val_loss: 0.7532 - val_acc: 0.7430 - val_top5-acc: 0.9850 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6691 - acc: 0.7621 - top5-acc: 0.9864 - val_loss: 0.7549 - val_acc: 0.7510 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.6561 - acc: 0.7682 - top5-acc: 0.9875 - val_loss: 0.7370 - val_acc: 0.7510 - val_top5-acc: 0.9838 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6564 - acc: 0.7702 - top5-acc: 0.9883 - val_loss: 0.7517 - val_acc: 0.7482 - val_top5-acc: 0.9840 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6533 - acc: 0.7696 - top5-acc: 0.9878 - val_loss: 0.7369 - val_acc: 0.7558 - val_top5-acc: 0.9860 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6404 - acc: 0.7722 - top5-acc: 0.9888 - val_loss: 0.7607 - val_acc: 0.7470 - val_top5-acc: 0.9844 - lr: 0.0025\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6477 - acc: 0.7720 - top5-acc: 0.9880 - val_loss: 0.7424 - val_acc: 0.7550 - val_top5-acc: 0.9842 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6407 - acc: 0.7738 - top5-acc: 0.9885 - val_loss: 0.7373 - val_acc: 0.7488 - val_top5-acc: 0.9868 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5806 - acc: 0.7936 - top5-acc: 0.9907 - val_loss: 0.7109 - val_acc: 0.7656 - val_top5-acc: 0.9876 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5611 - acc: 0.8009 - top5-acc: 0.9919 - val_loss: 0.6990 - val_acc: 0.7660 - val_top5-acc: 0.9848 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5604 - acc: 0.8000 - top5-acc: 0.9912 - val_loss: 0.7026 - val_acc: 0.7640 - val_top5-acc: 0.9858 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 22s 246ms/step - loss: 0.5502 - acc: 0.8051 - top5-acc: 0.9921 - val_loss: 0.7193 - val_acc: 0.7628 - val_top5-acc: 0.9866 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 21s 235ms/step - loss: 0.5504 - acc: 0.8050 - top5-acc: 0.9920 - val_loss: 0.7192 - val_acc: 0.7660 - val_top5-acc: 0.9848 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5397 - acc: 0.8092 - top5-acc: 0.9925 - val_loss: 0.7020 - val_acc: 0.7666 - val_top5-acc: 0.9858 - lr: 0.0012\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.7199 - acc: 0.7641 - top5-acc: 0.9856\n",
      "Test accuracy: 76.41%\n",
      "Test top 5 accuracy: 98.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_60_layer_call_fn, layer_normalization_60_layer_call_and_return_conditional_losses, layer_normalization_61_layer_call_fn, layer_normalization_61_layer_call_and_return_conditional_losses, layer_normalization_62_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 30s 244ms/step - loss: 4.1914 - acc: 0.2397 - top5-acc: 0.7304 - val_loss: 1.7027 - val_acc: 0.3788 - val_top5-acc: 0.8846 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.5973 - acc: 0.4201 - top5-acc: 0.8970 - val_loss: 1.4271 - val_acc: 0.4872 - val_top5-acc: 0.9302 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.4455 - acc: 0.4787 - top5-acc: 0.9227 - val_loss: 1.3250 - val_acc: 0.5236 - val_top5-acc: 0.9400 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3589 - acc: 0.5113 - top5-acc: 0.9325 - val_loss: 1.2252 - val_acc: 0.5570 - val_top5-acc: 0.9506 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3036 - acc: 0.5323 - top5-acc: 0.9410 - val_loss: 1.1988 - val_acc: 0.5664 - val_top5-acc: 0.9524 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.2543 - acc: 0.5505 - top5-acc: 0.9456 - val_loss: 1.2139 - val_acc: 0.5666 - val_top5-acc: 0.9530 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.2152 - acc: 0.5663 - top5-acc: 0.9493 - val_loss: 1.1076 - val_acc: 0.6118 - val_top5-acc: 0.9598 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1798 - acc: 0.5792 - top5-acc: 0.9525 - val_loss: 1.1117 - val_acc: 0.6076 - val_top5-acc: 0.9592 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.1445 - acc: 0.5931 - top5-acc: 0.9554 - val_loss: 1.0533 - val_acc: 0.6310 - val_top5-acc: 0.9664 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1253 - acc: 0.5999 - top5-acc: 0.9576 - val_loss: 1.0544 - val_acc: 0.6242 - val_top5-acc: 0.9664 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1081 - acc: 0.6036 - top5-acc: 0.9592 - val_loss: 1.0734 - val_acc: 0.6170 - val_top5-acc: 0.9664 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0705 - acc: 0.6191 - top5-acc: 0.9625 - val_loss: 1.0838 - val_acc: 0.6168 - val_top5-acc: 0.9646 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0627 - acc: 0.6237 - top5-acc: 0.9613 - val_loss: 0.9894 - val_acc: 0.6578 - val_top5-acc: 0.9698 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0380 - acc: 0.6324 - top5-acc: 0.9657 - val_loss: 1.0383 - val_acc: 0.6474 - val_top5-acc: 0.9634 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0225 - acc: 0.6386 - top5-acc: 0.9657 - val_loss: 0.9795 - val_acc: 0.6532 - val_top5-acc: 0.9724 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 1.0072 - acc: 0.6453 - top5-acc: 0.9664 - val_loss: 0.9695 - val_acc: 0.6574 - val_top5-acc: 0.9720 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9768 - acc: 0.6542 - top5-acc: 0.9694 - val_loss: 0.9130 - val_acc: 0.6808 - val_top5-acc: 0.9734 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9744 - acc: 0.6561 - top5-acc: 0.9708 - val_loss: 0.9091 - val_acc: 0.6758 - val_top5-acc: 0.9752 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9533 - acc: 0.6642 - top5-acc: 0.9710 - val_loss: 0.8970 - val_acc: 0.6840 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9399 - acc: 0.6675 - top5-acc: 0.9723 - val_loss: 0.8772 - val_acc: 0.6934 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.9267 - acc: 0.6742 - top5-acc: 0.9731 - val_loss: 0.8989 - val_acc: 0.6852 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9065 - acc: 0.6807 - top5-acc: 0.9739 - val_loss: 0.9162 - val_acc: 0.6854 - val_top5-acc: 0.9746 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.9091 - acc: 0.6771 - top5-acc: 0.9746 - val_loss: 0.8994 - val_acc: 0.6870 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8879 - acc: 0.6862 - top5-acc: 0.9759 - val_loss: 0.8916 - val_acc: 0.6976 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8800 - acc: 0.6898 - top5-acc: 0.9761 - val_loss: 0.8779 - val_acc: 0.7022 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7900 - acc: 0.7218 - top5-acc: 0.9814 - val_loss: 0.7920 - val_acc: 0.7256 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7668 - acc: 0.7301 - top5-acc: 0.9823 - val_loss: 0.7845 - val_acc: 0.7320 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7625 - acc: 0.7323 - top5-acc: 0.9830 - val_loss: 0.7712 - val_acc: 0.7344 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7556 - acc: 0.7318 - top5-acc: 0.9834 - val_loss: 0.7664 - val_acc: 0.7298 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7491 - acc: 0.7350 - top5-acc: 0.9830 - val_loss: 0.7851 - val_acc: 0.7288 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7420 - acc: 0.7375 - top5-acc: 0.9838 - val_loss: 0.7646 - val_acc: 0.7358 - val_top5-acc: 0.9820 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7355 - acc: 0.7394 - top5-acc: 0.9837 - val_loss: 0.7943 - val_acc: 0.7326 - val_top5-acc: 0.9808 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7287 - acc: 0.7410 - top5-acc: 0.9846 - val_loss: 0.7867 - val_acc: 0.7338 - val_top5-acc: 0.9822 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7293 - acc: 0.7416 - top5-acc: 0.9843 - val_loss: 0.7688 - val_acc: 0.7360 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7236 - acc: 0.7434 - top5-acc: 0.9844 - val_loss: 0.7562 - val_acc: 0.7394 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7241 - acc: 0.7436 - top5-acc: 0.9845 - val_loss: 0.7983 - val_acc: 0.7290 - val_top5-acc: 0.9834 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7112 - acc: 0.7477 - top5-acc: 0.9855 - val_loss: 0.7647 - val_acc: 0.7430 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 232ms/step - loss: 0.7066 - acc: 0.7506 - top5-acc: 0.9852 - val_loss: 0.7879 - val_acc: 0.7346 - val_top5-acc: 0.9812 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7005 - acc: 0.7516 - top5-acc: 0.9859 - val_loss: 0.7733 - val_acc: 0.7386 - val_top5-acc: 0.9836 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.7066 - acc: 0.7511 - top5-acc: 0.9859 - val_loss: 0.7890 - val_acc: 0.7312 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6347 - acc: 0.7773 - top5-acc: 0.9879 - val_loss: 0.7297 - val_acc: 0.7534 - val_top5-acc: 0.9856 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6206 - acc: 0.7802 - top5-acc: 0.9895 - val_loss: 0.7133 - val_acc: 0.7576 - val_top5-acc: 0.9848 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6201 - acc: 0.7800 - top5-acc: 0.9888 - val_loss: 0.7313 - val_acc: 0.7506 - val_top5-acc: 0.9842 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 231ms/step - loss: 0.6061 - acc: 0.7848 - top5-acc: 0.9901 - val_loss: 0.7379 - val_acc: 0.7514 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6127 - acc: 0.7834 - top5-acc: 0.9894 - val_loss: 0.7231 - val_acc: 0.7620 - val_top5-acc: 0.9884 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6047 - acc: 0.7863 - top5-acc: 0.9901 - val_loss: 0.7118 - val_acc: 0.7602 - val_top5-acc: 0.9862 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5978 - acc: 0.7895 - top5-acc: 0.9904 - val_loss: 0.7343 - val_acc: 0.7508 - val_top5-acc: 0.9868 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5956 - acc: 0.7894 - top5-acc: 0.9902 - val_loss: 0.7171 - val_acc: 0.7602 - val_top5-acc: 0.9878 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5976 - acc: 0.7886 - top5-acc: 0.9905 - val_loss: 0.7276 - val_acc: 0.7544 - val_top5-acc: 0.9868 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.5929 - acc: 0.7903 - top5-acc: 0.9903 - val_loss: 0.7164 - val_acc: 0.7566 - val_top5-acc: 0.9868 - lr: 0.0012\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.7330 - acc: 0.7550 - top5-acc: 0.9832\n",
      "Test accuracy: 75.5%\n",
      "Test top 5 accuracy: 98.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_72_layer_call_fn, layer_normalization_72_layer_call_and_return_conditional_losses, layer_normalization_73_layer_call_fn, layer_normalization_73_layer_call_and_return_conditional_losses, layer_normalization_74_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_7\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 28s 243ms/step - loss: 3.7925 - acc: 0.2594 - top5-acc: 0.7425 - val_loss: 1.6329 - val_acc: 0.4182 - val_top5-acc: 0.8984 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.5711 - acc: 0.4313 - top5-acc: 0.9002 - val_loss: 1.3968 - val_acc: 0.5010 - val_top5-acc: 0.9338 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.4371 - acc: 0.4836 - top5-acc: 0.9224 - val_loss: 1.3123 - val_acc: 0.5290 - val_top5-acc: 0.9410 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.3413 - acc: 0.5178 - top5-acc: 0.9354 - val_loss: 1.2004 - val_acc: 0.5690 - val_top5-acc: 0.9544 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2815 - acc: 0.5405 - top5-acc: 0.9420 - val_loss: 1.2364 - val_acc: 0.5706 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2242 - acc: 0.5633 - top5-acc: 0.9466 - val_loss: 1.1494 - val_acc: 0.5910 - val_top5-acc: 0.9580 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1969 - acc: 0.5754 - top5-acc: 0.9512 - val_loss: 1.1265 - val_acc: 0.5948 - val_top5-acc: 0.9600 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1579 - acc: 0.5879 - top5-acc: 0.9543 - val_loss: 1.0501 - val_acc: 0.6304 - val_top5-acc: 0.9646 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1195 - acc: 0.6018 - top5-acc: 0.9575 - val_loss: 1.0466 - val_acc: 0.6232 - val_top5-acc: 0.9658 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1072 - acc: 0.6079 - top5-acc: 0.9589 - val_loss: 1.0196 - val_acc: 0.6394 - val_top5-acc: 0.9676 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0627 - acc: 0.6246 - top5-acc: 0.9625 - val_loss: 1.0142 - val_acc: 0.6438 - val_top5-acc: 0.9690 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0539 - acc: 0.6270 - top5-acc: 0.9638 - val_loss: 1.0580 - val_acc: 0.6264 - val_top5-acc: 0.9666 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0398 - acc: 0.6307 - top5-acc: 0.9650 - val_loss: 0.9672 - val_acc: 0.6546 - val_top5-acc: 0.9722 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0137 - acc: 0.6414 - top5-acc: 0.9664 - val_loss: 0.9290 - val_acc: 0.6748 - val_top5-acc: 0.9768 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9847 - acc: 0.6538 - top5-acc: 0.9683 - val_loss: 0.9088 - val_acc: 0.6840 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9713 - acc: 0.6564 - top5-acc: 0.9690 - val_loss: 0.9530 - val_acc: 0.6652 - val_top5-acc: 0.9778 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9543 - acc: 0.6610 - top5-acc: 0.9712 - val_loss: 0.9669 - val_acc: 0.6658 - val_top5-acc: 0.9696 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9442 - acc: 0.6668 - top5-acc: 0.9716 - val_loss: 0.8891 - val_acc: 0.6870 - val_top5-acc: 0.9732 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9194 - acc: 0.6743 - top5-acc: 0.9725 - val_loss: 0.8644 - val_acc: 0.6938 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9286 - acc: 0.6735 - top5-acc: 0.9722 - val_loss: 0.9182 - val_acc: 0.6720 - val_top5-acc: 0.9768 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9186 - acc: 0.6791 - top5-acc: 0.9730 - val_loss: 0.8726 - val_acc: 0.6928 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9053 - acc: 0.6798 - top5-acc: 0.9747 - val_loss: 0.8657 - val_acc: 0.6956 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8792 - acc: 0.6929 - top5-acc: 0.9749 - val_loss: 0.8445 - val_acc: 0.7054 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8618 - acc: 0.6962 - top5-acc: 0.9774 - val_loss: 0.8344 - val_acc: 0.7142 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8625 - acc: 0.6977 - top5-acc: 0.9767 - val_loss: 0.8366 - val_acc: 0.7142 - val_top5-acc: 0.9816 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8634 - acc: 0.6956 - top5-acc: 0.9771 - val_loss: 0.8732 - val_acc: 0.7056 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8482 - acc: 0.7015 - top5-acc: 0.9782 - val_loss: 0.8397 - val_acc: 0.7140 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 13.3543 - acc: 0.4338 - top5-acc: 0.8246 - val_loss: 2.4552 - val_acc: 0.3094 - val_top5-acc: 0.8386 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.5164 - acc: 0.4636 - top5-acc: 0.9137 - val_loss: 1.2715 - val_acc: 0.5540 - val_top5-acc: 0.9448 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2582 - acc: 0.5525 - top5-acc: 0.9471 - val_loss: 1.1543 - val_acc: 0.5924 - val_top5-acc: 0.9580 - lr: 0.0025\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.1841 - acc: 0.5781 - top5-acc: 0.9530 - val_loss: 1.0809 - val_acc: 0.6184 - val_top5-acc: 0.9626 - lr: 0.0025\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1197 - acc: 0.6055 - top5-acc: 0.9594 - val_loss: 1.0273 - val_acc: 0.6340 - val_top5-acc: 0.9682 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0719 - acc: 0.6206 - top5-acc: 0.9625 - val_loss: 0.9990 - val_acc: 0.6514 - val_top5-acc: 0.9708 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 1.0195 - acc: 0.6393 - top5-acc: 0.9670 - val_loss: 0.9512 - val_acc: 0.6616 - val_top5-acc: 0.9728 - lr: 0.0025\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.8831 - acc: 0.7030 - top5-acc: 0.9746\n",
      "Test accuracy: 70.3%\n",
      "Test top 5 accuracy: 97.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_84_layer_call_fn, layer_normalization_84_layer_call_and_return_conditional_losses, layer_normalization_85_layer_call_fn, layer_normalization_85_layer_call_and_return_conditional_losses, layer_normalization_86_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 29s 242ms/step - loss: 4.1663 - acc: 0.2488 - top5-acc: 0.7403 - val_loss: 1.6421 - val_acc: 0.4004 - val_top5-acc: 0.8854 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.5764 - acc: 0.4297 - top5-acc: 0.8993 - val_loss: 1.4050 - val_acc: 0.4900 - val_top5-acc: 0.9312 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.4397 - acc: 0.4808 - top5-acc: 0.9203 - val_loss: 1.3716 - val_acc: 0.5006 - val_top5-acc: 0.9378 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.3604 - acc: 0.5108 - top5-acc: 0.9339 - val_loss: 1.2219 - val_acc: 0.5610 - val_top5-acc: 0.9522 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2909 - acc: 0.5376 - top5-acc: 0.9423 - val_loss: 1.1592 - val_acc: 0.5852 - val_top5-acc: 0.9572 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2464 - acc: 0.5546 - top5-acc: 0.9463 - val_loss: 1.1705 - val_acc: 0.5834 - val_top5-acc: 0.9560 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.2019 - acc: 0.5705 - top5-acc: 0.9511 - val_loss: 1.1204 - val_acc: 0.6038 - val_top5-acc: 0.9564 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1567 - acc: 0.5866 - top5-acc: 0.9566 - val_loss: 1.0944 - val_acc: 0.6106 - val_top5-acc: 0.9596 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1347 - acc: 0.5966 - top5-acc: 0.9556 - val_loss: 1.0697 - val_acc: 0.6234 - val_top5-acc: 0.9628 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0977 - acc: 0.6116 - top5-acc: 0.9590 - val_loss: 1.0425 - val_acc: 0.6272 - val_top5-acc: 0.9650 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0841 - acc: 0.6164 - top5-acc: 0.9616 - val_loss: 1.0008 - val_acc: 0.6390 - val_top5-acc: 0.9712 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0635 - acc: 0.6232 - top5-acc: 0.9626 - val_loss: 1.0047 - val_acc: 0.6430 - val_top5-acc: 0.9666 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0504 - acc: 0.6285 - top5-acc: 0.9636 - val_loss: 0.9838 - val_acc: 0.6528 - val_top5-acc: 0.9668 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0287 - acc: 0.6365 - top5-acc: 0.9645 - val_loss: 0.9599 - val_acc: 0.6608 - val_top5-acc: 0.9728 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0034 - acc: 0.6459 - top5-acc: 0.9673 - val_loss: 0.9451 - val_acc: 0.6666 - val_top5-acc: 0.9704 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9973 - acc: 0.6478 - top5-acc: 0.9678 - val_loss: 0.9446 - val_acc: 0.6746 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9640 - acc: 0.6579 - top5-acc: 0.9704 - val_loss: 0.8959 - val_acc: 0.6830 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9475 - acc: 0.6664 - top5-acc: 0.9701 - val_loss: 0.9544 - val_acc: 0.6706 - val_top5-acc: 0.9738 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9466 - acc: 0.6644 - top5-acc: 0.9711 - val_loss: 0.8711 - val_acc: 0.6912 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9364 - acc: 0.6685 - top5-acc: 0.9721 - val_loss: 0.8685 - val_acc: 0.6966 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9092 - acc: 0.6806 - top5-acc: 0.9743 - val_loss: 0.8682 - val_acc: 0.6936 - val_top5-acc: 0.9760 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9359 - acc: 0.6693 - top5-acc: 0.9726 - val_loss: 0.8753 - val_acc: 0.6970 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8942 - acc: 0.6852 - top5-acc: 0.9745 - val_loss: 0.8975 - val_acc: 0.6928 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8824 - acc: 0.6902 - top5-acc: 0.9757 - val_loss: 0.8598 - val_acc: 0.7070 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8869 - acc: 0.6860 - top5-acc: 0.9766 - val_loss: 0.8688 - val_acc: 0.7014 - val_top5-acc: 0.9774 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8698 - acc: 0.6930 - top5-acc: 0.9761 - val_loss: 0.7990 - val_acc: 0.7276 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8694 - acc: 0.6921 - top5-acc: 0.9769 - val_loss: 0.8453 - val_acc: 0.7126 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8438 - acc: 0.7032 - top5-acc: 0.9777 - val_loss: 0.8383 - val_acc: 0.7128 - val_top5-acc: 0.9764 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8369 - acc: 0.7051 - top5-acc: 0.9799 - val_loss: 0.8183 - val_acc: 0.7172 - val_top5-acc: 0.9822 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8243 - acc: 0.7089 - top5-acc: 0.9807 - val_loss: 0.8028 - val_acc: 0.7226 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8213 - acc: 0.7125 - top5-acc: 0.9792 - val_loss: 0.8674 - val_acc: 0.7058 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.7465 - acc: 0.7343 - top5-acc: 0.9836 - val_loss: 0.7468 - val_acc: 0.7508 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.7009 - acc: 0.7538 - top5-acc: 0.9859 - val_loss: 0.7405 - val_acc: 0.7488 - val_top5-acc: 0.9854 - lr: 0.0025\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6923 - acc: 0.7557 - top5-acc: 0.9864 - val_loss: 0.7356 - val_acc: 0.7456 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6987 - acc: 0.7532 - top5-acc: 0.9859 - val_loss: 0.7325 - val_acc: 0.7474 - val_top5-acc: 0.9830 - lr: 0.0025\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6855 - acc: 0.7600 - top5-acc: 0.9861 - val_loss: 0.7339 - val_acc: 0.7530 - val_top5-acc: 0.9838 - lr: 0.0025\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6885 - acc: 0.7576 - top5-acc: 0.9867 - val_loss: 0.7563 - val_acc: 0.7440 - val_top5-acc: 0.9832 - lr: 0.0025\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6810 - acc: 0.7588 - top5-acc: 0.9870 - val_loss: 0.7736 - val_acc: 0.7468 - val_top5-acc: 0.9818 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6813 - acc: 0.7591 - top5-acc: 0.9874 - val_loss: 0.7879 - val_acc: 0.7404 - val_top5-acc: 0.9822 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6774 - acc: 0.7612 - top5-acc: 0.9873 - val_loss: 0.7752 - val_acc: 0.7472 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.6107 - acc: 0.7843 - top5-acc: 0.9889 - val_loss: 0.7307 - val_acc: 0.7604 - val_top5-acc: 0.9836 - lr: 0.0012\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5998 - acc: 0.7872 - top5-acc: 0.9904 - val_loss: 0.7166 - val_acc: 0.7630 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5917 - acc: 0.7916 - top5-acc: 0.9905 - val_loss: 0.7188 - val_acc: 0.7584 - val_top5-acc: 0.9844 - lr: 0.0012\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5858 - acc: 0.7932 - top5-acc: 0.9906 - val_loss: 0.7083 - val_acc: 0.7674 - val_top5-acc: 0.9848 - lr: 0.0012\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.5866 - acc: 0.7915 - top5-acc: 0.9908 - val_loss: 0.7224 - val_acc: 0.7626 - val_top5-acc: 0.9866 - lr: 0.0012\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5819 - acc: 0.7938 - top5-acc: 0.9912 - val_loss: 0.7281 - val_acc: 0.7606 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5744 - acc: 0.7951 - top5-acc: 0.9917 - val_loss: 0.7149 - val_acc: 0.7624 - val_top5-acc: 0.9860 - lr: 0.0012\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5818 - acc: 0.7931 - top5-acc: 0.9906 - val_loss: 0.7011 - val_acc: 0.7712 - val_top5-acc: 0.9854 - lr: 0.0012\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5733 - acc: 0.7985 - top5-acc: 0.9912 - val_loss: 0.7020 - val_acc: 0.7682 - val_top5-acc: 0.9870 - lr: 0.0012\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.5715 - acc: 0.7965 - top5-acc: 0.9918 - val_loss: 0.7070 - val_acc: 0.7662 - val_top5-acc: 0.9844 - lr: 0.0012\n",
      "313/313 [==============================] - 14s 43ms/step - loss: 0.7283 - acc: 0.7577 - top5-acc: 0.9844\n",
      "Test accuracy: 75.77%\n",
      "Test top 5 accuracy: 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_96_layer_call_fn, layer_normalization_96_layer_call_and_return_conditional_losses, layer_normalization_97_layer_call_fn, layer_normalization_97_layer_call_and_return_conditional_losses, layer_normalization_98_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 28s 242ms/step - loss: 3.6856 - acc: 0.2594 - top5-acc: 0.7523 - val_loss: 1.5885 - val_acc: 0.4188 - val_top5-acc: 0.9000 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.5637 - acc: 0.4337 - top5-acc: 0.9031 - val_loss: 1.4406 - val_acc: 0.4852 - val_top5-acc: 0.9250 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.4193 - acc: 0.4893 - top5-acc: 0.9246 - val_loss: 1.2983 - val_acc: 0.5282 - val_top5-acc: 0.9482 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.3333 - acc: 0.5230 - top5-acc: 0.9378 - val_loss: 1.2343 - val_acc: 0.5656 - val_top5-acc: 0.9514 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2848 - acc: 0.5405 - top5-acc: 0.9420 - val_loss: 1.1739 - val_acc: 0.5802 - val_top5-acc: 0.9574 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.2452 - acc: 0.5530 - top5-acc: 0.9465 - val_loss: 1.1812 - val_acc: 0.5810 - val_top5-acc: 0.9572 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1920 - acc: 0.5753 - top5-acc: 0.9510 - val_loss: 1.1047 - val_acc: 0.6106 - val_top5-acc: 0.9586 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1624 - acc: 0.5871 - top5-acc: 0.9554 - val_loss: 1.1010 - val_acc: 0.6012 - val_top5-acc: 0.9606 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.1341 - acc: 0.5975 - top5-acc: 0.9574 - val_loss: 1.0609 - val_acc: 0.6248 - val_top5-acc: 0.9666 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1145 - acc: 0.6071 - top5-acc: 0.9586 - val_loss: 1.0417 - val_acc: 0.6306 - val_top5-acc: 0.9658 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0890 - acc: 0.6145 - top5-acc: 0.9611 - val_loss: 1.0446 - val_acc: 0.6274 - val_top5-acc: 0.9690 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0606 - acc: 0.6241 - top5-acc: 0.9640 - val_loss: 0.9912 - val_acc: 0.6508 - val_top5-acc: 0.9688 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0380 - acc: 0.6315 - top5-acc: 0.9646 - val_loss: 0.9908 - val_acc: 0.6448 - val_top5-acc: 0.9722 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0075 - acc: 0.6428 - top5-acc: 0.9675 - val_loss: 0.9775 - val_acc: 0.6514 - val_top5-acc: 0.9714 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 1.0037 - acc: 0.6474 - top5-acc: 0.9674 - val_loss: 0.9567 - val_acc: 0.6638 - val_top5-acc: 0.9716 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9827 - acc: 0.6510 - top5-acc: 0.9695 - val_loss: 0.9501 - val_acc: 0.6714 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9583 - acc: 0.6618 - top5-acc: 0.9701 - val_loss: 0.9398 - val_acc: 0.6744 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9406 - acc: 0.6677 - top5-acc: 0.9734 - val_loss: 0.8902 - val_acc: 0.6872 - val_top5-acc: 0.9766 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.9378 - acc: 0.6719 - top5-acc: 0.9722 - val_loss: 0.8860 - val_acc: 0.6850 - val_top5-acc: 0.9760 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9189 - acc: 0.6770 - top5-acc: 0.9725 - val_loss: 0.8792 - val_acc: 0.6908 - val_top5-acc: 0.9772 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9223 - acc: 0.6768 - top5-acc: 0.9740 - val_loss: 0.9373 - val_acc: 0.6778 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9041 - acc: 0.6819 - top5-acc: 0.9750 - val_loss: 0.8652 - val_acc: 0.7034 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8922 - acc: 0.6860 - top5-acc: 0.9756 - val_loss: 0.8936 - val_acc: 0.6948 - val_top5-acc: 0.9734 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8807 - acc: 0.6920 - top5-acc: 0.9760 - val_loss: 0.8358 - val_acc: 0.7078 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8487 - acc: 0.7016 - top5-acc: 0.9780 - val_loss: 0.8312 - val_acc: 0.7180 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8870 - acc: 0.6873 - top5-acc: 0.9762 - val_loss: 0.9223 - val_acc: 0.6890 - val_top5-acc: 0.9736 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8464 - acc: 0.6999 - top5-acc: 0.9787 - val_loss: 0.8221 - val_acc: 0.7192 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8300 - acc: 0.7103 - top5-acc: 0.9788 - val_loss: 0.8486 - val_acc: 0.7110 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8181 - acc: 0.7146 - top5-acc: 0.9797 - val_loss: 0.8909 - val_acc: 0.6992 - val_top5-acc: 0.9760 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8128 - acc: 0.7128 - top5-acc: 0.9802 - val_loss: 0.8678 - val_acc: 0.7132 - val_top5-acc: 0.9790 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8200 - acc: 0.7118 - top5-acc: 0.9797 - val_loss: 0.8513 - val_acc: 0.7134 - val_top5-acc: 0.9784 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.7909 - acc: 0.7214 - top5-acc: 0.9810 - val_loss: 0.8122 - val_acc: 0.7226 - val_top5-acc: 0.9794 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.7986 - acc: 0.7197 - top5-acc: 0.9814 - val_loss: 0.8702 - val_acc: 0.7120 - val_top5-acc: 0.9808 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 9.5068 - acc: 0.5924 - top5-acc: 0.8997 - val_loss: 15.8273 - val_acc: 0.1688 - val_top5-acc: 0.6740 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 2.6430 - acc: 0.4128 - top5-acc: 0.8769 - val_loss: 1.1997 - val_acc: 0.5702 - val_top5-acc: 0.9552 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.1911 - acc: 0.5768 - top5-acc: 0.9522 - val_loss: 1.0607 - val_acc: 0.6280 - val_top5-acc: 0.9664 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 1.0569 - acc: 0.6252 - top5-acc: 0.9653 - val_loss: 0.9365 - val_acc: 0.6678 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9693 - acc: 0.6562 - top5-acc: 0.9699 - val_loss: 0.8879 - val_acc: 0.6872 - val_top5-acc: 0.9770 - lr: 0.0025\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.9273 - acc: 0.6732 - top5-acc: 0.9728 - val_loss: 0.8685 - val_acc: 0.6962 - val_top5-acc: 0.9772 - lr: 0.0025\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 20s 231ms/step - loss: 0.8985 - acc: 0.6826 - top5-acc: 0.9746 - val_loss: 0.8556 - val_acc: 0.7012 - val_top5-acc: 0.9796 - lr: 0.0025\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 20s 229ms/step - loss: 0.8675 - acc: 0.6945 - top5-acc: 0.9772 - val_loss: 0.8156 - val_acc: 0.7160 - val_top5-acc: 0.9808 - lr: 0.0025\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 20s 230ms/step - loss: 0.8377 - acc: 0.7052 - top5-acc: 0.9784 - val_loss: 0.8346 - val_acc: 0.7136 - val_top5-acc: 0.9802 - lr: 0.0025\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.8447 - acc: 0.7108 - top5-acc: 0.9798\n",
      "Test accuracy: 71.08%\n",
      "Test top 5 accuracy: 97.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_108_layer_call_fn, layer_normalization_108_layer_call_and_return_conditional_losses, layer_normalization_109_layer_call_fn, layer_normalization_109_layer_call_and_return_conditional_losses, layer_normalization_110_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_10\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/4A_SC/mlpmixer_B-32_10\\assets\n"
     ]
    }
   ],
   "source": [
    "mlpmixer_generator(num_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Results_Article/4A_SC/mlpmixer_B-32_'\n",
    "total_models = list()\n",
    "for k in range(num_models):  \n",
    "    current_model = tf.keras.models.load_model(path + str(k+1))\n",
    "    total_models.append(current_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell once to avoid randomness\n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F486F558B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F486F558B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F4E8DB7E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F4E8DB7E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "total_activations = list()\n",
    "for k in range(num_models):\n",
    "    tested_model = total_models[k] \n",
    "    ave_mixer_activations = Prom_Mixer_Activations_Blocks(tested_model,batch_prepro)\n",
    "    total_activations.append(ave_mixer_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check with RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sanity check with a CKA RFF: 0.2 has an accuracy of 9.63%\n",
      "The Sanity check with a CKA RFF: 0.4 has an accuracy of 10.56%\n",
      "The Sanity check with a CKA RFF: 0.8 has an accuracy of 16.48%\n"
     ]
    }
   ],
   "source": [
    "pwd='Results_Article/4A_SC'\n",
    "with open(pwd + '/Activations_10Net.pkl','wb') as file:\n",
    "        pickle.dump(total_activations,file)\n",
    "\n",
    "for sigma in rbf_index:\n",
    "    positive_events, total_events = sanity_check(total_activations, num_models, num_blocks,'rbf',sigma)\n",
    "    SC_accuracy = (positive_events/total_events) \n",
    "    print(f\"The Sanity check with a CKA RBF: {sigma} has an accuracy of {round(SC_accuracy * 100, 2)}%\")\n",
    "    #Multiplying by 100 to ensure proper saving of the file\n",
    "    with open(pwd + '/SCaccuracy_RBF'+ str(round(sigma*100))  +'.pkl','wb') as file:\n",
    "        pickle.dump(SC_accuracy,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check with Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sanity check with a CKA linear has an accuracy of 12.96%\n"
     ]
    }
   ],
   "source": [
    "positive_events, total_events = sanity_check(total_activations, num_models, num_blocks,'linear',sigma=None)\n",
    "SC_accuracy = (positive_events/total_events) \n",
    "print(f\"The Sanity check with a CKA linear has an accuracy of {round(SC_accuracy * 100, 2)}%\")\n",
    "with open(pwd + '/SCaccuracy_linear.pkl','wb') as file:\n",
    "        pickle.dump(SC_accuracy,file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_image_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
