{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zSzgroG_Suq"
      },
      "source": [
        "# Study of Image classification with modern MLP Mixer model and CKA\n",
        "\n",
        "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
        "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U087DdDw_Suy"
      },
      "source": [
        "# Setup for the MLP-Mixer Architecture\n",
        "\n",
        "################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AnTyoluw_Suz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Program_Files\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt \n",
        "from scipy.stats import norm\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "import datetime\n",
        "import pickle\n",
        "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
        "from CKA_Google import *\n",
        "import seaborn as sns \n",
        "import random\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 5 Appendix:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DAOrIHu_Su2"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1iMiVS7o_Su3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image size: 224 X 224 = 50176\n",
            "Patch size: 32 X 32 = 1024 \n",
            "Patches per image: 49\n",
            "Elements per patch (3 channels): 3072\n"
          ]
        }
      ],
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 512 \n",
        "num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "learning_rate = 0.005\n",
        "\n",
        "## Selected Architecture: B/32\n",
        "\n",
        "image_size = 224  # We'll resize input images to this size. Square\n",
        "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
        "embedding_dim = 384\n",
        "num_blocks = 8 # Fixed number of layers Dimension\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "date = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
        "path = 'Results_Article/5Appendix/mlpmixer_' + str(date) + '_independent'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUuu2OAS_Su0"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "#Dataset for training \n",
        "\n",
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
        "#plt.imshow(x_train[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08mpnEZH_Su5"
      },
      "source": [
        "## Build a classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ra-bXojQ_Su6"
      },
      "outputs": [],
      "source": [
        "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data. \n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches. \n",
        "    patches = Patches(patch_size, num_patches)(augmented)\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
        "    x = layers.Dense(units=embedding_dim)(patches)\n",
        "    if positional_encoding:\n",
        "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "        position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=embedding_dim\n",
        "        )(positions)\n",
        "        x = x + position_embedding\n",
        "    # Process x using the module blocks. ## (sequential_82)\n",
        "    x = blocks(x)\n",
        "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
        "    representation = layers.GlobalAveragePooling1D()(x)\n",
        "    # Apply dropout.\n",
        "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
        "    # Compute logits outputs.\n",
        "    logits = layers.Dense(num_classes)(representation) \n",
        "    # Create the Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpQFU8H__Su8"
      },
      "source": [
        "## Define an experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9E1zD2BY_Su8"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
        "    )\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        #Negative Log Likelihood = Categorical Cross Entropy\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5\n",
        "    )\n",
        "    # Create an early stopping regularization callback. \n",
        "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history, accuracy, top_5_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxeE0cmM_Su9"
      },
      "source": [
        "## Use data augmentation\n",
        "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zh6hFPWX_Su-"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G77cUgiu_Su-"
      },
      "source": [
        "## Implement patch extraction as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RYKNktXe_Su_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, num_patches):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def call(self, images):\n",
        "        #Extract the shape dimension in the position 0 = columns\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            #Without overlapping, stride horizontally and vertically\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
        "            rates=[1, 1, 1, 1],\n",
        "            #Patches contained in the images are considered, no zero padding\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        #shape[-1], number of colummns, as well as shape[0]\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Patches, self).get_config().copy()\n",
        "        config.update ({\n",
        "            'patch_size' : self.patch_size ,\n",
        "            'num_patches' : self.num_patches\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7yehcSS_Su_"
      },
      "source": [
        "## The MLP-Mixer model\n",
        "\n",
        "The MLP-Mixer is an architecture based exclusively on\n",
        "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
        "\n",
        "1. One applied independently to image patches, which mixes the per-location features.\n",
        "2. The other applied across patches (along channels), which mixes spatial information.\n",
        "\n",
        "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
        "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
        "instead of batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvwg4e2n_SvA"
      },
      "source": [
        "### Implement the MLP-Mixer module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dT6wVEki_SvA"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MLPMixerLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.mlp1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=num_patches),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.mlp2 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize(inputs)\n",
        "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "        x_channels = tf.linalg.matrix_transpose(x)\n",
        "        # Apply mlp1 on each channel independently.\n",
        "        mlp1_outputs = self.mlp1(x_channels)\n",
        "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
        "        # Add skip connection.\n",
        "        x = mlp1_outputs + inputs\n",
        "        # Apply layer normalization.\n",
        "        x_patches = self.normalize(x)\n",
        "        # Apply mlp2 on each patch independtenly.\n",
        "        mlp2_outputs = self.mlp2(x_patches)\n",
        "        # Add skip connection.\n",
        "        x = x + mlp2_outputs\n",
        "        return x\n",
        "\n",
        "    def get_config(self): \n",
        "        config = super(MLPMixerLayer, self).get_config().copy()\n",
        "        config.update ({\n",
        "            'num_patches' : num_patches,\n",
        "            'embedding_dim' : embedding_dim,\n",
        "            'dropout_rate' : dropout_rate,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUiufq92_SvA"
      },
      "source": [
        "## Build, train, and evaluate the MLP-Mixer model\n",
        "\n",
        "Note that training the model with the current settings on a V100 GPUs\n",
        "takes around 8 seconds per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Report: Learning Curve\n",
        "def curves(history):\n",
        "    ymax1 = min(history[\"loss\"])\n",
        "    xmax1 = history[\"loss\"].index(ymax1)\n",
        "    ymax2 = min(history[\"val_loss\"])\n",
        "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
        "    plt.title(\"Cross Entropy Loss\")\n",
        "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
        "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
        "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
        "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
        "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
        "    plt.xlim([0,num_epochs])\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # Graph accuracy\n",
        "    ymax3 = max(history[\"acc\"])\n",
        "    xmax3 = history[\"acc\"].index(ymax3)\n",
        "    ymax4 = max(history[\"val_acc\"])\n",
        "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
        "    ymax5 = max(history[\"top5-acc\"])\n",
        "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
        "    ymax6 = max(history[\"val_top5-acc\"])\n",
        "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.title('Classification accuracy')\n",
        "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
        "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
        "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
        "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
        "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
        "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.title('Classification top5-acc')\n",
        "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
        "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
        "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
        "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
        "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
        "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
        "    plt.xlim([0,num_epochs])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.suptitle(\"Learning Curves\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtain activations + Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing Layers + Patches + One dense layer\n",
        "def Preprocessing(num_example):\n",
        "    augmented = data_augmentation(x_train[num_example])\n",
        "    b = Patches(patch_size, num_patches)(augmented)\n",
        "    a = layers.Dense(units=embedding_dim)(b)\n",
        "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
        "    return inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
        "def Batch_Preprocessing(batch_size):\n",
        "    #Vector with the number of Sample of the Xtrain\n",
        "    a  = list(range(0,x_train.shape[0]))\n",
        "    b = random.sample(a,batch_size)\n",
        "    batch_regularization = list()\n",
        "    for i in range(0,batch_size):\n",
        "        inter_result = Preprocessing(b[i])\n",
        "        batch_regularization.append(inter_result)\n",
        "    return batch_regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_out(result,layer_number,example):\n",
        "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
        "    ax1.imshow(x_train[example])\n",
        "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
        "    ax2.imshow(result[layer_number])\n",
        "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
        "    #The input is fixed to the beginning of the mlp blocks\n",
        "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
        "    #This reshape is necessary for the input of the model\n",
        "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
        "    #Inference\n",
        "    intermediate_prediction =intermediate_model.predict(example)\n",
        "    #This reshape is standardize the output\n",
        "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
        "    return layactivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Computes the outputs of each MLP-mixer Layer\n",
        "def Mixer_Activations(model, example):\n",
        "    total_activations = list()\n",
        "    for i in range(num_blocks):\n",
        "        model_input = model.layers[4].layers[0]\n",
        "        model_output = model.layers[4].layers[i]\n",
        "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
        "        total_activations.append(int_total_activations)\n",
        "    return  total_activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Average of layer's activation\n",
        "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
        "    sum = list()\n",
        "    for i in range(0,num_blocks):\n",
        "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
        "        sum.append(sum_raw)\n",
        "    for i in range(0,batch_size):\n",
        "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
        "        for i in range(0,num_blocks):\n",
        "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
        "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
        "    return prom_mixer_activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CKA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
        "def Heatmap(result,type,sigma):\n",
        "    dim = len(result)\n",
        "    k = (dim - 1)\n",
        "    heatmap_CKA = np.zeros((dim,dim))\n",
        "    for i in range(0,dim):\n",
        "        tr = (dim - 1)\n",
        "        for j in range(0,dim):\n",
        "            if type == 'kernel':\n",
        "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
        "            elif type == 'linear':\n",
        "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
        "            else:\n",
        "                print('There is no such category, try again')\n",
        "                break\n",
        "\n",
        "            tr -= 1\n",
        "        k -= 1\n",
        "    #print('CKA' + type + 'calculated')\n",
        "    return heatmap_CKA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Average of heatmaps (obsolet)\n",
        "def Prom_Mixer_Heatmaps(batch_result,type):\n",
        "    mat_heatmaps = list()\n",
        "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
        "    for i in range(0,batch_size):\n",
        "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
        "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
        "        mat_heatmaps.append(heatmap_raw)\n",
        "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
        "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
        "    return prom_mixer_heatmap,mat_heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_Heatmap(heatmap,type,bl):\n",
        "    #Number of thats that you want to appear in the plot\n",
        "    tri = 4\n",
        "    if type == 'kernel' or type == 'linear':\n",
        "        dim = len(heatmap)\n",
        "        axis_labels = list()\n",
        "        for i in range(0,dim):\n",
        "            axis_labels_inter = str('%i'%(i+1))\n",
        "            axis_labels.append(axis_labels_inter)\n",
        "        _, ax = plt.subplots(figsize=(6,6))\n",
        "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
        "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
        "        ax.invert_xaxis()\n",
        "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
        "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
        "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
        "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
        "\n",
        "        ax.set_title(\"CKA-\"+ type)   \n",
        "        ax.set_xlabel(\"Layer\")\n",
        "        ax.set_ylabel(\"Layer\")\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.locator_params(axis='x',nbins=tri)\n",
        "        plt.locator_params(axis='y',nbins=tri)\n",
        "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
        "        \n",
        "    else:\n",
        "        print('There is no such category, try again')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 5 (Appendix) : Explore the independent approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Mixer_Layer_Outputs2(model,example,shape):\n",
        "    intermediate_model=tf.keras.models.Model(inputs=model.input,outputs=model.output)\n",
        "    #This reshape is necessary for the input of the model\n",
        "    example = tf.reshape(example,[1,shape[0],shape[1]])\n",
        "    #Inference\n",
        "    intermediate_prediction =intermediate_model.predict(example)\n",
        "    #This reshape is standardize the output\n",
        "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
        "    return layactivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Computes the outputs of each MLP-mixer Layer\n",
        "def Mixer_Activations_2(model,example):\n",
        "    total_activations = list()\n",
        "    for i in range(num_blocks):\n",
        "        #Shape of the input for the mlp1\n",
        "        shape=(embedding_dim,num_patches)\n",
        "        modelf = model.layers[4].layers[i].mlp1\n",
        "        int_total_activations = Mixer_Layer_Outputs2(modelf,example,shape)\n",
        "        #Shape of the input for the mlp2\n",
        "        total_activations.append(int_total_activations)\n",
        "        shape=(num_patches,embedding_dim)\n",
        "        modelf = model.layers[4].layers[i].mlp2\n",
        "        int_total_activations = Mixer_Layer_Outputs2(modelf,example,shape)\n",
        "        total_activations.append(int_total_activations)\n",
        "    return total_activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Prom_Mixer_Activations_Blocks_2(model,batch_regularization):\n",
        "    sum = list()\n",
        "    for i in range(0,2*num_blocks):\n",
        "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
        "        sum.append(sum_raw)\n",
        "    for i in range(0,batch_size):\n",
        "        mixer_raw = Mixer_Activations_2(model,batch_regularization[i])\n",
        "        for i in range(0,2*num_blocks):\n",
        "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
        "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
        "    return prom_mixer_activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlpmixer_blocks = keras.Sequential(\n",
        "[MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
        ")\n",
        "mlpmixer_independent= build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run once to avoid randomness\n",
        "batch_prepro = Batch_Preprocessing(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D9193F2D08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D91937EEA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#After training, this cell is going to be same as the after-training heatmap\n",
        "sigma = 1\n",
        "type = 'kernel' # 'kernel' or 'linear'\n",
        "###############################################################################################\n",
        "A5_bt_ave_mixer_activations = Prom_Mixer_Activations_Blocks_2(mlpmixer_independent,batch_prepro)\n",
        "A5_bt_global_heatmap = Heatmap(A5_bt_ave_mixer_activations,type,sigma)\n",
        "visualize_Heatmap(A5_bt_global_heatmap,type,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Independent blocks\n",
        "A5_bt_ave_mixer_activations_block = Prom_Mixer_Activations_Blocks(mlpmixer_independent,batch_prepro)\n",
        "A5_bt_global_heatmap_block = Heatmap(A5_bt_ave_mixer_activations_block,type,sigma)\n",
        "visualize_Heatmap(A5_bt_global_heatmap_block,type,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = run_experiment(mlpmixer_independent)\n",
        "mlpmixer_independent.save(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#After training heatmap\n",
        "A5_at_ave_mixer_activations = Prom_Mixer_Activations_Blocks_2(mlpmixer_independent,batch_prepro)\n",
        "A5_at_global_heatmap = Heatmap(A5_at_ave_mixer_activations,type,sigma)\n",
        "visualize_Heatmap(A5_at_global_heatmap,type,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A5_at_ave_mixer_activations_block = Prom_Mixer_Activations_Blocks(mlpmixer_independent,batch_prepro)\n",
        "A5_at_global_heatmap_block = Heatmap(A5_at_ave_mixer_activations_block,type,sigma)\n",
        "visualize_Heatmap(A5_at_global_heatmap_block,type,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(path + '/heatmap_bt.pkl','wb') as file:\n",
        "    pickle.dump(A5_bt_global_heatmap,file)\n",
        "with open(path + '/heatmap_bt_block.pkl','wb') as file:\n",
        "    pickle.dump(A5_bt_global_heatmap_block,file)  \n",
        "with open(path + '/heatmap_at.pkl','wb') as file:\n",
        "    pickle.dump(A5_at_global_heatmap,file)\n",
        "with open(path + '/heatmap_at_block.pkl','wb') as file:\n",
        "    pickle.dump(A5_at_global_heatmap_block,file)  \n",
        "    \n",
        "with open(path + '/heatmap_at.pkl','rb') as file:\n",
        "    tested_heatmap = pickle.load(file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mlp_image_classification",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
