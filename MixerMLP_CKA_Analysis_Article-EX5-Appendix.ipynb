{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSzgroG_Suq"
   },
   "source": [
    "# Study of Image classification with modern MLP Mixer model and CKA\n",
    "\n",
    "**Author:** [Arturo Flores](https://www.linkedin.com/in/afloresalv/)<br>\n",
    "**Based on (MLP-MIXER):**  https://keras.io/examples/vision/mlp_image_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U087DdDw_Suy"
   },
   "source": [
    "# Setup for the MLP-Mixer Architecture\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AnTyoluw_Suz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\Anaconda3\\envs\\AI\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import pickle\n",
    "# Files imported from the sleected GitHub https://cka-similarity.github.io/\n",
    "from CKA_Google import *\n",
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5 Appendix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DAOrIHu_Su2"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1iMiVS7o_Su3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 224 X 224 = 50176\n",
      "Patch size: 32 X 32 = 1024 \n",
      "Patches per image: 49\n",
      "Elements per patch (3 channels): 3072\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "batch_size = 512 \n",
    "num_epochs = 50\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.005\n",
    "\n",
    "## Selected Architecture: B/32\n",
    "\n",
    "image_size = 224  # We'll resize input images to this size. Square\n",
    "patch_size = 32  # Size of the patches to be extracted from the input images. Square\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array, or sequence length (S)\n",
    "embedding_dim = 384\n",
    "num_blocks = 12 # Fixed number of layers Dimension\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "path = 'Results_Article/5Appendix/mlpmixer_independent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuu2OAS_Su0"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Dataset for training \n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "#plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08mpnEZH_Su5"
   },
   "source": [
    "## Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ra-bXojQ_Su6"
   },
   "outputs": [],
   "source": [
    "def build_classifier(blocks, embedding_dim, positional_encoding=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data. \n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches. \n",
    "    patches = Patches(patch_size, num_patches)(augmented)\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "    if positional_encoding:\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=embedding_dim\n",
    "        )(positions)\n",
    "        x = x + position_embedding\n",
    "    # Process x using the module blocks. ## (sequential_82)\n",
    "    x = blocks(x)\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor. \n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes)(representation) \n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpQFU8H__Su8"
   },
   "source": [
    "## Define an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9E1zD2BY_Su8"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Create Adam optimizer with weight decay. Regularization that penalizes the increase of weight - with a facto alpha - to correct the overfitting\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "    )\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        #Negative Log Likelihood = Categorical Cross Entropy\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
    "        ],\n",
    "    )\n",
    "    # Create a learning rate scheduler callback.\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\n",
    "    )\n",
    "    # Create an early stopping regularization callback. \n",
    "    # It ends at a point that corresponds to a minimum of the L2-regularized objective\n",
    "    #early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    #    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    #)\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[reduce_lr],\n",
    "    )\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Return history to plot learning curves.\n",
    "    return history, accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxeE0cmM_Su9"
   },
   "source": [
    "## Use data augmentation\n",
    "Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zh6hFPWX_Su-"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G77cUgiu_Su-"
   },
   "source": [
    "## Implement patch extraction as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RYKNktXe_Su_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        #Extract the shape dimension in the position 0 = columns\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Without overlapping, stride horizontally and vertically\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            #Rate: Dilation factor [1 1* 1* 1] controls the spacing between the kernel points.\n",
    "            rates=[1, 1, 1, 1],\n",
    "            #Patches contained in the images are considered, no zero padding\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        #shape[-1], number of colummns, as well as shape[0]\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Patches, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'patch_size' : self.patch_size ,\n",
    "            'num_patches' : self.num_patches\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7yehcSS_Su_"
   },
   "source": [
    "## The MLP-Mixer model\n",
    "\n",
    "The MLP-Mixer is an architecture based exclusively on\n",
    "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
    "\n",
    "1. One applied independently to image patches, which mixes the per-location features.\n",
    "2. The other applied across patches (along channels), which mixes spatial information.\n",
    "\n",
    "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
    "instead of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvwg4e2n_SvA"
   },
   "source": [
    "### Implement the MLP-Mixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dT6wVEki_SvA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPMixerLayer(layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.mlp1 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=num_patches),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mlp2 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=num_patches),\n",
    "                tfa.layers.GELU(),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize(inputs)\n",
    "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "        x_channels = tf.linalg.matrix_transpose(x)\n",
    "        # Apply mlp1 on each channel independently.\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
    "        # Add skip connection.\n",
    "        x = mlp1_outputs + inputs\n",
    "        # Apply layer normalization.\n",
    "        x_patches = self.normalize(x)\n",
    "        # Apply mlp2 on each patch independtenly.\n",
    "        mlp2_outputs = self.mlp2(x_patches)\n",
    "        # Add skip connection.\n",
    "        x = x + mlp2_outputs\n",
    "        return x\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super(MLPMixerLayer, self).get_config().copy()\n",
    "        config.update ({\n",
    "            'num_patches' : num_patches,\n",
    "            'embedding_dim' : embedding_dim,\n",
    "            'dropout_rate' : dropout_rate,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUiufq92_SvA"
   },
   "source": [
    "## Build, train, and evaluate the MLP-Mixer model\n",
    "\n",
    "Note that training the model with the current settings on a V100 GPUs\n",
    "takes around 8 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report: Learning Curve\n",
    "def curves(history):\n",
    "    ymax1 = min(history[\"loss\"])\n",
    "    xmax1 = history[\"loss\"].index(ymax1)\n",
    "    ymax2 = min(history[\"val_loss\"])\n",
    "    xmax2 = history[\"val_loss\"].index(ymax2)\n",
    "    plt.title(\"Cross Entropy Loss\")\n",
    "    plt.plot(history[\"loss\"], color = 'blue', label = 'Training')\n",
    "    plt.plot(history[\"val_loss\"], color = 'orange', label = 'Testing')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.annotate('Max:' + str(round(ymax1,2)) , xy = (xmax1, ymax1), xytext = (xmax1*0.93, 1.07*ymax1), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax2,2)) , xy = (xmax2, ymax2), xytext = (xmax2*0.93, 1.07*ymax2), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Graph accuracy\n",
    "    ymax3 = max(history[\"acc\"])\n",
    "    xmax3 = history[\"acc\"].index(ymax3)\n",
    "    ymax4 = max(history[\"val_acc\"])\n",
    "    xmax4 = history[\"val_acc\"].index(ymax4)\n",
    "    ymax5 = max(history[\"top5-acc\"])\n",
    "    xmax5 = history[\"top5-acc\"].index(ymax5)\n",
    "    ymax6 = max(history[\"val_top5-acc\"])\n",
    "    xmax6 = history[\"val_top5-acc\"].index(ymax6)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Classification accuracy')\n",
    "    plt.plot(history['acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax3,2)) , xy = (xmax3, ymax3), xytext = (xmax3*0.93, 1.2*ymax3), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax4,2)) , xy = (xmax4, ymax4), xytext = (xmax4*0.93, 0.7*ymax4), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Classification top5-acc')\n",
    "    plt.plot(history['top5-acc'], color = 'blue', label = 'Training')\n",
    "    plt.plot(history['val_top5-acc'], color = 'orange', label = 'Testing')\n",
    "    plt.annotate('Max:' + str(round(ymax5,2)) , xy = (xmax5, ymax5), xytext = (xmax5*0.93, 1.2*ymax5), \n",
    "                    arrowprops=dict(facecolor='blue', headwidth= 6, headlength =9))\n",
    "    plt.annotate('Max:' + str(round(ymax6,2)) , xy = (xmax6, ymax6), xytext = (xmax6*0.87, 1.2*ymax6), \n",
    "                    arrowprops=dict(facecolor='goldenrod', headwidth= 6, headlength =9))\n",
    "    plt.xlim([0,num_epochs])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.suptitle(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain activations + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Layers + Patches + One dense layer\n",
    "def Preprocessing(num_example):\n",
    "    augmented = data_augmentation(x_train[num_example])\n",
    "    b = Patches(patch_size, num_patches)(augmented)\n",
    "    a = layers.Dense(units=embedding_dim)(b)\n",
    "    inp = tf.reshape(a,[1,embedding_dim,num_patches])\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random vector with indexes of a random batch selection and also regularizes the selected batch\n",
    "def Batch_Preprocessing(batch_size):\n",
    "    #Vector with the number of Sample of the Xtrain\n",
    "    a  = list(range(0,x_train.shape[0]))\n",
    "    b = random.sample(a,batch_size)\n",
    "    batch_regularization = list()\n",
    "    for i in range(0,batch_size):\n",
    "        inter_result = Preprocessing(b[i])\n",
    "        batch_regularization.append(inter_result)\n",
    "    return batch_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_out(result,layer_number,example):\n",
    "    fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "    ax1.imshow(x_train[example])\n",
    "    ax1.set_title('Original_Figure, Class: #' + str(y_train[example][0]))\n",
    "    ax2.imshow(result[layer_number])\n",
    "    ax2.set_title('Activations of MLP block of the Mixer #: '+ '\"' + str(layer_number) + '\"')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs(model_input, model_output,example):\n",
    "    #The input is fixed to the beginning of the mlp blocks\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model_input.input,outputs=model_output.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,num_patches,embedding_dim])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[0]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of layer's activation\n",
    "def Prom_Mixer_Activations_Blocks(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations_ind(model, example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        model_input = model.layers[4].layers[i]\n",
    "        model_output = model.layers[4].layers[i]\n",
    "        int_total_activations = Mixer_Layer_Outputs(model_input, model_output, example)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return  total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prom_Mixer_Activations_Blocks_ind(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations_ind(model,batch_regularization[i])\n",
    "        for i in range(0,num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates a heatmap according to the selection of a CKA_Kernel (preferred) or CKA_Linear\n",
    "def Heatmap(result,type,sigma):\n",
    "    dim = len(result)\n",
    "    k = (dim - 1)\n",
    "    heatmap_CKA = np.zeros((dim,dim))\n",
    "    for i in range(0,dim):\n",
    "        tr = (dim - 1)\n",
    "        for j in range(0,dim):\n",
    "            if type == 'kernel':\n",
    "                heatmap_CKA[k][tr] = cka(gram_rbf(result[i],sigma),gram_rbf(result[j],sigma))\n",
    "            elif type == 'linear':\n",
    "                heatmap_CKA[k][tr] = cka(gram_linear(result[i]),gram_linear(result[j])) \n",
    "            else:\n",
    "                print('There is no such category, try again')\n",
    "                break\n",
    "\n",
    "            tr -= 1\n",
    "        k -= 1\n",
    "    #print('CKA' + type + 'calculated')\n",
    "    return heatmap_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of heatmaps (obsolet)\n",
    "def Prom_Mixer_Heatmaps(batch_result,type):\n",
    "    mat_heatmaps = list()\n",
    "    prom_mixer_heatmap_raw = np.zeros((num_blocks,num_blocks))\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_activations_raw = Mixer_Activations(batch_result[i])\n",
    "        heatmap_raw = Heatmap(mixer_activations_raw, type)\n",
    "        mat_heatmaps.append(heatmap_raw)\n",
    "        prom_mixer_heatmap_raw = np.add(heatmap_raw,prom_mixer_heatmap_raw)\n",
    "    prom_mixer_heatmap =  prom_mixer_heatmap_raw/batch_size  \n",
    "    return prom_mixer_heatmap,mat_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 4\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i'%(i+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(3,3))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_Heatmap_mlp(heatmap,type,bl):\n",
    "    #Number of thats that you want to appear in the plot\n",
    "    tri = 5\n",
    "    if type == 'kernel' or type == 'linear':\n",
    "        dim = len(heatmap)\n",
    "        axis_labels = list()\n",
    "        for i in range(0,dim):\n",
    "            axis_labels_inter = str('%i/%i'%((i%2)+1,(i//2)+1))\n",
    "            axis_labels.append(axis_labels_inter)\n",
    "        _, ax = plt.subplots(figsize=(3,3))\n",
    "        ax = sns.heatmap(heatmap, xticklabels=axis_labels[::-1], yticklabels=axis_labels[::-1], ax = ax, annot=bl)\n",
    "        #sns.heatmap(heatmap, xticklabels=2, yticklabels=2, ax = ax, annot=bl, cbar=True)   \n",
    "        ax.invert_xaxis()\n",
    "        ax.axhline(y = 0, color='k',linewidth = 4)\n",
    "        ax.axhline(y = heatmap.shape[1], color = 'k', linewidth = 4)\n",
    "        ax.axvline(x = 0, color ='k',linewidth = 4)\n",
    "        ax.axvline(x = heatmap.shape[0], color = 'k', linewidth = 4)\n",
    "\n",
    "        ax.set_title(\"CKA-\"+ type)   \n",
    "        ax.set_xlabel(\"MLP/Layer\")\n",
    "        ax.set_ylabel(\"MLP/Layer\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.locator_params(axis='x',nbins=tri)\n",
    "        plt.locator_params(axis='y',nbins=tri)\n",
    "        plt.savefig('CKA_'+ type +'.png', dpi=300)\n",
    "        \n",
    "    else:\n",
    "        print('There is no such category, try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5 (Appendix) : Explore the independent approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mlp1 + mlp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixer_Layer_Outputs2(model,example,shape):\n",
    "    intermediate_model=tf.keras.models.Model(inputs=model.input,outputs=model.output)\n",
    "    #This reshape is necessary for the input of the model\n",
    "    example = tf.reshape(example,[1,shape[0],shape[1]])\n",
    "    #Inference\n",
    "    intermediate_prediction =intermediate_model.predict(example)\n",
    "    #This reshape is standardize the output\n",
    "    layactivation = intermediate_prediction.reshape((embedding_dim,num_patches))\n",
    "    return layactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the outputs of each MLP-mixer Layer\n",
    "def Mixer_Activations_2(model,example):\n",
    "    total_activations = list()\n",
    "    for i in range(num_blocks):\n",
    "        #Shape of the input for the mlp1\n",
    "        shape=(embedding_dim,num_patches)\n",
    "        modelf = model.layers[4].layers[i].mlp1\n",
    "        int_total_activations = Mixer_Layer_Outputs2(modelf,example,shape)\n",
    "        #Shape of the input for the mlp2\n",
    "        total_activations.append(int_total_activations)\n",
    "        shape=(num_patches,embedding_dim)\n",
    "        modelf = model.layers[4].layers[i].mlp2\n",
    "        int_total_activations = Mixer_Layer_Outputs2(modelf,example,shape)\n",
    "        total_activations.append(int_total_activations)\n",
    "    return total_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prom_Mixer_Activations_Blocks_2(model,batch_regularization):\n",
    "    sum = list()\n",
    "    for i in range(0,2*num_blocks):\n",
    "        sum_raw = np.zeros((embedding_dim,num_patches))\n",
    "        sum.append(sum_raw)\n",
    "    for i in range(0,batch_size):\n",
    "        mixer_raw = Mixer_Activations_2(model,batch_regularization[i])\n",
    "        for i in range(0,2*num_blocks):\n",
    "            sum[i] = np.add(mixer_raw[i],sum[i])\n",
    "    prom_mixer_activations = [ (number / batch_size)  for number in sum]\n",
    "    return prom_mixer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpmixer_blocks = keras.Sequential(\n",
    "[MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)] # creates the number of block without a \n",
    ")\n",
    "mlpmixer_independent= build_classifier(mlpmixer_blocks,embedding_dim) # Returns the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run once to avoid randomness\n",
    "batch_prepro = Batch_Preprocessing(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A02BE40EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A02BDC1F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#After training, this cell is going to be same as the after-training heatmap\n",
    "sigma = 1\n",
    "type = 'kernel' # 'kernel' or 'linear'\n",
    "###############################################################################################\n",
    "A5_bt_ave_mixer_activations = Prom_Mixer_Activations_Blocks_2(mlpmixer_independent,batch_prepro)\n",
    "A5_bt_global_heatmap = Heatmap(A5_bt_ave_mixer_activations,type,sigma)\n",
    "visualize_Heatmap_mlp(A5_bt_global_heatmap,type,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cumulative blocks\n",
    "A5_bt_ave_mixer_activations_block = Prom_Mixer_Activations_Blocks(mlpmixer_independent,batch_prepro)\n",
    "A5_bt_global_heatmap_block = Heatmap(A5_bt_ave_mixer_activations_block,type,sigma)\n",
    "visualize_Heatmap(A5_bt_global_heatmap_block,type,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Independent blocks\n",
    "A5_bt_ave_mixer_activations_block_ind = Prom_Mixer_Activations_Blocks_ind(mlpmixer_independent,batch_prepro)\n",
    "A5_bt_global_heatmap_block_ind = Heatmap(A5_bt_ave_mixer_activations_block_ind,type,sigma)\n",
    "visualize_Heatmap(A5_bt_global_heatmap_block_ind,type,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88/88 [==============================] - 20s 173ms/step - loss: 4.6027 - acc: 0.2440 - top5-acc: 0.7303 - val_loss: 1.7254 - val_acc: 0.3920 - val_top5-acc: 0.8830 - lr: 0.0050\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.6010 - acc: 0.4211 - top5-acc: 0.8966 - val_loss: 1.4514 - val_acc: 0.4764 - val_top5-acc: 0.9230 - lr: 0.0050\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.4543 - acc: 0.4741 - top5-acc: 0.9218 - val_loss: 1.3501 - val_acc: 0.5100 - val_top5-acc: 0.9432 - lr: 0.0050\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 15s 165ms/step - loss: 1.3838 - acc: 0.5034 - top5-acc: 0.9295 - val_loss: 1.2631 - val_acc: 0.5532 - val_top5-acc: 0.9456 - lr: 0.0050\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.3099 - acc: 0.5307 - top5-acc: 0.9388 - val_loss: 1.1978 - val_acc: 0.5708 - val_top5-acc: 0.9528 - lr: 0.0050\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2761 - acc: 0.5437 - top5-acc: 0.9434 - val_loss: 1.2330 - val_acc: 0.5658 - val_top5-acc: 0.9534 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.2296 - acc: 0.5624 - top5-acc: 0.9487 - val_loss: 1.1581 - val_acc: 0.5954 - val_top5-acc: 0.9532 - lr: 0.0050\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.1984 - acc: 0.5730 - top5-acc: 0.9513 - val_loss: 1.1721 - val_acc: 0.5892 - val_top5-acc: 0.9598 - lr: 0.0050\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.1651 - acc: 0.5853 - top5-acc: 0.9547 - val_loss: 1.1758 - val_acc: 0.5960 - val_top5-acc: 0.9594 - lr: 0.0050\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.1332 - acc: 0.5987 - top5-acc: 0.9562 - val_loss: 1.0664 - val_acc: 0.6298 - val_top5-acc: 0.9624 - lr: 0.0050\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.1197 - acc: 0.6027 - top5-acc: 0.9581 - val_loss: 1.0558 - val_acc: 0.6352 - val_top5-acc: 0.9642 - lr: 0.0050\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.0922 - acc: 0.6100 - top5-acc: 0.9608 - val_loss: 1.0115 - val_acc: 0.6464 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.0779 - acc: 0.6171 - top5-acc: 0.9628 - val_loss: 1.0065 - val_acc: 0.6444 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.0614 - acc: 0.6241 - top5-acc: 0.9630 - val_loss: 1.0163 - val_acc: 0.6506 - val_top5-acc: 0.9678 - lr: 0.0050\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 1.0440 - acc: 0.6305 - top5-acc: 0.9641 - val_loss: 0.9964 - val_acc: 0.6528 - val_top5-acc: 0.9670 - lr: 0.0050\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.0364 - acc: 0.6329 - top5-acc: 0.9648 - val_loss: 1.0150 - val_acc: 0.6482 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.0242 - acc: 0.6379 - top5-acc: 0.9651 - val_loss: 1.0678 - val_acc: 0.6354 - val_top5-acc: 0.9684 - lr: 0.0050\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 1.0125 - acc: 0.6403 - top5-acc: 0.9674 - val_loss: 0.9527 - val_acc: 0.6592 - val_top5-acc: 0.9726 - lr: 0.0050\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 0.9860 - acc: 0.6521 - top5-acc: 0.9682 - val_loss: 0.9388 - val_acc: 0.6740 - val_top5-acc: 0.9762 - lr: 0.0050\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 15s 165ms/step - loss: 0.9732 - acc: 0.6550 - top5-acc: 0.9693 - val_loss: 0.9354 - val_acc: 0.6722 - val_top5-acc: 0.9744 - lr: 0.0050\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 15s 165ms/step - loss: 0.9631 - acc: 0.6591 - top5-acc: 0.9703 - val_loss: 0.9442 - val_acc: 0.6728 - val_top5-acc: 0.9756 - lr: 0.0050\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 0.9449 - acc: 0.6659 - top5-acc: 0.9723 - val_loss: 0.9244 - val_acc: 0.6800 - val_top5-acc: 0.9746 - lr: 0.0050\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.9268 - acc: 0.6729 - top5-acc: 0.9728 - val_loss: 0.9635 - val_acc: 0.6758 - val_top5-acc: 0.9750 - lr: 0.0050\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.9217 - acc: 0.6739 - top5-acc: 0.9722 - val_loss: 0.9063 - val_acc: 0.6884 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.9200 - acc: 0.6737 - top5-acc: 0.9738 - val_loss: 0.9184 - val_acc: 0.6884 - val_top5-acc: 0.9770 - lr: 0.0050\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.8927 - acc: 0.6832 - top5-acc: 0.9753 - val_loss: 0.8757 - val_acc: 0.6972 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.8965 - acc: 0.6839 - top5-acc: 0.9747 - val_loss: 0.8806 - val_acc: 0.6952 - val_top5-acc: 0.9748 - lr: 0.0050\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 15s 165ms/step - loss: 0.9041 - acc: 0.6807 - top5-acc: 0.9740 - val_loss: 0.8559 - val_acc: 0.7070 - val_top5-acc: 0.9776 - lr: 0.0050\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 15s 165ms/step - loss: 0.8793 - acc: 0.6890 - top5-acc: 0.9767 - val_loss: 0.8886 - val_acc: 0.6912 - val_top5-acc: 0.9780 - lr: 0.0050\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 0.8753 - acc: 0.6908 - top5-acc: 0.9759 - val_loss: 0.8257 - val_acc: 0.7136 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.8623 - acc: 0.6978 - top5-acc: 0.9771 - val_loss: 0.8306 - val_acc: 0.7132 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.8622 - acc: 0.6950 - top5-acc: 0.9780 - val_loss: 0.8499 - val_acc: 0.7146 - val_top5-acc: 0.9806 - lr: 0.0050\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.8473 - acc: 0.7024 - top5-acc: 0.9793 - val_loss: 0.8362 - val_acc: 0.7148 - val_top5-acc: 0.9798 - lr: 0.0050\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 0.8449 - acc: 0.7033 - top5-acc: 0.9786 - val_loss: 0.8593 - val_acc: 0.7056 - val_top5-acc: 0.9792 - lr: 0.0050\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 0.8352 - acc: 0.7057 - top5-acc: 0.9792 - val_loss: 0.8078 - val_acc: 0.7244 - val_top5-acc: 0.9802 - lr: 0.0050\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.8240 - acc: 0.7081 - top5-acc: 0.9804 - val_loss: 0.8460 - val_acc: 0.7122 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 15s 166ms/step - loss: 0.8301 - acc: 0.7066 - top5-acc: 0.9792 - val_loss: 0.8070 - val_acc: 0.7254 - val_top5-acc: 0.9830 - lr: 0.0050\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.8169 - acc: 0.7119 - top5-acc: 0.9800 - val_loss: 0.8099 - val_acc: 0.7268 - val_top5-acc: 0.9824 - lr: 0.0050\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.8162 - acc: 0.7126 - top5-acc: 0.9795 - val_loss: 0.8836 - val_acc: 0.7122 - val_top5-acc: 0.9754 - lr: 0.0050\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.8093 - acc: 0.7139 - top5-acc: 0.9807 - val_loss: 0.8578 - val_acc: 0.7106 - val_top5-acc: 0.9810 - lr: 0.0050\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 0.8261 - acc: 0.7095 - top5-acc: 0.9804 - val_loss: 0.8517 - val_acc: 0.7086 - val_top5-acc: 0.9788 - lr: 0.0050\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.7928 - acc: 0.7210 - top5-acc: 0.9815 - val_loss: 0.8135 - val_acc: 0.7272 - val_top5-acc: 0.9804 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 0.7098 - acc: 0.7483 - top5-acc: 0.9847 - val_loss: 0.7655 - val_acc: 0.7436 - val_top5-acc: 0.9826 - lr: 0.0025\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 14s 164ms/step - loss: 0.6816 - acc: 0.7606 - top5-acc: 0.9868 - val_loss: 0.7325 - val_acc: 0.7558 - val_top5-acc: 0.9844 - lr: 0.0025\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.6670 - acc: 0.7635 - top5-acc: 0.9871 - val_loss: 0.7734 - val_acc: 0.7484 - val_top5-acc: 0.9828 - lr: 0.0025\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 14s 165ms/step - loss: 0.6686 - acc: 0.7620 - top5-acc: 0.9874 - val_loss: 0.8011 - val_acc: 0.7358 - val_top5-acc: 0.9824 - lr: 0.0025\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.6642 - acc: 0.7629 - top5-acc: 0.9882 - val_loss: 0.7632 - val_acc: 0.7446 - val_top5-acc: 0.9816 - lr: 0.0025\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.6697 - acc: 0.7618 - top5-acc: 0.9871 - val_loss: 0.7699 - val_acc: 0.7414 - val_top5-acc: 0.9860 - lr: 0.0025\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.6580 - acc: 0.7670 - top5-acc: 0.9876 - val_loss: 0.7478 - val_acc: 0.7530 - val_top5-acc: 0.9860 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 14s 164ms/step - loss: 0.6103 - acc: 0.7844 - top5-acc: 0.9904 - val_loss: 0.7167 - val_acc: 0.7616 - val_top5-acc: 0.9862 - lr: 0.0012\n",
      "313/313 [==============================] - 9s 30ms/step - loss: 0.7525 - acc: 0.7567 - top5-acc: 0.9842\n",
      "Test accuracy: 75.67%\n",
      "Test top 5 accuracy: 98.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn, layer_normalization_1_layer_call_and_return_conditional_losses, layer_normalization_2_layer_call_fn while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/5Appendix/mlpmixer_independent\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results_Article/5Appendix/mlpmixer_independent\\assets\n"
     ]
    }
   ],
   "source": [
    "history = run_experiment(mlpmixer_independent)\n",
    "mlpmixer_independent.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAN0AAADjCAYAAAABtBHHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAY/UlEQVR4nO3dd5hV1bnH8e8PFQUcikhTVBBBRERREAXFAlEsKTemYIstUXMtSdT43KiPmtzoY4xJNIk1isaSxG6I0YgVEguCWBIRC4KICEhXLGFm3vvH3mMOc8+sdWbOnH3OHN6Pz34c9tpnrTXDvKxd1l6vzAznXHbalbsDzm1oPOicy5gHnXMZ86BzLmMedM5lzIPOuYx50FU4SbdI+mkF9GO+pPHl7kc18KBrIUlHSZop6SNJ70t6WNI+ki6WdHvOcVtLmiPp15KU7pOktyXNLt934MrFg64FJJ0FXAlcCvQCtgWuAb7c6LjtgGnAZDM70/4zE2Es0BPYXtLIjPq8URbtuDgPumaS1AX4CXCamd1nZmvNbJ2Z/cXMfphz3ACSgLvDzM5tVM1xwJ+Bh9KvC227RtKTDaOmpMGSHpW0QtLrkr6Rc+wtkq6V9JCktcAB6SniOZJekbRa0p2SNsv5zOGSXpK0StIzkoa16Ifkgjzomm9vYDPg/sAx25ME3PVmdmFugaSOwNeAO9JtoqT2sUYldQceB542szOBjsCjwB9IRs2JwDWShuR87CjgEqAG+Ee67xvABKA/MAw4Pq1/ODAJOAXoDlwPTJa0aaxvrnk86JqvO7DMzGoDxwwFOgF35in7KvAZMAX4K7AJcFikza2AqcDdZnZBuu9wYL6Z3WxmtWb2InAv8PWcz/3ZzJ42s3oz+zTd92szW2RmK4C/ALul+08m+UdiupnVmdnv037uFembayYPuuZbDmwpaePAMZNJRo0n0uu6XMcBd6WB8ilJoBwHIOm89MbMR5Kuy/nMYUAHIHffdsCo9FRwlaRVwNFA75xj3s3Tt8U5X38MbJ5T39mN6tuGJOBdKwr94rj8niUZAb4C3NPUQWZ2Vnpq9oSksWb2nqS+wIHAnpKOSA/tCGwmaUszu5Tk5kxjvwO6AQ9JmmBma0kCaqqZfSHQ1+a8QvIucImZXdKMz7gW8JGumcxsNXAhcLWkr0jqKGkTSYdIurzR4acDTwKPS+oFHAu8AexIclq3GzAIWAgcGWn6dOB14C+SOgAPAoMkHZu2v4mkkZJ2auG39jvgVEmj0ps0nSQdJqmmhfW5JnjQtYCZ/QI4C7gA+IBklDgdeKDRcUZyrfQ88BhwEnCNmS3O3UhOG4N3MXPqWkhy53MdcBDJDZRFJKeNPwNadOPDzGYC3wF+C6wE3iK9yeJal/wlVuey5SOdcxnzoHMuQNIkSUsl/auJcqWTFd5KJx3sHqvTg865sFtIJhM05RBgYLqdDFwbq9CDzrkAM5sGrAgc8mXgVks8B3SV1CdUpwedc8XZmvUnISxM9zWpZA/HJU0imaq01MyG5uw/AzgNqAP+mmcycL66/Bar+3/MTKHydcveDv7etO8x4BSSU8IGN5jZDa3Rt5BSzki5heSZz60NOyQdQDIc72pmn0nqWcL23Yaubl2wOA2wYoPsPZLpcg36pvuaVLKgM7Npkvo12v1d4DIz+yw9Zmlz6uxeMzDv/j26DIh+dsaqt4LlI7vuUNTnAYZ1aTzNcn2vrH4nWkesHy+snhssH9ttcLSNJ5eH353dJfJ9vLx6frSNQZ2DZ1i8vPztaB1r35uWd3/7HvG/bwDq6ws7rjiTgdMl/QkYBaw2s/dDH8h67uUgYF9JlwCfAueY2YyM++A2EFYXehGkMJL+COxPMsl9IXARyZshmNl1JO9EHkoyg+dj4IRYnVkH3cbAFiSvi4wE7pK0vfm0GFcKkdPLQphZcE5s+rt7WnPqzDroFgL3pR19XlI9sCXJ/MX1SDqZ9S9ynWsey+T0stmyfmTwAHAAgKRBQHtgWb4DzewGMxthZiOy656rJlZXG9zKpZSPDPKdC08CJqVTav4NHOenlq5ksrmR0mylvHvZ1LnwMaVq07n1tMI1XSn4m+OuelXoNV2bCrpzuu2Zd/8F7z8Z/ezEPqOC5fcsmRks/27v0dE2rn7/H8Hy83rvF63j0sVTg+UH9do1WP7g4lnRNl7YaniwfPh74TqO6BNfqvP+xeGfZ1PP4HJ12nps9JigMl63hbSpoHOuOcxPL53LmJ9eOpcxP710LmP1deXuQV4edK56+UjnXMb8ms65jNX6SOdcpvyRgXNZq9DTyzaxwnPDGik9O++Yt3y7Tr2idbyzdkmwvH+n3sHyeWsXB8sBhtRsEyyf/WG+JDrr271z/2D5rDXzguVju+b/GeV6bPmrwfLBXcLfx6ur4m/AL3/nsWB5IbNNdu2+fd79M9//OxBfI+WTx64L/nJ3GH9q8POl4iOdq14b2lsGzpWdPzJwLmM+0jmXMR/pnMuYB51zGavQRwYedK56+YwU5zJWoc+gPehc9arQkc5TZbnqZfXhrQCSJkh6Pc20+j95yreV9KSkF9NMrIfG6mxTI931HfJnlj161TPRz95fs1ew/OtrwikVrui6d7SNc1Y9Gyy/cIt4HT9eHv5eTuiRf3GmBjctnR5t43s9w/34xeLwAksfLnwq2kb37cYHy/+rd3wN4b8t/2f0mKC64l5ilbQRcDXwBZLVyWdImmxmuRlYLgDuMrNrJQ0hyW3QL1Rvmwo655ql+NPLPYG3zOxtgDQzz5eB3KAzoHP6dRdgUaxSDzpXvYp/ZJAvy2rjtRwvBqakyU47AeEhHr+mc1XMauuCm6STJc3M2VqSsOZI4BYz60uSMus2ScG48pHOVa/ISFdAJtZCsqyeBExI63tW0mYkmaiaTHjqI52rXrV14S1uBjBQUn9J7YGJJJlXcy0AxgFI2gnYjDyp33L5SOeqV5FvGZhZraTTgUeAjYBJZvaqpJ8AM81sMnA28DtJPyC5qXJ8LBOVB52rXkU+MgAws4dIHgPk7rsw5+vZwJjm1OlB56pXKwRdKXjQuepVX5lzL0u2MJGkScDhwFIzG5ru+znwRZIsrHOBE8xsVQF1GUBNx/yL9uzRbUC0Py+snBss33uL8II+z618I9rGjp37BstfX7Ow6DrmrA4vbjSy2w7RNqaveDNYvnLB48Hymr77R9sY3j38dzLjg/jPc3SPwXn3/33RE0B8YaK1l3wr+Mvd6fxby7IwUSnvXt5Ceis1x6PAUDMbBrwB/KiE7bsNXSvMvSyFkgWdmU0DVjTaN8XMGubmPEfy3MO50ij+kUFJlPOa7kTgzjK276pdhV7TlSXoJJ0P1AJ3lKN9t4Hwu5cJSceT3GAZF3qImM6Da8lcOOeAZO5lJco06CRNAM4F9jOzj0PH5s6La7h76VyzbGinl5L+COwPbClpIXARyd3KTYFHJQE8Z2anlqoPbgO3oZ1emtmReXbfVKr2nGvMNrSRzrmy82u64j3Ta0je/aMW/yv62btqwuuCHLf6hWD54113ibYxblV4TY9C6jhw5SvB8tsia72csDq81gvEZ5x023ZcsPzs3vtE27hm2fPB8he3zr/eTa6xH7wWPSbIRzrnsmV1vsKzc9mq9aBzLlN+I8W5rNV60DmXKR/pnMuaj3TOZctHOucyZj7SOZctq8xMWW0r6PZfln9djTHd86+lkevYVTOD5YNqtg6WH7T61WgbI7qG1wWZsGZ2sBzia5yctCb8fSyZ90i0jV79Dw6Wj9piYLD8qqXh7EQAh/QYFizfY1F4BhDA4b3zz1q5f214vZvPVeZjurYVdM41h490zmWsjGsPBXkuA1e16mvDWyFimVjTY74habakVyX9IVanj3SueoWXxYwqJBOrpIEkL2ePMbOVknrG6vWgc1WrvrbotWQLycT6HeBqM1sJYGZNpshq4KeXrmrV1ym4FSBfJtbGt7kHAYMkPS3puXQdoCAf6VzVit1IybPi3A3pgljNsTEwkGQ9oL7ANEm7hNIFeNC5qhUbzVopE+tCYLqZrQPmSXqDJAibfIU/enopaSNJviisa3OsXsGtAIVkYn2AZJRD0pYkp5tvhyqNjnRmVidpO0ntzezfhfS0VKZvmz9rz+7zw1loAN4YsU2wfOdZi4LlrwwJfx5g+Jx3guV3dhgereNrq8IzNZbOnxIs77N99JKCmzuNDJYfu/K5YPlJPUdF2/jTypeC5Qf12jVax7SVc6LHhBR43dakAjOxPgIcJGk2UAf80MyWh+ot9PTybeBpSZOBtTmd+mULvhfnMlHgaBauI56J1YCz0q0ghQbd3HRrB9QUWrlz5VTsSFcqBQWdmf0YQFLH2HLozlWK+iIfjpdKQc/pJO2dnrPOSf+8q6RrStoz54pUX9cuuJVLoS1fCRwMLAcws5eBsSXqk3OtohUejpdEwc/pzOzdNOlHg8pcs9q5VKWeXhYadO9KGg2YpE2A7wFFrnntXGnVt8Ldy1IoNOhOBa4imXf2HjAFOK1UnXKuNbT1kc7M7OiS9sS5VlapI12hN1Kek3S3pEPU6MLOuUpVbwpu5aJA2u//HJQE2njgRGAkcBdwi5nlXykoXt8PgG8DBvwTOMHMPg0cbwDdNs+/aM8ONVtF23xzTeN5qusbUNMnWD73w/ejbfTbvFewfP5HS6J1xKZ59ex3ULB8967bR9uYsfKtYPngLuEpb3NWvxssBxizxY7B8seXhtOKARzQM39qsSkL/waAWThypm/11eAv96hF95Ul8goa6SzxaJpd9TvAccDzkqZKCid+a0TS1sCZwAgzG0oyp21iM/vtXFSdKbiVS0HXdJK6A8cAxwJLgDNIZlvvBtwN5J+JHG63g6R1QEcgPNvYuRaIDIRlU+iNlGeB24CvmNnCnP0zJV3XnAbN7D1JVwALgE+AKWYWPqdyrgXqaNtBt6M1cfFnZj9rToOSupGsM9EfWAXcLekYM7u9OfU4F1OhqQwKDrotJZ0L7Axs1rDTzA5sQZvjgXlm9gGApPuA0cB6QZfnVXrnmqWuQpcAKrRXd5BMdu4P/BiYT+B19IgFwF6SOqZ3RceRZ3aLmd1gZiPMbEQL23EbuPrIVi6FBl13M7sJWGdmU83sRKAloxxmNh24B5hF8rigHeF1KpxrkToU3Mql0NPLden/35d0GMndxi1a2qiZXQRc1NLPO1eItn4j5aeSugBnA78BOgPfL1WnnGsNFToLrOA3xx9Mv1wNHAAg6fsl6lOTXtpx27z7d40sCAQwfZtwCqoxi+YFy+eMDKfSAthpZvhxY2y2CcRnnNzTaY9g+TfXvBht44lu4TRW41aFZ4tc1H10tI3LV4dTep3Xe79oHb9e8Xz0mJBKHemKub1T8EIszpVDpd5IKWax2cr8Z8S5VF2Fzs0vJugq9NGjc4kKTU8XDjpJH5I/uAR0KEmPnGsltW1xpDMzX+PStVkVuuxlhc6Tca4VtMaNlEIysabHHSHJJEVnUHnWHle1ih3pCsnEmh5XQ7JY1/RC6vWRzlWtVhjpPs/EmibPacjE2tj/Aj8Dmlz9IJcHnatadQpvBYhmYpW0O7CNmf210H61qdPLkW/mX+dkSJf8M1Vy7btofrB8ny6DguXDZsXTcS2Z90iwvFf/g6N1xNZZmfjJS8HyXTv3i7YRm3GyY+e+wfKfrHg22kZsrZZLF0+N1jG6x+C8+6d+FF7jpUFsNCs2E6ukdsAvgeML/Qy0saBzrjliS5C3QibWGmAo8FS6SF5vYLKkL5lZk/PgPOhc1aot/pHB55lYSYJtInBUQ6GZrQa2bPizpKeAc0IBB35N56pYsTdSzKwWaMjE+hpwV0MmVklfamm/fKRzVas1Ho7HMrE22r9/IXV60Lmq1SbnXjrXltVV6Jx8DzpXtXykcy5jlZq11IPOVa1a+ell0V4dnX+2xoC/x9dIee+0XYPlO1wfTiy7cO5DwXKAvgMODZa/NiKeXWjwjHB2oX/uHJ59M2x2eK0XgMe75s+G0yA2Y+WKrvGcMRd8GF4j5bQ++0Tr+P3yF6LHhFRmyLWxoHOuOWorNOw86FzVqsyQ86BzVcxHOucyVpkh50Hnqpg/HHcuY3566VzGKjPkPOhcFfORzrmMmQdd8XZ6ZnHe/aO7hdc3Aeh/3avB8kVzHw6Wb7PDYdE2RncZGCwfOuuNaB0DavoEy3d7bWGwfFBNPLvQhDWzg+V7RX6e56yMr5GyU+dtguXXLn4mWsd+PXbOu39KgWukVOqNlLK9OS5pI0kvSnowfrRzzVeNWXuK9T2SV+A7l7EProrVmY90n5PUFzgMuLEc7bsNQx31wa1cyjXSXQmcS7KEmXMlUakvsWY+0kk6HFhqZsH3NiSdLGmmpPA7Is41wUe6/xgDfEnSocBmQGdJt5vZMbkH5S4EKlXo24iuovlIlzKzH5lZXzPrR7J45xONA8651lBn9cGtXNrUczrnmqNSR7qyBp2ZPQU8Vc4+uOpVzuu2kDY10s0Z2zvv/v5PvB797PJ3HguWbzXgkGD5vFPzz47INeC68EyPp7fqH61j1Lvh2RZzRoQz6uz0QniNFYDbOoaThR65Ipzb8J7N94y2cezal4LlX+sVTVjKQyvCa7XEtMZzOkkTgKuAjYAbzeyyRuVnAd8GaoEPgBPNLLhoj+cycFXLIv/F5GRiPQQYAhwpaUijw14ERpjZMOAe4PJYvR50rmq1wo2UaCZWM3vSzD5O//gcSTqtIA86V7XqseBWgGgm1kZOAsIz52lj13TONUdsNCs2E2ujuo4BRgD7xY71oHNVKzaWtUImVgAkjQfOB/Yzs89i/fKgc1WrtvhHBsFMrACShgPXAxPMbGkhlXrQuapV7KwTM6uV1JCJdSNgUkMmVmCmmU0Gfg5sDtyd5h1fYGbBLK0edK5qtcZyDbFMrGY2vrl1etC5qlXO+ZUhHnSualmFvjmuSu1YroZXe7p2GpC3fOn8KdE6um8XPgsYs8WOwfJnVsYXFRraZbtg+ezVC6J1DOwcXljo9TXhhYlGdQsvjgTw9PI5wfI9twjX8fyKN6Nt7Nw1/LOYtSy+uNC4XsPy7p+y8G8AmJlCnx/We+/gL/cri58Nfr5UfKRzVau+QgcUDzpXtfyazrmMedA5lzFf4dm5jPlI51zGzIPOuWz5SOdcxgp8Zy5zHnSuatXV+0hXtKZmnvTsd1D0s/MODKduGjzt7WD53H2jb+Gz8zOLguUvDw7P0gDY7fXwrJVZ/XYIlo9aEP4+AKb3HhosH70kvMDSHZ1HR9s4Ze2sYPlP+xwQreOKlc9Hjwmp99NL57Llp5fOZcxPL53LWKVO5vegc1XLHxk4lzF/y8C5jPlI51zGKvWark29Oe5crtib4+037Rv8vfn3Zwv9zXHnWlOlXtO1iZGuHCSd3NIltqutH5XQh0rqR7E8gUjTTo4fkolK6Ecl9AEqpx9F8aBzLmMedM5lzIOuaZVy7VAJ/aiEPkDl9KMofiPFuYz5SOdcxjzonMuYB51zGfOga4KkWzNur72kb6WpdJF0lKTfSjpN0iZZ9sWVlt9IASRNbrwLOAB4AiCWWbOV+nAHybS8jsAqkuye9wHjSP6ejit1HyqdpO5mtrzc/SiWz71M9AVmAzeS5IcXMAL4RYZ92MXMhknamCS/9VZmVifpduDlrDohqTPwI5KfycNm9oecsmvM7L8z6sdlwBVmtkzSCOAuoD4d9b9lZlOz6Ecp+OllYgTwAnA+sNrMngI+MbOpGf7ltpPUHqghGe26pPs3BbI8vbyZ5B+de4GJku6VtGlatleG/TjMzJalX/8c+KaZ7QB8gWz/MWx1PtIBlqy//StJd6f/X0L2P5ubgDkkCeXPJ0kc/zbJL/qfMuzHADM7Iv36AUnnA09IKvkpdiMbS9rYzGqBDmY2A8DM3sj5R6BN8mu6PCQdBowxs/MybncrADNbJKkrMB5YYGbFLQDZvD68BuxsOYkAJB0P/BDY3Mzii3e2Tj/OAL4IXAaMBbqRXOMeCGxvZsdm0Y9S8KBz65F0OTDFzB5rtH8C8Bszi+dXbr2+7A98FxhEcubxLvAAMCkdAdskDzpXMEknmNnN3o/ieNC5gklaYGbbej+K4zdS3HokvdJUEdBrQ+tHKXjQucZ6AQcDKxvtF/DMBtiPVudB5xp7kOQu5UuNCyQ9tQH2o9X5NZ1zGfMZKc5lzIPOuYx50JWRpI/K3QeXPQ+6DUD65oKrEB50FUbSFyVNl/SipMck9ZLUTtKbknqkx7ST9JakHul2r6QZ6TYmPeZiSbdJehq4razflFuPB13l+Qewl5kNJ3m74Nx08vHtwNHpMeOBl83sA+Aq4FdmNhI4guSdwAZDgPFmdmRmvXdRftpRefoCd0rqA7QH5qX7JwF/Bq4ETiR57w2SABwifZ6AprOkzdOvJ5vZJ1l02hXOR7rK8xvgt2a2C3AKsBmAmb0LLJF0ILAn8HB6fDuSkXG3dNvazBpu0KzNuO+uAB50lacLyXINAI3XRbmR5DTzbjOrS/dNAc5oOEDSbqXuoCuOB115dZS0MGc7C7iY5K3xF4BljY6fTLJgUe5rLWcCIyS9Imk2cGoWHXct59PA2pB0gZ5fmdm+5e6Lazm/kdJGSPofkreoj44d6yqbj3TOZcyv6ZzLmAedcxnzoHMuYx50zmXMg865jHnQOZex/wOYDcac/dpp2QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#After training heatmap\n",
    "A5_at_ave_mixer_activations = Prom_Mixer_Activations_Blocks_2(mlpmixer_independent,batch_prepro)\n",
    "A5_at_global_heatmap = Heatmap(A5_at_ave_mixer_activations,type,sigma)\n",
    "visualize_Heatmap_mlp(A5_at_global_heatmap,type,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cumulative\n",
    "A5_at_ave_mixer_activations_block = Prom_Mixer_Activations_Blocks(mlpmixer_independent,batch_prepro)\n",
    "A5_at_global_heatmap_block = Heatmap(A5_at_ave_mixer_activations_block,type,sigma)\n",
    "visualize_Heatmap(A5_at_global_heatmap_block,type,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Independent blocks\n",
    "A5_at_ave_mixer_activations_block_ind = Prom_Mixer_Activations_Blocks_ind(mlpmixer_independent,batch_prepro)\n",
    "A5_at_global_heatmap_block_ind = Heatmap(A5_at_ave_mixer_activations_block_ind,type,sigma)\n",
    "visualize_Heatmap(A5_at_global_heatmap_block_ind,type,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/heatmap_bt_block.pkl','wb') as file:\n",
    "    pickle.dump(A5_bt_global_heatmap_block,file)  \n",
    "with open(path + '/heatmap_bt.pkl','wb') as file:\n",
    "    pickle.dump(A5_bt_global_heatmap,file)\n",
    "with open(path + '/heatmap_bt_block_ind.pkl','wb') as file:\n",
    "    pickle.dump(A5_bt_global_heatmap_block_ind,file) \n",
    "##########################################################################\n",
    "with open(path + '/heatmap_at.pkl','wb') as file:\n",
    "    pickle.dump(A5_at_global_heatmap,file)\n",
    "with open(path + '/heatmap_at_block.pkl','wb') as file:\n",
    "    pickle.dump(A5_at_global_heatmap_block,file) \n",
    "with open(path + '/heatmap_at_block_ind.pkl','wb') as file:\n",
    "    pickle.dump(A5_at_global_heatmap_block_ind,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(path + '/heatmap_at.pkl','rb') as file:\n",
    "#    tested_hola = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_Heatmap_mlp(tested_hola,'kernel',False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_image_classification",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
